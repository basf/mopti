{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Opti The primary purpose of opti is to define tasks or problems in a number of closely related fields, including experimental design, multiobjective optimization and decision making and Bayesian optimization. Opti specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved. Experimental design In the context of experimental design opti allows to define a design space \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] where the design parameters may take values depending on their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) and a set of equations define additional experimental constraints, e.g. linear equality: \\(\\sum x_i = 1\\) linear inequality: \\(2 x_1 \\leq x_2\\) non-linear inequality: \\(\\sum x_i^2 \\leq 1\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values. Multiobjective optimization In the context of multiobjective optimization opti allows to define a vector-valued optimization problem \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] where \\(x \\in \\mathbb{X}\\) is again the experimental design space \\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and \\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized. Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision. Bayesian optimization In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\) . An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.","title":"Opti"},{"location":"#opti","text":"The primary purpose of opti is to define tasks or problems in a number of closely related fields, including experimental design, multiobjective optimization and decision making and Bayesian optimization. Opti specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.","title":"Opti"},{"location":"#experimental-design","text":"In the context of experimental design opti allows to define a design space \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] where the design parameters may take values depending on their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) and a set of equations define additional experimental constraints, e.g. linear equality: \\(\\sum x_i = 1\\) linear inequality: \\(2 x_1 \\leq x_2\\) non-linear inequality: \\(\\sum x_i^2 \\leq 1\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.","title":"Experimental design"},{"location":"#multiobjective-optimization","text":"In the context of multiobjective optimization opti allows to define a vector-valued optimization problem \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] where \\(x \\in \\mathbb{X}\\) is again the experimental design space \\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and \\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized. Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.","title":"Multiobjective optimization"},{"location":"#bayesian-optimization","text":"In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\) . An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.","title":"Bayesian optimization"},{"location":"install/","text":"Install Note: the package name is mopti while the import name is opti . pip install mopti or install the latest version with pip install git+https://github.com/basf/mopti.git","title":"Install"},{"location":"install/#install","text":"Note: the package name is mopti while the import name is opti . pip install mopti or install the latest version with pip install git+https://github.com/basf/mopti.git","title":"Install"},{"location":"overview/","text":"Overview Opti problems consist of a definition of the input space \\(x \\in \\mathbb{X}\\) , output space \\(y \\in \\mathbb{Y}\\) , objectives \\(s(y)\\) , constraints \\(g(x) \\leq 0\\) , output constraints \\(h(y)\\) and possibly an existing data set. Parameters Input and output spaces are defined using the Parameters class, for example from opti.parameter import Parameters , Continuous , Discrete , Categorical inputs = Parameters ([ Continuous ( \"x1\" , domain = [ 0 , 1 ]), Continuous ( \"x2\" , domain = [ 0 , 1 ]), Continuous ( \"x3\" , domain = [ 0 , 1 ]), Discrete ( \"x4\" , domain = [ 1 , 2 , 5 , 7.5 ]), Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) ]) outputs = Parameters ([ Continuous ( \"y1\" , domain = [ 0 , None ]), Continuous ( \"y2\" , domain = [ None , None ]), Continuous ( \"y3\" , domain = [ 0 , 100 ]) ]) Note that for some of the outputs we didn't specify bounds as we may not know them. Individual parameters can be indexed by name. inputs [ \"x5\" ] >>> Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) and all parameter names can retrieved with inputs.names >>> [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"] We can sample from individual parameters, parameter spaces or parameter spaces including constraints (more on that later) x5 = inputs [ \"x1\" ] . sample ( 3 ) print ( x5 . values ) >>> array ([ \"A\" , \"C\" , \"A\" , \"C\" , \"A\" ], dtype = object ) X = inputs . sample ( 5 ) print ( X ) >>> x1 x2 x3 x4 x5 0 0.760116 0.063584 0.518885 7.5 A 1 0.807928 0.496213 0.885545 1.0 C 2 0.351253 0.993993 0.340414 5.0 B 3 0.385825 0.857306 0.355267 1.0 C 4 0.191907 0.993494 0.384322 2.0 A and check for each point in a dataframe, whether it is contained in the space. inputs . contains ( X ) >>> array ([ True , True , True , True , True ]) Note that in opti all functions operating on dataframes use the parameter name to identify corresponding column. Hence, a dataframe may contain additional columns and columns may be in arbitrary order. The index of a dataframe is preserved, meaning that the returned dataframe will have the same indices as the original dataframe. Constraints Input constraints are defined separately from the input space. There are currently five supported types of constraints. from opti import Constraints , LinearEquality , LinearInequality , NonlinearEquality , NonlinearInequality , NChooseK Linear constraints are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input parameters they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\) . # A mixture: x1 + x2 + x3 = 1 constr1 = LinearEquality ([ \"x1\" , \"x2\" , \"x3\" ], lhs = 1 , rhs = 1 ) # x1 + 2 * x3 < 0.8 constr2 = LinearInequality ([ \"x1\" , \"x3\" ], lhs = [ 1 , 2 ], rhs = 0.8 ) Because of the product \\(a_i x_i\\) , linear constraints cannot operate on categorical parameters. Nonlinear constraints take any expression that can be evaluated by pandas.eval , including mathematical operators such as sin , exp , log10 or exponentiation. # The unit circle: x1**2 + x2**2 = 1 constr3 = NonlinearEquality ( \"x1**2 + x2**2 - 1\" ) Nonlinear constraints can also operate on categorical parameters and support conditional statements. # Require x1 < 0.5 if x5 == \"A\" constr4 = NonlinearInequality ( \"(x1 - 0.5) * (x5 =='A')\" ) Finally, there's a constraint to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take non-zero values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe. # Only 2 parameters can be non-zero constr5 = NChooseK ([ \"x1\" , \"x2\" , \"x3\" ], max_active = 2 ) As with the parameters there's a container which acts as the union of a list of multiple constraints. constraints = Constraints ([ constr1 , constr2 , constr3 , constr4 , constr5 ]) We can check whether a point is satisfies individual constraints or the list of constraints. constr2 . satisfied ( X ) . values >>> array ([ False , False , True , True , True ]) The distance to the constraint boundary can also be evaluated for use in numerical optimization methods, where values \\(\\leq 0\\) correspond to a satisified constraint. constr2 ( X ) . values >>> array ([ 0.479001 , 0.89347371 , - 0.10833372 , - 0.05890873 , - 0.22377122 ]) Opti contains a number of methods to generate random samples from constrained spaces, see the sampling reference. Objectives In an optimization problem we want to be able to define the target direction or target value individually for each output. This is done using objectives from opti.objective import Objectives , Minimize , Maximize , CloseToTarget objectives = Objectives ([ Minimize ( \"y1\" ), Maximize ( \"y2\" ), CloseToTarget ( \"y3\" , target = 7 ) ]) We can compute objective values from output values. Y = pd . DataFrame ({ \"y1\" : [ 1 , 2 , 3 ], \"y2\" : [ 7 , 4 , 5 ], \"y3\" : [ 5 , 6.9 , 12 ] }) objectives ( Y ) >>> minimize_y1 maximize_y2 closetotarget_y3 0 1 - 7 4.00 1 2 - 4 0.01 2 3 - 5 25.00 Objectives can also be used as output constraints. This is different from an objective in that we want the constraint to be satisfied and not explore possible tradeoffs. Problem Finally, a problem is the combination of inputs, outputs, objectives, constraints, output_constraints, (true) function and data. problem = opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , objectives = objectives ) Problems can be serialized to and from a dictionary config = problem . to_config () problem = Problem ( ** config ) or to a json file problem . to_json ( \"problem.json\" ) problem = opti . read_json ( \"problem.json\" )","title":"Overview"},{"location":"overview/#overview","text":"Opti problems consist of a definition of the input space \\(x \\in \\mathbb{X}\\) , output space \\(y \\in \\mathbb{Y}\\) , objectives \\(s(y)\\) , constraints \\(g(x) \\leq 0\\) , output constraints \\(h(y)\\) and possibly an existing data set.","title":"Overview"},{"location":"overview/#parameters","text":"Input and output spaces are defined using the Parameters class, for example from opti.parameter import Parameters , Continuous , Discrete , Categorical inputs = Parameters ([ Continuous ( \"x1\" , domain = [ 0 , 1 ]), Continuous ( \"x2\" , domain = [ 0 , 1 ]), Continuous ( \"x3\" , domain = [ 0 , 1 ]), Discrete ( \"x4\" , domain = [ 1 , 2 , 5 , 7.5 ]), Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) ]) outputs = Parameters ([ Continuous ( \"y1\" , domain = [ 0 , None ]), Continuous ( \"y2\" , domain = [ None , None ]), Continuous ( \"y3\" , domain = [ 0 , 100 ]) ]) Note that for some of the outputs we didn't specify bounds as we may not know them. Individual parameters can be indexed by name. inputs [ \"x5\" ] >>> Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) and all parameter names can retrieved with inputs.names >>> [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"] We can sample from individual parameters, parameter spaces or parameter spaces including constraints (more on that later) x5 = inputs [ \"x1\" ] . sample ( 3 ) print ( x5 . values ) >>> array ([ \"A\" , \"C\" , \"A\" , \"C\" , \"A\" ], dtype = object ) X = inputs . sample ( 5 ) print ( X ) >>> x1 x2 x3 x4 x5 0 0.760116 0.063584 0.518885 7.5 A 1 0.807928 0.496213 0.885545 1.0 C 2 0.351253 0.993993 0.340414 5.0 B 3 0.385825 0.857306 0.355267 1.0 C 4 0.191907 0.993494 0.384322 2.0 A and check for each point in a dataframe, whether it is contained in the space. inputs . contains ( X ) >>> array ([ True , True , True , True , True ]) Note that in opti all functions operating on dataframes use the parameter name to identify corresponding column. Hence, a dataframe may contain additional columns and columns may be in arbitrary order. The index of a dataframe is preserved, meaning that the returned dataframe will have the same indices as the original dataframe.","title":"Parameters"},{"location":"overview/#constraints","text":"Input constraints are defined separately from the input space. There are currently five supported types of constraints. from opti import Constraints , LinearEquality , LinearInequality , NonlinearEquality , NonlinearInequality , NChooseK Linear constraints are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input parameters they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\) . # A mixture: x1 + x2 + x3 = 1 constr1 = LinearEquality ([ \"x1\" , \"x2\" , \"x3\" ], lhs = 1 , rhs = 1 ) # x1 + 2 * x3 < 0.8 constr2 = LinearInequality ([ \"x1\" , \"x3\" ], lhs = [ 1 , 2 ], rhs = 0.8 ) Because of the product \\(a_i x_i\\) , linear constraints cannot operate on categorical parameters. Nonlinear constraints take any expression that can be evaluated by pandas.eval , including mathematical operators such as sin , exp , log10 or exponentiation. # The unit circle: x1**2 + x2**2 = 1 constr3 = NonlinearEquality ( \"x1**2 + x2**2 - 1\" ) Nonlinear constraints can also operate on categorical parameters and support conditional statements. # Require x1 < 0.5 if x5 == \"A\" constr4 = NonlinearInequality ( \"(x1 - 0.5) * (x5 =='A')\" ) Finally, there's a constraint to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take non-zero values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe. # Only 2 parameters can be non-zero constr5 = NChooseK ([ \"x1\" , \"x2\" , \"x3\" ], max_active = 2 ) As with the parameters there's a container which acts as the union of a list of multiple constraints. constraints = Constraints ([ constr1 , constr2 , constr3 , constr4 , constr5 ]) We can check whether a point is satisfies individual constraints or the list of constraints. constr2 . satisfied ( X ) . values >>> array ([ False , False , True , True , True ]) The distance to the constraint boundary can also be evaluated for use in numerical optimization methods, where values \\(\\leq 0\\) correspond to a satisified constraint. constr2 ( X ) . values >>> array ([ 0.479001 , 0.89347371 , - 0.10833372 , - 0.05890873 , - 0.22377122 ]) Opti contains a number of methods to generate random samples from constrained spaces, see the sampling reference.","title":"Constraints"},{"location":"overview/#objectives","text":"In an optimization problem we want to be able to define the target direction or target value individually for each output. This is done using objectives from opti.objective import Objectives , Minimize , Maximize , CloseToTarget objectives = Objectives ([ Minimize ( \"y1\" ), Maximize ( \"y2\" ), CloseToTarget ( \"y3\" , target = 7 ) ]) We can compute objective values from output values. Y = pd . DataFrame ({ \"y1\" : [ 1 , 2 , 3 ], \"y2\" : [ 7 , 4 , 5 ], \"y3\" : [ 5 , 6.9 , 12 ] }) objectives ( Y ) >>> minimize_y1 maximize_y2 closetotarget_y3 0 1 - 7 4.00 1 2 - 4 0.01 2 3 - 5 25.00 Objectives can also be used as output constraints. This is different from an objective in that we want the constraint to be satisfied and not explore possible tradeoffs.","title":"Objectives"},{"location":"overview/#problem","text":"Finally, a problem is the combination of inputs, outputs, objectives, constraints, output_constraints, (true) function and data. problem = opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , objectives = objectives ) Problems can be serialized to and from a dictionary config = problem . to_config () problem = Problem ( ** config ) or to a json file problem . to_json ( \"problem.json\" ) problem = opti . read_json ( \"problem.json\" )","title":"Problem"},{"location":"ref-constraint/","text":"Constraints Constraint Base class to define constraints on the input space, g(x) == 0 or g(x) <= 0. Source code in opti/constraint.py class Constraint : \"\"\"Base class to define constraints on the input space, g(x) == 0 or g(x) <= 0.\"\"\" def __call__ ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint g(x).\"\"\" raise NotImplementedError def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities.\"\"\" raise NotImplementedError def to_config ( self ) -> Dict : raise NotImplementedError __call__ ( self , data ) special Numerically evaluate the constraint g(x). Source code in opti/constraint.py def __call__ ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint g(x).\"\"\" raise NotImplementedError satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities.\"\"\" raise NotImplementedError Constraints List of optimization constraints Source code in opti/constraint.py class Constraints : \"\"\"List of optimization constraints\"\"\" def __init__ ( self , constraints : Sequence ): self . constraints = [] for c in constraints : if not isinstance ( c , Constraint ): if \"names\" in c and len ( c [ \"names\" ]) == 0 : continue # skip empty constraints c = make_constraint ( ** c ) self . constraints . append ( c ) def __repr__ ( self ): return \"Constraints( \\n \" + pprint . pformat ( self . constraints ) + \" \\n )\" def __iter__ ( self ): return iter ( self . constraints ) def __len__ ( self ): return len ( self . constraints ) def __getitem__ ( self , i ): return self . constraints [ i ] def __call__ ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c ( data ) for c in self . constraints ], axis = 1 ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) def to_config ( self ) -> List [ Dict ]: return [ obj . to_config () for obj in self . constraints ] __call__ ( self , data ) special Numerically evaluate all constraints. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description DataFrame Constraint evaluation g(x) for each of the constraints. Source code in opti/constraint.py def __call__ ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c ( data ) for c in self . constraints ], axis = 1 ) satisfied ( self , data ) Check if all constraints are satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description Series Series of booleans indicating if all constraints are satisfied. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) LinearEquality ( Constraint ) Source code in opti/constraint.py class LinearEquality ( Constraint ): def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) def __repr__ ( self ): return ( f \"LinearEquality(names= { self . names } , lhs= { list ( self . lhs ) } , rhs= { self . rhs } )\" ) def to_config ( self ) -> Dict : return dict ( type = \"linear-equality\" , names = self . names , lhs = self . lhs . tolist (), rhs = self . rhs , ) __init__ ( self , names , lhs = 1 , rhs = 0 ) special Linear / affine inequality of the form 'lhs * x == rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, List[float], numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as LinearEquality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients of A, B and C are not 1 they are passed explicitly. LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) LinearInequality ( Constraint ) Source code in opti/constraint.py class LinearInequality ( Constraint ): def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 def __repr__ ( self ): return f \"LinearInequality(names= { self . names } , lhs= { list ( self . lhs ) } , rhs= { self . rhs } )\" def to_config ( self ) -> Dict : return dict ( type = \"linear-inequality\" , names = self . names , lhs = self . lhs . tolist (), rhs = self . rhs , ) __init__ ( self , names , lhs = 1 , rhs = 0 ) special Linear / affine inequality of the form 'lhs * x <= rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, List[float], numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as LinearInequality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients are not 1, they need to be passed explicitly. LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both lhs and rhs need to be multiplied by -1. LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 NChooseK ( Constraint ) Source code in opti/constraint.py class NChooseK ( Constraint ): def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : x = data [ self . names ] . values num_zeros = x . shape [ 1 ] - self . max_active violation = np . apply_along_axis ( func1d = lambda r : sum ( sorted ( r )[: num_zeros ]), axis = 1 , arr = x ) return pd . Series ( violation , index = data . index ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self ( data ) <= 0 , index = data . index ) def __repr__ ( self ): return f \"NChooseK(names= { self . names } , max_active= { self . max_active } )\" def to_config ( self ) -> Dict : return dict ( type = \"n-choose-k\" , names = self . names , max_active = self . max_active ) __init__ ( self , names , max_active ) special Only k out of n values are allowed to take nonzero values. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required max_active int Maximium number of non-zero parameter values. required Examples: A choice of 2 or less from A, B, C, D or E can be defined as NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self ( data ) <= 0 , index = data . index ) NonlinearEquality ( Constraint ) Source code in opti/constraint.py class NonlinearEquality ( Constraint ): def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) def __repr__ ( self ): return f \"NonlinearEquality(' { self . expression } ')\" def to_config ( self ) -> Dict : return dict ( type = \"nonlinear-equality\" , expression = self . expression ) __init__ ( self , expression ) special Equality of the form 'expression == 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 = 1, use NonlinearEquality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearEquality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearEquality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) NonlinearInequality ( Constraint ) Source code in opti/constraint.py class NonlinearInequality ( Constraint ): def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 def __repr__ ( self ): return f \"NonlinearInequality(' { self . expression } ')\" def to_config ( self ) -> Dict : return dict ( type = \"nonlinear-inequality\" , expression = self . expression ) __init__ ( self , expression ) special Inequality of the form 'expression <= 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 < 1, use NonlinearInequality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearInequality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearInequality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0","title":"Constraint"},{"location":"ref-constraint/#constraints","text":"","title":"Constraints"},{"location":"ref-constraint/#opti.constraint.Constraint","text":"Base class to define constraints on the input space, g(x) == 0 or g(x) <= 0. Source code in opti/constraint.py class Constraint : \"\"\"Base class to define constraints on the input space, g(x) == 0 or g(x) <= 0.\"\"\" def __call__ ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint g(x).\"\"\" raise NotImplementedError def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities.\"\"\" raise NotImplementedError def to_config ( self ) -> Dict : raise NotImplementedError","title":"Constraint"},{"location":"ref-constraint/#opti.constraint.Constraint.__call__","text":"Numerically evaluate the constraint g(x). Source code in opti/constraint.py def __call__ ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint g(x).\"\"\" raise NotImplementedError","title":"__call__()"},{"location":"ref-constraint/#opti.constraint.Constraint.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities.\"\"\" raise NotImplementedError","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.Constraints","text":"List of optimization constraints Source code in opti/constraint.py class Constraints : \"\"\"List of optimization constraints\"\"\" def __init__ ( self , constraints : Sequence ): self . constraints = [] for c in constraints : if not isinstance ( c , Constraint ): if \"names\" in c and len ( c [ \"names\" ]) == 0 : continue # skip empty constraints c = make_constraint ( ** c ) self . constraints . append ( c ) def __repr__ ( self ): return \"Constraints( \\n \" + pprint . pformat ( self . constraints ) + \" \\n )\" def __iter__ ( self ): return iter ( self . constraints ) def __len__ ( self ): return len ( self . constraints ) def __getitem__ ( self , i ): return self . constraints [ i ] def __call__ ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c ( data ) for c in self . constraints ], axis = 1 ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) def to_config ( self ) -> List [ Dict ]: return [ obj . to_config () for obj in self . constraints ]","title":"Constraints"},{"location":"ref-constraint/#opti.constraint.Constraints.__call__","text":"Numerically evaluate all constraints. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description DataFrame Constraint evaluation g(x) for each of the constraints. Source code in opti/constraint.py def __call__ ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c ( data ) for c in self . constraints ], axis = 1 )","title":"__call__()"},{"location":"ref-constraint/#opti.constraint.Constraints.satisfied","text":"Check if all constraints are satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description Series Series of booleans indicating if all constraints are satisfied. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.LinearEquality","text":"Source code in opti/constraint.py class LinearEquality ( Constraint ): def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) def __repr__ ( self ): return ( f \"LinearEquality(names= { self . names } , lhs= { list ( self . lhs ) } , rhs= { self . rhs } )\" ) def to_config ( self ) -> Dict : return dict ( type = \"linear-equality\" , names = self . names , lhs = self . lhs . tolist (), rhs = self . rhs , )","title":"LinearEquality"},{"location":"ref-constraint/#opti.constraint.LinearEquality.__init__","text":"Linear / affine inequality of the form 'lhs * x == rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, List[float], numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as LinearEquality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients of A, B and C are not 1 they are passed explicitly. LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.LinearEquality.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.LinearInequality","text":"Source code in opti/constraint.py class LinearInequality ( Constraint ): def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 def __repr__ ( self ): return f \"LinearInequality(names= { self . names } , lhs= { list ( self . lhs ) } , rhs= { self . rhs } )\" def to_config ( self ) -> Dict : return dict ( type = \"linear-inequality\" , names = self . names , lhs = self . lhs . tolist (), rhs = self . rhs , )","title":"LinearInequality"},{"location":"ref-constraint/#opti.constraint.LinearInequality.__init__","text":"Linear / affine inequality of the form 'lhs * x <= rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, List[float], numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as LinearInequality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients are not 1, they need to be passed explicitly. LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both lhs and rhs need to be multiplied by -1. LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.LinearInequality.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NChooseK","text":"Source code in opti/constraint.py class NChooseK ( Constraint ): def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : x = data [ self . names ] . values num_zeros = x . shape [ 1 ] - self . max_active violation = np . apply_along_axis ( func1d = lambda r : sum ( sorted ( r )[: num_zeros ]), axis = 1 , arr = x ) return pd . Series ( violation , index = data . index ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self ( data ) <= 0 , index = data . index ) def __repr__ ( self ): return f \"NChooseK(names= { self . names } , max_active= { self . max_active } )\" def to_config ( self ) -> Dict : return dict ( type = \"n-choose-k\" , names = self . names , max_active = self . max_active )","title":"NChooseK"},{"location":"ref-constraint/#opti.constraint.NChooseK.__init__","text":"Only k out of n values are allowed to take nonzero values. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required max_active int Maximium number of non-zero parameter values. required Examples: A choice of 2 or less from A, B, C, D or E can be defined as NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NChooseK.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self ( data ) <= 0 , index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality","text":"Source code in opti/constraint.py class NonlinearEquality ( Constraint ): def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) def __repr__ ( self ): return f \"NonlinearEquality(' { self . expression } ')\" def to_config ( self ) -> Dict : return dict ( type = \"nonlinear-equality\" , expression = self . expression )","title":"NonlinearEquality"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.__init__","text":"Equality of the form 'expression == 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 = 1, use NonlinearEquality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearEquality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearEquality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality","text":"Source code in opti/constraint.py class NonlinearInequality ( Constraint ): def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 def __repr__ ( self ): return f \"NonlinearInequality(' { self . expression } ')\" def to_config ( self ) -> Dict : return dict ( type = \"nonlinear-inequality\" , expression = self . expression )","title":"NonlinearInequality"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.__init__","text":"Inequality of the form 'expression <= 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 < 1, use NonlinearInequality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearInequality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearInequality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0","title":"satisfied()"},{"location":"ref-metric/","text":"Metrics crowding_distance ( A ) Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II Parameters: Name Type Description Default A 2D-array Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. required Returns: Type Description array Crowding distance indicator for each point in the front. Source code in opti/metric.py def crowding_distance ( A ): \"\"\"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference: [Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II](https://link.springer.com/chapter/10.1007/3-540-45356-3_83) Args: A (2D-array): Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. Returns: array: Crowding distance indicator for each point in the front. \"\"\" A = pareto_front ( A ) N , m = A . shape # no crowding distance for 2 points if N <= 2 : return np . full ( N , np . inf ) # sort points along each objective sort = np . argsort ( A , axis = 0 ) A = A [ sort , np . arange ( m )] # normalize all objectives norm = np . max ( A , axis = 0 ) - np . min ( A , axis = 0 ) A = A / norm A [:, norm == 0 ] = 0 # handle min = max # distance to previous and to next point along each objective d = np . diff ( A , axis = 0 ) inf = np . full (( 1 , m ), np . inf ) d0 = np . concatenate ([ inf , d ]) d1 = np . concatenate ([ d , inf ]) # TODO: handle cases with duplicate objective values leading to 0 distances # cuboid side length = distance between previous and next point unsort = np . argsort ( sort , axis = 0 ) cuboid = d0 [ unsort , np . arange ( m )] + d1 [ unsort , np . arange ( m )] return np . mean ( cuboid , axis = 1 ) generational_distance ( A , R , p = 1 , clip = True ) Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 clip bool Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. True Returns: Type Description float Generational distance indicator. Source code in opti/metric.py def generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 , clip : bool = True ) -> float : r \"\"\"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference: [David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art](http://dx.doi.org/10.1162/106365600568158) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. clip (bool, optional): Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. Returns: float: Generational distance indicator. \"\"\" A = pareto_front ( A ) distances = A [:, np . newaxis ] - R [ np . newaxis ] if clip : distances = distances . clip ( 0 , None ) distances = np . linalg . norm ( distances , axis = 2 ) . min ( axis = 1 ) return np . linalg . norm ( distances , p ) / len ( A ) inverted_generational_distance ( A , R , p = 1 ) Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 Returns: Type Description float Inverted generational distance indicator. Source code in opti/metric.py def inverted_generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 ) -> float : \"\"\"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference: [CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm](https://doi.org/10.1007/978-3-540-24694-7_71) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. Returns: float: Inverted generational distance indicator. \"\"\" return generational_distance ( R , A , p , clip = False ) is_pareto_efficient ( A ) Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 1D-array of bools Boolean mask for the Pareto efficient points in A. Source code in opti/metric.py def is_pareto_efficient ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 1D-array of bools: Boolean mask for the Pareto efficient points in A. \"\"\" efficient = np . ones ( len ( A ), dtype = bool ) idx = np . arange ( len ( A )) for i , a in enumerate ( A ): if not efficient [ i ]: continue # set all *other* efficent points to False, if they are not strictly better in at least one objective efficient [ efficient ] = np . any ( A [ efficient ] < a , axis = 1 ) | ( i == idx [ efficient ]) return efficient pareto_front ( A ) Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 2D-array Pareto efficient points in A. Source code in opti/metric.py def pareto_front ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 2D-array: Pareto efficient points in A. \"\"\" return A [ is_pareto_efficient ( A )]","title":"Metric"},{"location":"ref-metric/#metrics","text":"","title":"Metrics"},{"location":"ref-metric/#opti.metric.crowding_distance","text":"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II Parameters: Name Type Description Default A 2D-array Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. required Returns: Type Description array Crowding distance indicator for each point in the front. Source code in opti/metric.py def crowding_distance ( A ): \"\"\"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference: [Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II](https://link.springer.com/chapter/10.1007/3-540-45356-3_83) Args: A (2D-array): Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. Returns: array: Crowding distance indicator for each point in the front. \"\"\" A = pareto_front ( A ) N , m = A . shape # no crowding distance for 2 points if N <= 2 : return np . full ( N , np . inf ) # sort points along each objective sort = np . argsort ( A , axis = 0 ) A = A [ sort , np . arange ( m )] # normalize all objectives norm = np . max ( A , axis = 0 ) - np . min ( A , axis = 0 ) A = A / norm A [:, norm == 0 ] = 0 # handle min = max # distance to previous and to next point along each objective d = np . diff ( A , axis = 0 ) inf = np . full (( 1 , m ), np . inf ) d0 = np . concatenate ([ inf , d ]) d1 = np . concatenate ([ d , inf ]) # TODO: handle cases with duplicate objective values leading to 0 distances # cuboid side length = distance between previous and next point unsort = np . argsort ( sort , axis = 0 ) cuboid = d0 [ unsort , np . arange ( m )] + d1 [ unsort , np . arange ( m )] return np . mean ( cuboid , axis = 1 )","title":"crowding_distance()"},{"location":"ref-metric/#opti.metric.generational_distance","text":"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 clip bool Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. True Returns: Type Description float Generational distance indicator. Source code in opti/metric.py def generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 , clip : bool = True ) -> float : r \"\"\"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference: [David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art](http://dx.doi.org/10.1162/106365600568158) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. clip (bool, optional): Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. Returns: float: Generational distance indicator. \"\"\" A = pareto_front ( A ) distances = A [:, np . newaxis ] - R [ np . newaxis ] if clip : distances = distances . clip ( 0 , None ) distances = np . linalg . norm ( distances , axis = 2 ) . min ( axis = 1 ) return np . linalg . norm ( distances , p ) / len ( A )","title":"generational_distance()"},{"location":"ref-metric/#opti.metric.inverted_generational_distance","text":"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 Returns: Type Description float Inverted generational distance indicator. Source code in opti/metric.py def inverted_generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 ) -> float : \"\"\"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference: [CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm](https://doi.org/10.1007/978-3-540-24694-7_71) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. Returns: float: Inverted generational distance indicator. \"\"\" return generational_distance ( R , A , p , clip = False )","title":"inverted_generational_distance()"},{"location":"ref-metric/#opti.metric.is_pareto_efficient","text":"Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 1D-array of bools Boolean mask for the Pareto efficient points in A. Source code in opti/metric.py def is_pareto_efficient ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 1D-array of bools: Boolean mask for the Pareto efficient points in A. \"\"\" efficient = np . ones ( len ( A ), dtype = bool ) idx = np . arange ( len ( A )) for i , a in enumerate ( A ): if not efficient [ i ]: continue # set all *other* efficent points to False, if they are not strictly better in at least one objective efficient [ efficient ] = np . any ( A [ efficient ] < a , axis = 1 ) | ( i == idx [ efficient ]) return efficient","title":"is_pareto_efficient()"},{"location":"ref-metric/#opti.metric.pareto_front","text":"Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 2D-array Pareto efficient points in A. Source code in opti/metric.py def pareto_front ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 2D-array: Pareto efficient points in A. \"\"\" return A [ is_pareto_efficient ( A )]","title":"pareto_front()"},{"location":"ref-model/","text":"Models CustomModel ( Model ) Custom model for arbitrary functions. Source code in opti/model.py class CustomModel ( Model ): \"\"\"Custom model for arbitrary functions.\"\"\" def __init__ ( self , names : List [ str ], f : Callable ): super () . __init__ ( names ) self . f = f def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : return self . f ( df ) def __repr__ ( self ): return f \"CustomModel( { self . names } , f= { self . f } )\" LinearModel ( Model ) Model to compute an output as a linear/affine function of the inputs. Source code in opti/model.py class LinearModel ( Model ): \"\"\"Model to compute an output as a linear/affine function of the inputs.\"\"\" def __init__ ( self , names : List [ str ], coefficients , offset : float = 0 ): super () . __init__ ( names ) if len ( names ) > 1 : raise ValueError ( \"LinearModel can only describe a single output\" ) self . coefficients = coefficients self . offset = offset def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : y = df . to_numpy () @ self . coefficients + self . offset return pd . DataFrame ( y , columns = self . names ) def __repr__ ( self ): return f \"LinearModel( { self . names } , coefficients= { self . coefficients } , offset= { self . offset } )\" def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , ) to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , ) Model Source code in opti/model.py class Model : def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs \"\"\" for name in names : if not isinstance ( name , str ): ValueError ( \"Model: names must be a list of strings\" ) self . names = list ( names ) def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error __call__ ( self , df ) special Evaluate the objective values for a given DataFrame. Source code in opti/model.py def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError __init__ ( self , names ) special Base class for models of outputs as function of inputs. Parameters: Name Type Description Default names List[str] names of the modeled outputs required Source code in opti/model.py def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs \"\"\" for name in names : if not isinstance ( name , str ): ValueError ( \"Model: names must be a list of strings\" ) self . names = list ( names ) to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error Models Container for models. Source code in opti/model.py class Models : \"\"\"Container for models.\"\"\" def __init__ ( self , models : Union [ List [ Model ], List [ Dict ]]): _models = [] for m in models : if isinstance ( m , Model ): _models . append ( m ) else : _models . append ( make_model ( ** m )) self . models = _models def __call__ ( self , y : pd . DataFrame ) -> pd . DataFrame : return pd . concat ([ model ( y ) for model in self . models ], axis = 1 ) def __repr__ ( self ): return \"Models( \\n \" + pprint . pformat ( self . models ) + \" \\n )\" def __iter__ ( self ): return iter ( self . models ) def __len__ ( self ): return len ( self . models ) def __getitem__ ( self , i : int ) -> Model : return self . models [ i ] @property def names ( self ): names = [] for model in self . models : names += model . names return names def to_config ( self ) -> List [ Dict ]: return [ model . to_config () for model in self . models if model . to_config () is not None ]","title":"Model"},{"location":"ref-model/#models","text":"","title":"Models"},{"location":"ref-model/#opti.model.CustomModel","text":"Custom model for arbitrary functions. Source code in opti/model.py class CustomModel ( Model ): \"\"\"Custom model for arbitrary functions.\"\"\" def __init__ ( self , names : List [ str ], f : Callable ): super () . __init__ ( names ) self . f = f def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : return self . f ( df ) def __repr__ ( self ): return f \"CustomModel( { self . names } , f= { self . f } )\"","title":"CustomModel"},{"location":"ref-model/#opti.model.LinearModel","text":"Model to compute an output as a linear/affine function of the inputs. Source code in opti/model.py class LinearModel ( Model ): \"\"\"Model to compute an output as a linear/affine function of the inputs.\"\"\" def __init__ ( self , names : List [ str ], coefficients , offset : float = 0 ): super () . __init__ ( names ) if len ( names ) > 1 : raise ValueError ( \"LinearModel can only describe a single output\" ) self . coefficients = coefficients self . offset = offset def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : y = df . to_numpy () @ self . coefficients + self . offset return pd . DataFrame ( y , columns = self . names ) def __repr__ ( self ): return f \"LinearModel( { self . names } , coefficients= { self . coefficients } , offset= { self . offset } )\" def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , )","title":"LinearModel"},{"location":"ref-model/#opti.model.LinearModel.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , )","title":"to_config()"},{"location":"ref-model/#opti.model.Model","text":"Source code in opti/model.py class Model : def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs \"\"\" for name in names : if not isinstance ( name , str ): ValueError ( \"Model: names must be a list of strings\" ) self . names = list ( names ) def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error","title":"Model"},{"location":"ref-model/#opti.model.Model.__call__","text":"Evaluate the objective values for a given DataFrame. Source code in opti/model.py def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError","title":"__call__()"},{"location":"ref-model/#opti.model.Model.__init__","text":"Base class for models of outputs as function of inputs. Parameters: Name Type Description Default names List[str] names of the modeled outputs required Source code in opti/model.py def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs \"\"\" for name in names : if not isinstance ( name , str ): ValueError ( \"Model: names must be a list of strings\" ) self . names = list ( names )","title":"__init__()"},{"location":"ref-model/#opti.model.Model.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error","title":"to_config()"},{"location":"ref-model/#opti.model.Models","text":"Container for models. Source code in opti/model.py class Models : \"\"\"Container for models.\"\"\" def __init__ ( self , models : Union [ List [ Model ], List [ Dict ]]): _models = [] for m in models : if isinstance ( m , Model ): _models . append ( m ) else : _models . append ( make_model ( ** m )) self . models = _models def __call__ ( self , y : pd . DataFrame ) -> pd . DataFrame : return pd . concat ([ model ( y ) for model in self . models ], axis = 1 ) def __repr__ ( self ): return \"Models( \\n \" + pprint . pformat ( self . models ) + \" \\n )\" def __iter__ ( self ): return iter ( self . models ) def __len__ ( self ): return len ( self . models ) def __getitem__ ( self , i : int ) -> Model : return self . models [ i ] @property def names ( self ): names = [] for model in self . models : names += model . names return names def to_config ( self ) -> List [ Dict ]: return [ model . to_config () for model in self . models if model . to_config () is not None ]","title":"Models"},{"location":"ref-objective/","text":"Objectives CloseToTarget ( Objective ) Source code in opti/objective.py class CloseToTarget ( Objective ): def __init__ ( self , name : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: name: output to optimize target: target value exponent: exponent of the difference tolerance: distance to target below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target self . exponent = exponent self . tolerance = tolerance def __call__ ( self , y : pd . Series ) -> pd . Series : return ( y - self . target ) . abs () ** self . exponent - self . tolerance ** self . exponent def __repr__ ( self ): return f \"CloseToTarget(' { self . name } ', target= { self . target } )\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config __init__ ( self , name , target = 0 , exponent = 1 , tolerance = 0 ) special Objective for getting as close as possible to a given value. s(y) = |y - target| exponent - tolerance exponent Parameters: Name Type Description Default name str output to optimize required target float target value 0 exponent float exponent of the difference 1 tolerance float distance to target below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: name: output to optimize target: target value exponent: exponent of the difference tolerance: distance to target below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target self . exponent = exponent self . tolerance = tolerance to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config Maximize ( Objective ) Source code in opti/objective.py class Maximize ( Objective ): def __init__ ( self , name : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: output to maximize target: value above which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target def __call__ ( self , y : pd . Series ) -> pd . Series : return self . target - y def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return self . target - y def __repr__ ( self ): return f \"Maximize(' { self . name } ')\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config __init__ ( self , name , target = 0 ) special Maximization objective s(y) = target - y Parameters: Name Type Description Default name str output to maximize required target float value above which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: output to maximize target: value above which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config untransform ( self , y ) Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return self . target - y Minimize ( Objective ) Source code in opti/objective.py class Minimize ( Objective ): def __init__ ( self , name : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: name: output to minimize target: value below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target def __call__ ( self , y : pd . Series ) -> pd . Series : return y - self . target def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target def __repr__ ( self ): return f \"Minimize(' { self . name } ')\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config __init__ ( self , name , target = 0 ) special Minimization objective s(y) = y - target Parameters: Name Type Description Default name str output to minimize required target float value below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: name: output to minimize target: value below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config untransform ( self , y ) Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target Objective Source code in opti/objective.py class Objective : def __init__ ( self , name : str ): \"\"\"Base class for optimzation objectives.\"\"\" self . name = name def __call__ ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError __call__ ( self , df ) special Evaluate the objective values for given output values. Source code in opti/objective.py def __call__ ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError __init__ ( self , name ) special Base class for optimzation objectives. Source code in opti/objective.py def __init__ ( self , name : str ): \"\"\"Base class for optimzation objectives.\"\"\" self . name = name to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError Objectives Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) Source code in opti/objective.py class Objectives : \"\"\"Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) \"\"\" def __init__ ( self , objectives : Union [ List [ Objective ], List [ Dict ]]): _objectives = [] for m in objectives : if isinstance ( m , Objective ): _objectives . append ( m ) else : _objectives . append ( make_objective ( ** m )) self . objectives = _objectives def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : return pd . concat ([ obj ( df [ obj . name ]) for obj in self . objectives ], axis = 1 ) def __repr__ ( self ): return \"Objectives( \\n \" + pprint . pformat ( self . objectives ) + \" \\n )\" def __iter__ ( self ): return iter ( self . objectives ) def __len__ ( self ): return len ( self . objectives ) def __getitem__ ( self , i : int ) -> Objective : return self . objectives [ i ] @property def names ( self ): return [ obj . name for obj in self ] def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds def to_config ( self ) -> List [ Dict ]: return [ obj . to_config () for obj in self . objectives ] bounds ( self , outputs ) Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7) 2 for y in [0, 10] -> ideal = 0, nadir = 7 2 Parameters: Name Type Description Default outputs Parameters Output parameters. required Source code in opti/objective.py def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds make_objective ( type , name , ** kwargs ) Make an objective from a configuration. obj = make_objective(**config) Parameters: Name Type Description Default type str objective type required name str output to optimize required Source code in opti/objective.py def make_objective ( type : str , name : str , ** kwargs ) -> Objective : \"\"\"Make an objective from a configuration. ``` obj = make_objective(**config) ``` Args: type: objective type name: output to optimize \"\"\" objective = { \"minimize\" : Minimize , \"maximize\" : Maximize , \"close-to-target\" : CloseToTarget , }[ type . lower ()] return objective ( name , ** kwargs )","title":"Objective"},{"location":"ref-objective/#objectives","text":"","title":"Objectives"},{"location":"ref-objective/#opti.objective.CloseToTarget","text":"Source code in opti/objective.py class CloseToTarget ( Objective ): def __init__ ( self , name : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: name: output to optimize target: target value exponent: exponent of the difference tolerance: distance to target below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target self . exponent = exponent self . tolerance = tolerance def __call__ ( self , y : pd . Series ) -> pd . Series : return ( y - self . target ) . abs () ** self . exponent - self . tolerance ** self . exponent def __repr__ ( self ): return f \"CloseToTarget(' { self . name } ', target= { self . target } )\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config","title":"CloseToTarget"},{"location":"ref-objective/#opti.objective.CloseToTarget.__init__","text":"Objective for getting as close as possible to a given value. s(y) = |y - target| exponent - tolerance exponent Parameters: Name Type Description Default name str output to optimize required target float target value 0 exponent float exponent of the difference 1 tolerance float distance to target below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: name: output to optimize target: target value exponent: exponent of the difference tolerance: distance to target below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target self . exponent = exponent self . tolerance = tolerance","title":"__init__()"},{"location":"ref-objective/#opti.objective.CloseToTarget.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Maximize","text":"Source code in opti/objective.py class Maximize ( Objective ): def __init__ ( self , name : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: output to maximize target: value above which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target def __call__ ( self , y : pd . Series ) -> pd . Series : return self . target - y def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return self . target - y def __repr__ ( self ): return f \"Maximize(' { self . name } ')\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"Maximize"},{"location":"ref-objective/#opti.objective.Maximize.__init__","text":"Maximization objective s(y) = target - y Parameters: Name Type Description Default name str output to maximize required target float value above which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: output to maximize target: value above which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target","title":"__init__()"},{"location":"ref-objective/#opti.objective.Maximize.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Maximize.untransform","text":"Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return self . target - y","title":"untransform()"},{"location":"ref-objective/#opti.objective.Minimize","text":"Source code in opti/objective.py class Minimize ( Objective ): def __init__ ( self , name : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: name: output to minimize target: value below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target def __call__ ( self , y : pd . Series ) -> pd . Series : return y - self . target def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target def __repr__ ( self ): return f \"Minimize(' { self . name } ')\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"Minimize"},{"location":"ref-objective/#opti.objective.Minimize.__init__","text":"Minimization objective s(y) = y - target Parameters: Name Type Description Default name str output to minimize required target float value below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: name: output to minimize target: value below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target","title":"__init__()"},{"location":"ref-objective/#opti.objective.Minimize.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Minimize.untransform","text":"Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target","title":"untransform()"},{"location":"ref-objective/#opti.objective.Objective","text":"Source code in opti/objective.py class Objective : def __init__ ( self , name : str ): \"\"\"Base class for optimzation objectives.\"\"\" self . name = name def __call__ ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError","title":"Objective"},{"location":"ref-objective/#opti.objective.Objective.__call__","text":"Evaluate the objective values for given output values. Source code in opti/objective.py def __call__ ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError","title":"__call__()"},{"location":"ref-objective/#opti.objective.Objective.__init__","text":"Base class for optimzation objectives. Source code in opti/objective.py def __init__ ( self , name : str ): \"\"\"Base class for optimzation objectives.\"\"\" self . name = name","title":"__init__()"},{"location":"ref-objective/#opti.objective.Objective.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError","title":"to_config()"},{"location":"ref-objective/#opti.objective.Objectives","text":"Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) Source code in opti/objective.py class Objectives : \"\"\"Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) \"\"\" def __init__ ( self , objectives : Union [ List [ Objective ], List [ Dict ]]): _objectives = [] for m in objectives : if isinstance ( m , Objective ): _objectives . append ( m ) else : _objectives . append ( make_objective ( ** m )) self . objectives = _objectives def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : return pd . concat ([ obj ( df [ obj . name ]) for obj in self . objectives ], axis = 1 ) def __repr__ ( self ): return \"Objectives( \\n \" + pprint . pformat ( self . objectives ) + \" \\n )\" def __iter__ ( self ): return iter ( self . objectives ) def __len__ ( self ): return len ( self . objectives ) def __getitem__ ( self , i : int ) -> Objective : return self . objectives [ i ] @property def names ( self ): return [ obj . name for obj in self ] def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds def to_config ( self ) -> List [ Dict ]: return [ obj . to_config () for obj in self . objectives ]","title":"Objectives"},{"location":"ref-objective/#opti.objective.Objectives.bounds","text":"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7) 2 for y in [0, 10] -> ideal = 0, nadir = 7 2 Parameters: Name Type Description Default outputs Parameters Output parameters. required Source code in opti/objective.py def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds","title":"bounds()"},{"location":"ref-objective/#opti.objective.make_objective","text":"Make an objective from a configuration. obj = make_objective(**config) Parameters: Name Type Description Default type str objective type required name str output to optimize required Source code in opti/objective.py def make_objective ( type : str , name : str , ** kwargs ) -> Objective : \"\"\"Make an objective from a configuration. ``` obj = make_objective(**config) ``` Args: type: objective type name: output to optimize \"\"\" objective = { \"minimize\" : Minimize , \"maximize\" : Maximize , \"close-to-target\" : CloseToTarget , }[ type . lower ()] return objective ( name , ** kwargs )","title":"make_objective()"},{"location":"ref-parameter/","text":"Parameters Categorical ( Parameter ) Categorical parameter (nominal scale, values cannot be put into order). Attributes: Name Type Description name str name of the parameter domain list list possible values Source code in opti/parameter.py class Categorical ( Parameter ): \"\"\"Categorical parameter (nominal scale, values cannot be put into order). Attributes: name (str): name of the parameter domain (list): list possible values \"\"\" def __init__ ( self , name : str , domain : Sequence , ** kwargs ): if not isinstance ( domain , list ): raise ValueError ( \"domain must be of type list\" ) if len ( domain ) < 2 : raise ValueError ( \"domain must a least contain 2 values\" ) if len ( set ( domain )) != len ( domain ): raise ValueError ( \"domain contains duplicates\" ) super () . __init__ ( name , domain , type = \"categorical\" , ** kwargs ) def __repr__ ( self ): return f \"Categorical(' { self . name } ', domain= { self . domain } )\" def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( \"cannot round categorical variable\" ) return point def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float ) def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s def to_dummy_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain [ 1 :]}, dtype = float , ) def from_dummy_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from dummy encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols [ 1 :] for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) points = points . copy () points [ cat_cols [ 0 ]] = 1 - points [ cat_cols [ 1 :]] . sum ( axis = 1 ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index ) contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) from_dummy_encoding ( self , points ) Convert points back from dummy encoding. Source code in opti/parameter.py def from_dummy_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from dummy encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols [ 1 :] for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) points = points . copy () points [ cat_cols [ 0 ]] = 1 - points [ cat_cols [ 1 :]] . sum ( axis = 1 ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s from_label_encoding ( self , points ) Convert points back from label-encoding. Source code in opti/parameter.py def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index ) from_onehot_encoding ( self , points ) Convert points back from one-hot encoding. Source code in opti/parameter.py def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point . Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( \"cannot round categorical variable\" ) return point sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) to_dummy_encoding ( self , points ) Convert points to a dummy-hot encoding, dropping the first categorical level. Source code in opti/parameter.py def to_dummy_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain [ 1 :]}, dtype = float , ) to_label_encoding ( self , points ) Convert points to label-encoding. Source code in opti/parameter.py def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s to_onehot_encoding ( self , points ) Convert points to a one-hot encoding. Source code in opti/parameter.py def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float ) Continuous ( Parameter ) Parameter that can take on any real value in the specified domain. Attributes: Name Type Description name str name of the parameter domain list [lower bound, upper bound] Source code in opti/parameter.py class Continuous ( Parameter ): \"\"\"Parameter that can take on any real value in the specified domain. Attributes: name (str): name of the parameter domain (list): [lower bound, upper bound] \"\"\" def __init__ ( self , name : str , domain : Optional [ Sequence ] = None , ** kwargs , ): if domain is None : domain = [ - np . inf , np . inf ] else : if len ( domain ) != 2 : raise ValueError ( \"domain needs to consist of two values [low, high]\" ) # convert None to inf (json doesn't support infinity) low = - np . inf if domain [ 0 ] is None else domain [ 0 ] high = np . inf if domain [ 1 ] is None else domain [ 1 ] if high < low : raise ValueError ( f \"lower bound { low } must be less than upper bound { high } \" ) self . low = low self . high = high super () . __init__ ( name = name , domain = [ low , high ], type = \"continuous\" , ** kwargs ) def __repr__ ( self ): if np . isfinite ( self . low ) or np . isfinite ( self . high ): return f \"Continuous(' { self . name } ', domain= { self . domain } )\" else : return f \"Continuous(' { self . name } ')\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return self . low , self . high def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high ) def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n )) def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type ) low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) if low is not None or high is not None : conf . update ({ \"domain\" : [ low , high ]}) conf . update ( self . extra_fields ) return conf def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1].\"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0-1] -> [low, high].\"\"\" if np . isclose ( self . low , self . high ): return np . ones_like ( points ) * self . high else : return points * ( self . high - self . low ) + self . low bounds : Tuple [ float , float ] property readonly Return the domain bounds. contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high ) from_unit_range ( self , points ) Transform points from the unit range: [0-1] -> [low, high]. Source code in opti/parameter.py def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0-1] -> [low, high].\"\"\" if np . isclose ( self . low , self . high ): return np . ones_like ( points ) * self . high else : return points * ( self . high - self . low ) + self . low round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high ) sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n )) to_config ( self ) Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type ) low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) if low is not None or high is not None : conf . update ({ \"domain\" : [ low , high ]}) conf . update ( self . extra_fields ) return conf to_unit_range ( self , points ) Transform points to the unit range: [low, high] -> [0, 1]. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1].\"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) Discrete ( Parameter ) Discrete parameter (ordinal scale). Attributes: Name Type Description name str name of the parameter domain list list of possible numeric values Source code in opti/parameter.py class Discrete ( Parameter ): \"\"\"Discrete parameter (ordinal scale). Attributes: name (str): name of the parameter domain (list): list of possible numeric values \"\"\" def __init__ ( self , name : str , domain : Sequence , ** kwargs ): if len ( domain ) < 1 : raise ValueError ( \"domain must contain at least 1 value\" ) for d in domain : if not isinstance ( d , numbers . Number ): raise ValueError ( \"domain contains non-numeric values\" ) if len ( set ( domain )) != len ( domain ): raise ValueError ( \"domain contains duplicates\" ) # convert to python-native dtype to ensure json-serializability domain = np . array ( np . sort ( domain )) . tolist () self . low = min ( domain ) self . high = max ( domain ) super () . __init__ ( name , domain , type = \"discrete\" , ** kwargs ) def __repr__ ( self ): return f \"Discrete(' { self . name } ', domain= { self . domain } )\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return self . low , self . high def is_integer ( self ): \"\"\"Check if the domain is an integer range, such as [1, 2, 3]\"\"\" if isinstance ( self . low , int ) and isinstance ( self . high , int ): return self . domain == list ( range ( self . low , self . high + 1 )) else : return False def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index ) def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) def to_unit_range ( self , points ): \"\"\"Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) bounds : Tuple [ float , float ] property readonly Return the domain bounds. contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) is_integer ( self ) Check if the domain is an integer range, such as [1, 2, 3] Source code in opti/parameter.py def is_integer ( self ): \"\"\"Check if the domain is an integer range, such as [1, 2, 3]\"\"\" if isinstance ( self . low , int ) and isinstance ( self . high , int ): return self . domain == list ( range ( self . low , self . high + 1 )) else : return False round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index ) sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) to_unit_range ( self , points ) Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) Parameter Parameter base class. Source code in opti/parameter.py class Parameter : \"\"\"Parameter base class.\"\"\" def __init__ ( self , name : str , domain : Sequence , type : str = None , ** kwargs ): self . name = name self . domain = domain self . type = type self . extra_fields = kwargs def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf to_config ( self ) Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf Parameters Set of parameters representing either the input or the output space. Source code in opti/parameter.py class Parameters : \"\"\"Set of parameters representing either the input or the output space.\"\"\" def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Space expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d def __repr__ ( self ): return \"Parameters( \\n \" + pprint . pformat ( list ( self . parameters . values ())) + \" \\n )\" def __iter__ ( self ): return iter ( self . parameters . values ()) def __getitem__ ( self , name ): return self . parameters [ name ] def __len__ ( self ): return len ( self . parameters ) def __add__ ( self , other ): parameters = list ( self . parameters . values ()) + list ( other . parameters . values ()) return Parameters ( parameters ) @property def names ( self ): return list ( self . parameters . keys ()) @property def bounds ( self ) -> pd . DataFrame : \"\"\"Return the parameter bounds.\"\"\" for param in self : if isinstance ( param , Categorical ): raise TypeError ( \"Contains categorical parameters which are not bounded.\" ) return pd . DataFrame ({ p . name : p . bounds for p in self }, index = [ \"min\" , \"max\" ]) def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the space in each parameter.\"\"\" if isinstance ( points , pd . DataFrame ): points = points [ self . names ] b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis = 1 ) return b . all ( axis = 1 ) def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 ) def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 ) def transform ( self , points : pd . DataFrame , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"onehot-encode\" , ) -> pd . DataFrame : transformed = [] for p in self : s = points [ p . name ] if isinstance ( p , Continuous ): if continuous == \"none\" : transformed . append ( s ) elif continuous == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown continuous transform { continuous } \" ) if isinstance ( p , Discrete ): if discrete == \"none\" : transformed . append ( s ) elif discrete == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown discrete transform { continuous } \" ) if isinstance ( p , Categorical ): if categorical == \"none\" : transformed . append ( s ) elif categorical == \"onehot-encode\" : transformed . append ( p . to_onehot_encoding ( s )) elif categorical == \"dummy-encode\" : transformed . append ( p . to_dummy_encoding ( s )) elif categorical == \"label-encode\" : transformed . append ( p . to_label_encoding ( s )) else : raise ValueError ( f \"Unknown categorical transform { continuous } \" ) return pd . concat ( transformed , axis = 1 ) def to_config ( self ) -> List [ dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()] bounds : DataFrame property readonly Return the parameter bounds. __init__ ( self , parameters ) special It can be constructed either from a list / tuple (of at least one) Parameter objects Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) or from a list / tuple of dicts Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) In particular, Parameters(). init and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf Source code in opti/parameter.py def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Space expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d contains ( self , points ) Check if points are inside the space in each parameter. Source code in opti/parameter.py def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the space in each parameter.\"\"\" if isinstance ( points , pd . DataFrame ): points = points [ self . names ] b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis = 1 ) return b . all ( axis = 1 ) round ( self , points ) Round points to the closest contained values. Source code in opti/parameter.py def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 ) sample ( self , n = 1 ) Draw uniformly distributed random samples. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 ) to_config ( self ) Configuration of the parameter space. Source code in opti/parameter.py def to_config ( self ) -> List [ dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()] make_parameter ( name , type , domain = None , ** kwargs ) Make a parameter object from a configuration p = make_parameter(**config) Parameters: Name Type Description Default type str \"continuous\", \"discrete\" or \"categorical\" required name str Name of the parameter required domain list Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] None Source code in opti/parameter.py def make_parameter ( name : str , type : str , domain : Optional [ Sequence ] = None , ** kwargs , ): \"\"\"Make a parameter object from a configuration p = make_parameter(**config) Args: type (str): \"continuous\", \"discrete\" or \"categorical\" name (str): Name of the parameter domain (list): Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] \"\"\" parameter = { \"continuous\" : Continuous , \"discrete\" : Discrete , \"categorical\" : Categorical , }[ type . lower ()] if domain is None and parameter is not Continuous : raise ValueError ( f \"domain not given for parameter { name } \" ) return parameter ( name = name , domain = domain , ** kwargs )","title":"Parameter"},{"location":"ref-parameter/#parameters","text":"","title":"Parameters"},{"location":"ref-parameter/#opti.parameter.Categorical","text":"Categorical parameter (nominal scale, values cannot be put into order). Attributes: Name Type Description name str name of the parameter domain list list possible values Source code in opti/parameter.py class Categorical ( Parameter ): \"\"\"Categorical parameter (nominal scale, values cannot be put into order). Attributes: name (str): name of the parameter domain (list): list possible values \"\"\" def __init__ ( self , name : str , domain : Sequence , ** kwargs ): if not isinstance ( domain , list ): raise ValueError ( \"domain must be of type list\" ) if len ( domain ) < 2 : raise ValueError ( \"domain must a least contain 2 values\" ) if len ( set ( domain )) != len ( domain ): raise ValueError ( \"domain contains duplicates\" ) super () . __init__ ( name , domain , type = \"categorical\" , ** kwargs ) def __repr__ ( self ): return f \"Categorical(' { self . name } ', domain= { self . domain } )\" def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( \"cannot round categorical variable\" ) return point def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float ) def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s def to_dummy_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain [ 1 :]}, dtype = float , ) def from_dummy_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from dummy encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols [ 1 :] for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) points = points . copy () points [ cat_cols [ 0 ]] = 1 - points [ cat_cols [ 1 :]] . sum ( axis = 1 ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index )","title":"Categorical"},{"location":"ref-parameter/#opti.parameter.Categorical.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Categorical.from_dummy_encoding","text":"Convert points back from dummy encoding. Source code in opti/parameter.py def from_dummy_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from dummy encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols [ 1 :] for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) points = points . copy () points [ cat_cols [ 0 ]] = 1 - points [ cat_cols [ 1 :]] . sum ( axis = 1 ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s","title":"from_dummy_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.from_label_encoding","text":"Convert points back from label-encoding. Source code in opti/parameter.py def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index )","title":"from_label_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.from_onehot_encoding","text":"Convert points back from one-hot encoding. Source code in opti/parameter.py def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \"Column names don't match categorical levels: { points . columns } , { cat_cols } \" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s","title":"from_onehot_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point . Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( \"cannot round categorical variable\" ) return point","title":"round()"},{"location":"ref-parameter/#opti.parameter.Categorical.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Categorical.to_dummy_encoding","text":"Convert points to a dummy-hot encoding, dropping the first categorical level. Source code in opti/parameter.py def to_dummy_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain [ 1 :]}, dtype = float , )","title":"to_dummy_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.to_label_encoding","text":"Convert points to label-encoding. Source code in opti/parameter.py def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s","title":"to_label_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.to_onehot_encoding","text":"Convert points to a one-hot encoding. Source code in opti/parameter.py def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float )","title":"to_onehot_encoding()"},{"location":"ref-parameter/#opti.parameter.Continuous","text":"Parameter that can take on any real value in the specified domain. Attributes: Name Type Description name str name of the parameter domain list [lower bound, upper bound] Source code in opti/parameter.py class Continuous ( Parameter ): \"\"\"Parameter that can take on any real value in the specified domain. Attributes: name (str): name of the parameter domain (list): [lower bound, upper bound] \"\"\" def __init__ ( self , name : str , domain : Optional [ Sequence ] = None , ** kwargs , ): if domain is None : domain = [ - np . inf , np . inf ] else : if len ( domain ) != 2 : raise ValueError ( \"domain needs to consist of two values [low, high]\" ) # convert None to inf (json doesn't support infinity) low = - np . inf if domain [ 0 ] is None else domain [ 0 ] high = np . inf if domain [ 1 ] is None else domain [ 1 ] if high < low : raise ValueError ( f \"lower bound { low } must be less than upper bound { high } \" ) self . low = low self . high = high super () . __init__ ( name = name , domain = [ low , high ], type = \"continuous\" , ** kwargs ) def __repr__ ( self ): if np . isfinite ( self . low ) or np . isfinite ( self . high ): return f \"Continuous(' { self . name } ', domain= { self . domain } )\" else : return f \"Continuous(' { self . name } ')\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return self . low , self . high def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high ) def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n )) def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type ) low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) if low is not None or high is not None : conf . update ({ \"domain\" : [ low , high ]}) conf . update ( self . extra_fields ) return conf def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1].\"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0-1] -> [low, high].\"\"\" if np . isclose ( self . low , self . high ): return np . ones_like ( points ) * self . high else : return points * ( self . high - self . low ) + self . low","title":"Continuous"},{"location":"ref-parameter/#opti.parameter.Continuous.bounds","text":"Return the domain bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Continuous.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Continuous.from_unit_range","text":"Transform points from the unit range: [0-1] -> [low, high]. Source code in opti/parameter.py def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0-1] -> [low, high].\"\"\" if np . isclose ( self . low , self . high ): return np . ones_like ( points ) * self . high else : return points * ( self . high - self . low ) + self . low","title":"from_unit_range()"},{"location":"ref-parameter/#opti.parameter.Continuous.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Continuous.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Continuous.to_config","text":"Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type ) low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) if low is not None or high is not None : conf . update ({ \"domain\" : [ low , high ]}) conf . update ( self . extra_fields ) return conf","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.Continuous.to_unit_range","text":"Transform points to the unit range: [low, high] -> [0, 1]. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1].\"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low )","title":"to_unit_range()"},{"location":"ref-parameter/#opti.parameter.Discrete","text":"Discrete parameter (ordinal scale). Attributes: Name Type Description name str name of the parameter domain list list of possible numeric values Source code in opti/parameter.py class Discrete ( Parameter ): \"\"\"Discrete parameter (ordinal scale). Attributes: name (str): name of the parameter domain (list): list of possible numeric values \"\"\" def __init__ ( self , name : str , domain : Sequence , ** kwargs ): if len ( domain ) < 1 : raise ValueError ( \"domain must contain at least 1 value\" ) for d in domain : if not isinstance ( d , numbers . Number ): raise ValueError ( \"domain contains non-numeric values\" ) if len ( set ( domain )) != len ( domain ): raise ValueError ( \"domain contains duplicates\" ) # convert to python-native dtype to ensure json-serializability domain = np . array ( np . sort ( domain )) . tolist () self . low = min ( domain ) self . high = max ( domain ) super () . __init__ ( name , domain , type = \"discrete\" , ** kwargs ) def __repr__ ( self ): return f \"Discrete(' { self . name } ', domain= { self . domain } )\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return self . low , self . high def is_integer ( self ): \"\"\"Check if the domain is an integer range, such as [1, 2, 3]\"\"\" if isinstance ( self . low , int ) and isinstance ( self . high , int ): return self . domain == list ( range ( self . low , self . high + 1 )) else : return False def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index ) def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) def to_unit_range ( self , points ): \"\"\"Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low )","title":"Discrete"},{"location":"ref-parameter/#opti.parameter.Discrete.bounds","text":"Return the domain bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Discrete.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Discrete.is_integer","text":"Check if the domain is an integer range, such as [1, 2, 3] Source code in opti/parameter.py def is_integer ( self ): \"\"\"Check if the domain is an integer range, such as [1, 2, 3]\"\"\" if isinstance ( self . low , int ) and isinstance ( self . high , int ): return self . domain == list ( range ( self . low , self . high + 1 )) else : return False","title":"is_integer()"},{"location":"ref-parameter/#opti.parameter.Discrete.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Discrete.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Discrete.to_unit_range","text":"Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range. Note, if the given points are not inside the domain of the parameter, the transformed will not be inside the unit range. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low )","title":"to_unit_range()"},{"location":"ref-parameter/#opti.parameter.Parameter","text":"Parameter base class. Source code in opti/parameter.py class Parameter : \"\"\"Parameter base class.\"\"\" def __init__ ( self , name : str , domain : Sequence , type : str = None , ** kwargs ): self . name = name self . domain = domain self . type = type self . extra_fields = kwargs def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf","title":"Parameter"},{"location":"ref-parameter/#opti.parameter.Parameter.to_config","text":"Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.Parameters","text":"Set of parameters representing either the input or the output space. Source code in opti/parameter.py class Parameters : \"\"\"Set of parameters representing either the input or the output space.\"\"\" def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Space expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d def __repr__ ( self ): return \"Parameters( \\n \" + pprint . pformat ( list ( self . parameters . values ())) + \" \\n )\" def __iter__ ( self ): return iter ( self . parameters . values ()) def __getitem__ ( self , name ): return self . parameters [ name ] def __len__ ( self ): return len ( self . parameters ) def __add__ ( self , other ): parameters = list ( self . parameters . values ()) + list ( other . parameters . values ()) return Parameters ( parameters ) @property def names ( self ): return list ( self . parameters . keys ()) @property def bounds ( self ) -> pd . DataFrame : \"\"\"Return the parameter bounds.\"\"\" for param in self : if isinstance ( param , Categorical ): raise TypeError ( \"Contains categorical parameters which are not bounded.\" ) return pd . DataFrame ({ p . name : p . bounds for p in self }, index = [ \"min\" , \"max\" ]) def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the space in each parameter.\"\"\" if isinstance ( points , pd . DataFrame ): points = points [ self . names ] b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis = 1 ) return b . all ( axis = 1 ) def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 ) def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 ) def transform ( self , points : pd . DataFrame , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"onehot-encode\" , ) -> pd . DataFrame : transformed = [] for p in self : s = points [ p . name ] if isinstance ( p , Continuous ): if continuous == \"none\" : transformed . append ( s ) elif continuous == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown continuous transform { continuous } \" ) if isinstance ( p , Discrete ): if discrete == \"none\" : transformed . append ( s ) elif discrete == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown discrete transform { continuous } \" ) if isinstance ( p , Categorical ): if categorical == \"none\" : transformed . append ( s ) elif categorical == \"onehot-encode\" : transformed . append ( p . to_onehot_encoding ( s )) elif categorical == \"dummy-encode\" : transformed . append ( p . to_dummy_encoding ( s )) elif categorical == \"label-encode\" : transformed . append ( p . to_label_encoding ( s )) else : raise ValueError ( f \"Unknown categorical transform { continuous } \" ) return pd . concat ( transformed , axis = 1 ) def to_config ( self ) -> List [ dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()]","title":"Parameters"},{"location":"ref-parameter/#opti.parameter.Parameters.bounds","text":"Return the parameter bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Parameters.__init__","text":"It can be constructed either from a list / tuple (of at least one) Parameter objects Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) or from a list / tuple of dicts Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) In particular, Parameters(). init and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf Source code in opti/parameter.py def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Space([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Space([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Space expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d","title":"__init__()"},{"location":"ref-parameter/#opti.parameter.Parameters.contains","text":"Check if points are inside the space in each parameter. Source code in opti/parameter.py def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the space in each parameter.\"\"\" if isinstance ( points , pd . DataFrame ): points = points [ self . names ] b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis = 1 ) return b . all ( axis = 1 )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Parameters.round","text":"Round points to the closest contained values. Source code in opti/parameter.py def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Parameters.sample","text":"Draw uniformly distributed random samples. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 )","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Parameters.to_config","text":"Configuration of the parameter space. Source code in opti/parameter.py def to_config ( self ) -> List [ dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()]","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.make_parameter","text":"Make a parameter object from a configuration p = make_parameter(**config) Parameters: Name Type Description Default type str \"continuous\", \"discrete\" or \"categorical\" required name str Name of the parameter required domain list Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] None Source code in opti/parameter.py def make_parameter ( name : str , type : str , domain : Optional [ Sequence ] = None , ** kwargs , ): \"\"\"Make a parameter object from a configuration p = make_parameter(**config) Args: type (str): \"continuous\", \"discrete\" or \"categorical\" name (str): Name of the parameter domain (list): Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] \"\"\" parameter = { \"continuous\" : Continuous , \"discrete\" : Discrete , \"categorical\" : Categorical , }[ type . lower ()] if domain is None and parameter is not Continuous : raise ValueError ( f \"domain not given for parameter { name } \" ) return parameter ( name = name , domain = domain , ** kwargs )","title":"make_parameter()"},{"location":"ref-problem/","text":"Problem Problem Source code in opti/problem.py class Problem : def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ DataFrameLike ] = None , optima : Optional [ DataFrameLike ] = None , name : Optional [ str ] = None , ** kwargs , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f if isinstance ( data , dict ): data = pd . DataFrame ( ** data ) if isinstance ( optima , dict ): optima = pd . DataFrame ( ** optima ) self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models () @property def n_inputs ( self ) -> int : return len ( self . inputs ) @property def n_outputs ( self ) -> int : return len ( self . outputs ) @property def n_objectives ( self ) -> int : return len ( self . objectives ) @property def n_constraints ( self ) -> int : return 0 if self . constraints is None else len ( self . constraints ) def __repr__ ( self ): return self . __str__ () def __str__ ( self ): s = \"Problem( \\n \" s += f \"name= { self . name } , \\n \" s += f \"inputs= { self . inputs } , \\n \" s += f \"outputs= { self . outputs } , \\n \" s += f \"objectives= { self . objectives } , \\n \" if self . output_constraints is not None : s += f \"output_constraints= { self . output_constraints } , \\n \" if self . constraints is not None : s += f \"constraints= { self . constraints } , \\n \" if self . models is not None : s += f \"models= { self . models } , \\n \" if self . data is not None : s += f \"data= \\n { self . data . head () } \\n \" if self . optima is not None : s += f \"optima= \\n { self . optima . head () } \\n \" return s + \")\" @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" return Problem ( ** config ) def to_config ( self ) -> dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem ( ** config ) def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , separators = ( \",\" , \":\" )) outfile . write ( b . encode ( \"utf-8\" )) def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check for duplicate names duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if all objectives refer to an output for obj in self . objectives : if obj . name not in self . outputs . names : raise ValueError ( f \"Objective refers to unknown parameter: { obj . name } \" ) def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # check if parameter is present if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } not in data.\" ) # check for non-numeric values for continuous / discrete parameters if isinstance ( p , ( Continuous , Discrete )): if not is_numeric_dtype ( data [ p . name ]): raise ValueError ( f \"Non-numeric data for parameter { p . name } .\" ) # check for undefined categories for categorical parameters elif isinstance ( p , Categorical ): if not p . contains ( data [ p . name ]) . all (): raise ValueError ( f \"Unknown category for parameter { p . name } .\" ) # inputs need to be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Missing values for input parameter { p . name } .\" ) # outputs need to have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"No value for output parameter { p . name } in data.\" ) def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" ) def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : self . check_data ( data ) self . data = data def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 ) def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints ) def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\" if self . f is None : raise NotImplementedError ( \"problem.f is not implemented for the problem.\" ) X = self . sample_inputs ( n_samples ) Y = self . f ( X ) self . data = pd . concat ([ X , Y ], axis = 1 ) __init__ ( self , inputs , outputs , objectives = None , constraints = None , output_constraints = None , f = None , models = None , data = None , optima = None , name = None , ** kwargs ) special An optimization problem. Parameters: Name Type Description Default inputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Input parameters. required outputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Output parameters. required objectives Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Optimization objectives. Defaults to minimization. None constraints Union[opti.constraint.Constraints, List[opti.constraint.Constraint], List[Dict]] Constraints on the inputs. None output_constraints Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Constraints on the outputs. None f Optional[Callable] Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame None data Union[pandas.core.frame.DataFrame, Dict] Experimental data. None optima Union[pandas.core.frame.DataFrame, Dict] Pareto optima. None name Optional[str] Name of the problem. None Source code in opti/problem.py def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ DataFrameLike ] = None , optima : Optional [ DataFrameLike ] = None , name : Optional [ str ] = None , ** kwargs , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f if isinstance ( data , dict ): data = pd . DataFrame ( ** data ) if isinstance ( optima , dict ): optima = pd . DataFrame ( ** optima ) self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models () add_data ( self , data ) Add a number of data points. Source code in opti/problem.py def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 ) check_data ( self , data ) Check if data is consistent with input and output parameters. Source code in opti/problem.py def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # check if parameter is present if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } not in data.\" ) # check for non-numeric values for continuous / discrete parameters if isinstance ( p , ( Continuous , Discrete )): if not is_numeric_dtype ( data [ p . name ]): raise ValueError ( f \"Non-numeric data for parameter { p . name } .\" ) # check for undefined categories for categorical parameters elif isinstance ( p , Categorical ): if not p . contains ( data [ p . name ]) . all (): raise ValueError ( f \"Unknown category for parameter { p . name } .\" ) # inputs need to be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Missing values for input parameter { p . name } .\" ) # outputs need to have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"No value for output parameter { p . name } in data.\" ) check_models ( self ) Check if the models are well defined Source code in opti/problem.py def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" ) check_problem ( self ) Check if input and output parameters are consistent. Source code in opti/problem.py def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check for duplicate names duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if all objectives refer to an output for obj in self . objectives : if obj . name not in self . outputs . names : raise ValueError ( f \"Objective refers to unknown parameter: { obj . name } \" ) create_initial_data ( self , n_samples = 10 ) Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs. Source code in opti/problem.py def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\" if self . f is None : raise NotImplementedError ( \"problem.f is not implemented for the problem.\" ) X = self . sample_inputs ( n_samples ) Y = self . f ( X ) self . data = pd . concat ([ X , Y ], axis = 1 ) from_config ( config ) staticmethod Create a Problem instance from a configuration dict. Source code in opti/problem.py @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" return Problem ( ** config ) from_json ( fname ) staticmethod Read a problem from a JSON file. Source code in opti/problem.py @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem ( ** config ) get_X ( self , data = None ) Return the input values in data or self.data . Source code in opti/problem.py def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values get_XY ( self , outputs = None , data = None , continuous = 'none' , discrete = 'none' , categorical = 'none' ) Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Parameters: Name Type Description Default outputs optional Subset of the outputs to consider. None data optional Dataframe to consider instead of problem.data None Source code in opti/problem.py def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y get_X_bounds ( self ) Return the lower and upper data bounds. Source code in opti/problem.py def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi get_Y ( self , data = None ) Return the output values in data or self.data . Source code in opti/problem.py def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values get_data ( self ) Return self.data if it exists or an empty dataframe. Source code in opti/problem.py def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data sample_inputs ( self , n_samples = 10 ) Uniformly sample points from the input space subject to the constraints. Source code in opti/problem.py def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints ) set_data ( self , data ) Set the data. Source code in opti/problem.py def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : self . check_data ( data ) self . data = data set_optima ( self , optima ) Set the optima / Pareto front. Source code in opti/problem.py def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima to_config ( self ) Return json-serializable configuration dict. Source code in opti/problem.py def to_config ( self ) -> dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config to_json ( self , fname ) Save a problem from a JSON file. Source code in opti/problem.py def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , separators = ( \",\" , \":\" )) outfile . write ( b . encode ( \"utf-8\" )) read_json ( filepath ) Read a problem specification from a JSON file. Source code in opti/problem.py def read_json ( filepath : PathLike ) -> Problem : \"\"\"Read a problem specification from a JSON file.\"\"\" return Problem . from_json ( filepath )","title":"Problem"},{"location":"ref-problem/#problem","text":"","title":"Problem"},{"location":"ref-problem/#opti.problem.Problem","text":"Source code in opti/problem.py class Problem : def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ DataFrameLike ] = None , optima : Optional [ DataFrameLike ] = None , name : Optional [ str ] = None , ** kwargs , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f if isinstance ( data , dict ): data = pd . DataFrame ( ** data ) if isinstance ( optima , dict ): optima = pd . DataFrame ( ** optima ) self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models () @property def n_inputs ( self ) -> int : return len ( self . inputs ) @property def n_outputs ( self ) -> int : return len ( self . outputs ) @property def n_objectives ( self ) -> int : return len ( self . objectives ) @property def n_constraints ( self ) -> int : return 0 if self . constraints is None else len ( self . constraints ) def __repr__ ( self ): return self . __str__ () def __str__ ( self ): s = \"Problem( \\n \" s += f \"name= { self . name } , \\n \" s += f \"inputs= { self . inputs } , \\n \" s += f \"outputs= { self . outputs } , \\n \" s += f \"objectives= { self . objectives } , \\n \" if self . output_constraints is not None : s += f \"output_constraints= { self . output_constraints } , \\n \" if self . constraints is not None : s += f \"constraints= { self . constraints } , \\n \" if self . models is not None : s += f \"models= { self . models } , \\n \" if self . data is not None : s += f \"data= \\n { self . data . head () } \\n \" if self . optima is not None : s += f \"optima= \\n { self . optima . head () } \\n \" return s + \")\" @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" return Problem ( ** config ) def to_config ( self ) -> dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem ( ** config ) def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , separators = ( \",\" , \":\" )) outfile . write ( b . encode ( \"utf-8\" )) def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check for duplicate names duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if all objectives refer to an output for obj in self . objectives : if obj . name not in self . outputs . names : raise ValueError ( f \"Objective refers to unknown parameter: { obj . name } \" ) def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # check if parameter is present if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } not in data.\" ) # check for non-numeric values for continuous / discrete parameters if isinstance ( p , ( Continuous , Discrete )): if not is_numeric_dtype ( data [ p . name ]): raise ValueError ( f \"Non-numeric data for parameter { p . name } .\" ) # check for undefined categories for categorical parameters elif isinstance ( p , Categorical ): if not p . contains ( data [ p . name ]) . all (): raise ValueError ( f \"Unknown category for parameter { p . name } .\" ) # inputs need to be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Missing values for input parameter { p . name } .\" ) # outputs need to have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"No value for output parameter { p . name } in data.\" ) def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" ) def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : self . check_data ( data ) self . data = data def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 ) def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints ) def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\" if self . f is None : raise NotImplementedError ( \"problem.f is not implemented for the problem.\" ) X = self . sample_inputs ( n_samples ) Y = self . f ( X ) self . data = pd . concat ([ X , Y ], axis = 1 )","title":"Problem"},{"location":"ref-problem/#opti.problem.Problem.__init__","text":"An optimization problem. Parameters: Name Type Description Default inputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Input parameters. required outputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Output parameters. required objectives Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Optimization objectives. Defaults to minimization. None constraints Union[opti.constraint.Constraints, List[opti.constraint.Constraint], List[Dict]] Constraints on the inputs. None output_constraints Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Constraints on the outputs. None f Optional[Callable] Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame None data Union[pandas.core.frame.DataFrame, Dict] Experimental data. None optima Union[pandas.core.frame.DataFrame, Dict] Pareto optima. None name Optional[str] Name of the problem. None Source code in opti/problem.py def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ DataFrameLike ] = None , optima : Optional [ DataFrameLike ] = None , name : Optional [ str ] = None , ** kwargs , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f if isinstance ( data , dict ): data = pd . DataFrame ( ** data ) if isinstance ( optima , dict ): optima = pd . DataFrame ( ** optima ) self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models ()","title":"__init__()"},{"location":"ref-problem/#opti.problem.Problem.add_data","text":"Add a number of data points. Source code in opti/problem.py def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 )","title":"add_data()"},{"location":"ref-problem/#opti.problem.Problem.check_data","text":"Check if data is consistent with input and output parameters. Source code in opti/problem.py def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # check if parameter is present if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } not in data.\" ) # check for non-numeric values for continuous / discrete parameters if isinstance ( p , ( Continuous , Discrete )): if not is_numeric_dtype ( data [ p . name ]): raise ValueError ( f \"Non-numeric data for parameter { p . name } .\" ) # check for undefined categories for categorical parameters elif isinstance ( p , Categorical ): if not p . contains ( data [ p . name ]) . all (): raise ValueError ( f \"Unknown category for parameter { p . name } .\" ) # inputs need to be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Missing values for input parameter { p . name } .\" ) # outputs need to have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"No value for output parameter { p . name } in data.\" )","title":"check_data()"},{"location":"ref-problem/#opti.problem.Problem.check_models","text":"Check if the models are well defined Source code in opti/problem.py def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" )","title":"check_models()"},{"location":"ref-problem/#opti.problem.Problem.check_problem","text":"Check if input and output parameters are consistent. Source code in opti/problem.py def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check for duplicate names duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if all objectives refer to an output for obj in self . objectives : if obj . name not in self . outputs . names : raise ValueError ( f \"Objective refers to unknown parameter: { obj . name } \" )","title":"check_problem()"},{"location":"ref-problem/#opti.problem.Problem.create_initial_data","text":"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs. Source code in opti/problem.py def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\" if self . f is None : raise NotImplementedError ( \"problem.f is not implemented for the problem.\" ) X = self . sample_inputs ( n_samples ) Y = self . f ( X ) self . data = pd . concat ([ X , Y ], axis = 1 )","title":"create_initial_data()"},{"location":"ref-problem/#opti.problem.Problem.from_config","text":"Create a Problem instance from a configuration dict. Source code in opti/problem.py @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" return Problem ( ** config )","title":"from_config()"},{"location":"ref-problem/#opti.problem.Problem.from_json","text":"Read a problem from a JSON file. Source code in opti/problem.py @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem ( ** config )","title":"from_json()"},{"location":"ref-problem/#opti.problem.Problem.get_X","text":"Return the input values in data or self.data . Source code in opti/problem.py def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values","title":"get_X()"},{"location":"ref-problem/#opti.problem.Problem.get_XY","text":"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Parameters: Name Type Description Default outputs optional Subset of the outputs to consider. None data optional Dataframe to consider instead of problem.data None Source code in opti/problem.py def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y","title":"get_XY()"},{"location":"ref-problem/#opti.problem.Problem.get_X_bounds","text":"Return the lower and upper data bounds. Source code in opti/problem.py def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi","title":"get_X_bounds()"},{"location":"ref-problem/#opti.problem.Problem.get_Y","text":"Return the output values in data or self.data . Source code in opti/problem.py def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values","title":"get_Y()"},{"location":"ref-problem/#opti.problem.Problem.get_data","text":"Return self.data if it exists or an empty dataframe. Source code in opti/problem.py def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data","title":"get_data()"},{"location":"ref-problem/#opti.problem.Problem.sample_inputs","text":"Uniformly sample points from the input space subject to the constraints. Source code in opti/problem.py def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints )","title":"sample_inputs()"},{"location":"ref-problem/#opti.problem.Problem.set_data","text":"Set the data. Source code in opti/problem.py def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : self . check_data ( data ) self . data = data","title":"set_data()"},{"location":"ref-problem/#opti.problem.Problem.set_optima","text":"Set the optima / Pareto front. Source code in opti/problem.py def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima","title":"set_optima()"},{"location":"ref-problem/#opti.problem.Problem.to_config","text":"Return json-serializable configuration dict. Source code in opti/problem.py def to_config ( self ) -> dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config","title":"to_config()"},{"location":"ref-problem/#opti.problem.Problem.to_json","text":"Save a problem from a JSON file. Source code in opti/problem.py def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , separators = ( \",\" , \":\" )) outfile . write ( b . encode ( \"utf-8\" ))","title":"to_json()"},{"location":"ref-problem/#opti.problem.read_json","text":"Read a problem specification from a JSON file. Source code in opti/problem.py def read_json ( filepath : PathLike ) -> Problem : \"\"\"Read a problem specification from a JSON file.\"\"\" return Problem . from_json ( filepath )","title":"read_json()"},{"location":"ref-problems/","text":"Test Problems benchmark Daechert1 ( Problem ) Problem with a non-convex Pareto front. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. Source code in opti/problems/benchmark.py class Daechert1 ( Problem ): \"\"\"Problem with a non-convex Pareto front. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-1\" , inputs = [ Continuous ( \"x1\" , domain = [ 0 , np . pi ]), Continuous ( \"x2\" , domain = [ 0 , 10 ]), Continuous ( \"x3\" , domain = [ 1.2 , 10 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], constraints = [ NonlinearInequality ( \"- cos(x1) - exp(-x2) + x3\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y1\" : - X [ \"x1\" ], \"y2\" : - X [ \"x2\" ], \"y3\" : - X [ \"x3\" ] ** 2 }) Daechert2 ( Problem ) Unconstrained problem with a Pareto front resembling a comet. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. Source code in opti/problems/benchmark.py class Daechert2 ( Problem ): \"\"\"Unconstrained problem with a Pareto front resembling a comet. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-2\" , inputs = [ Continuous ( \"x1\" , domain = [ 1 , 3.5 ]), Continuous ( \"x2\" , domain = [ - 2 , 2 ]), Continuous ( \"x3\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y1\" : X . eval ( \"(1 + x3) * (x1**3 * x2**2 - 10 * x1 - 4 * x2)\" ), \"y2\" : X . eval ( \"(1 + x3) * (x1**3 * x2**2 - 10 * x1 + 4 * x2)\" ), \"y3\" : X . eval ( \"3 * (1 + x3) * x1**2\" ), } ) Daechert3 ( Problem ) Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. Source code in opti/problems/benchmark.py class Daechert3 ( Problem ): \"\"\"Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-3\" , inputs = [ Continuous ( f \"x { i + 1 } \" , domain = [ 0 , 1 ]) for i in range ( 2 )], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . values return pd . DataFrame ( { \"y1\" : X [ \"x1\" ], \"y2\" : X [ \"x2\" ], \"y3\" : 6 - np . sum ( x * ( 1 + np . sin ( 3 * np . pi * x )), axis = 1 ), } ) Hyperellipsoid ( Problem ) Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Parameters: Name Type Description Default n int Dimension of the hyperellipsoid. Defaults to 5. 5 a list-like Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. None Source code in opti/problems/benchmark.py class Hyperellipsoid ( Problem ): \"\"\"Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Args: n (int, optional): Dimension of the hyperellipsoid. Defaults to 5. a (list-like, optional): Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. \"\"\" def __init__ ( self , n : int = 5 , a : Optional [ Union [ list , np . ndarray ]] = None ): if a is None : a = np . ones ( n ) constr = \" + \" . join ([ f \"x { i + 1 } **2\" for i in range ( n )]) + \" - 1\" else : a = np . array ( a ) . squeeze () if len ( a ) != n : raise ValueError ( \"Dimension of half axes doesn't match input dimension\" ) constr = \" + \" . join ([ f \"(x { i + 1 } / { a [ i ] } )**2\" for i in range ( n )]) + \" - 1\" self . a = a super () . __init__ ( name = \"Hyperellipsoid\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - a [ i ], a [ i ]]) for i in range ( n )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - a [ i ], a [ i ]]) for i in range ( n )], constraints = [ NonlinearInequality ( constr )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : y = X [ self . inputs . names ] y . columns = self . outputs . names return y def get_optima ( self , n = 10 ) -> pd . DataFrame : X = opti . sampling . sphere . sample ( self . n_inputs , n , positive = True ) X = np . concatenate ([ - np . eye ( self . n_inputs ), - X ], axis = 0 )[: n ] Y = self . a * X return pd . DataFrame ( data = np . column_stack ([ X , Y ]), columns = self . inputs . names + self . outputs . names , ) Qapi1 ( Problem ) Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. Source code in opti/problems/benchmark.py class Qapi1 ( Problem ): \"\"\"Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Constrained bi-objective problem\" , inputs = [ Continuous ( \"x1\" , [ 0 , 10 ]), Continuous ( \"x2\" , [ - 10 , 10 ])], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], constraints = [ NonlinearInequality ( \"x2 - x1**2\" ), NonlinearInequality ( \"2 - x1 - x2\" ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y1\" : X . eval ( \"(x1 - 2)**2 + (x2 - 1)**2\" ), \"y2\" : X . eval ( \"x1**2 + (x2 - 3)**2\" ), } ) datasets Datasets: Problems with observed data but no underlying ground truth. Alkox ( Problem ) Alkox reaction optimization. Source code in opti/problems/datasets.py class Alkox ( Problem ): \"\"\"Alkox reaction optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Alkox\" , inputs = [ Continuous ( \"residence_time\" , domain = [ 0.05 , 1 ]), Continuous ( \"ratio\" , domain = [ 0.5 , 10 ]), Continuous ( \"concentration\" , domain = [ 2 , 8 ]), Continuous ( \"temperature\" , domain = [ 6 , 8 ]), ], outputs = [ Continuous ( \"conversion\" )], objectives = [ Maximize ( \"conversion\" )], data = get_data ( \"alkox.csv\" ), ) BaumgartnerAniline ( Problem ) Baumgartner Aniline optimization. Source code in opti/problems/datasets.py class BaumgartnerAniline ( Problem ): \"\"\"Baumgartner Aniline optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Baumgartner 2019 - Aniline Cross-Coupling\" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"tBuXPhos\" , \"tBuBrettPhos\" , \"AlPhos\" ]), Categorical ( \"base\" , domain = [ \"TEA\" , \"TMG\" , \"BTMG\" , \"DBU\" ]), Continuous ( \"base_equivalents\" , domain = [ 1.0 , 2.5 ]), Continuous ( \"temperature\" , domain = [ 30 , 100 ]), Continuous ( \"residence_time\" , domain = [ 60 , 1800 ]), ], outputs = [ Continuous ( \"yield\" , domain = [ 0 , 1 ])], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"baumgartner_aniline.csv\" ), ) BaumgartnerBenzamide ( Problem ) Benzamide optimization, Baumgartner 2019. Source code in opti/problems/datasets.py class BaumgartnerBenzamide ( Problem ): \"\"\"Benzamide optimization, Baumgartner 2019.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Baumgartner 2019 - Benzamide Cross-Coupling\" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"tBuXPhos\" , \"tBuBrettPhos\" ]), Categorical ( \"base\" , domain = [ \"TMG\" , \"BTMG\" , \"DBU\" , \"MTBD\" ]), Continuous ( \"base_equivalents\" , domain = [ 1.0 , 2.1 ]), Continuous ( \"temperature\" , domain = [ 30 , 100 ]), Continuous ( \"residence_time\" , domain = [ 60 , 1850 ]), ], outputs = [ Continuous ( \"yield\" , domain = [ 0 , 1 ])], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"baumgartner_benzamide.csv\" ), ) Benzylation ( Problem ) Benzylation optimization. Source code in opti/problems/datasets.py class Benzylation ( Problem ): \"\"\"Benzylation optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Benzylation\" , inputs = [ Continuous ( \"flow_rate\" , domain = [ 0.2 , 0.4 ]), Continuous ( \"ratio\" , domain = [ 1.0 , 5.0 ]), Continuous ( \"solvent\" , domain = [ 0.5 , 1.0 ]), Continuous ( \"temperature\" , domain = [ 110.0 , 150.0 ]), ], outputs = [ Continuous ( \"impurity\" )], objectives = [ Minimize ( \"impurity\" )], data = get_data ( \"benzylation.csv\" ), ) Cake ( Problem ) Cake recipe optimization with mixed objectives. Source code in opti/problems/datasets.py class Cake ( Problem ): \"\"\"Cake recipe optimization with mixed objectives.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Cake\" , inputs = [ Continuous ( \"wheat_flour\" , domain = [ 0 , 1 ]), Continuous ( \"spelt_flour\" , domain = [ 0 , 1 ]), Continuous ( \"sugar\" , domain = [ 0 , 1 ]), Continuous ( \"chocolate\" , domain = [ 0 , 1 ]), Continuous ( \"nuts\" , domain = [ 0 , 1 ]), Continuous ( \"carrot\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( \"calories\" , domain = [ 300 , 600 ]), Continuous ( \"taste\" , domain = [ 0 , 5 ]), Continuous ( \"browning\" , domain = [ 0 , 2 ]), ], objectives = [ Minimize ( \"calories\" ), Maximize ( \"taste\" ), CloseToTarget ( \"browning\" , target = 1.4 ), ], constraints = [ LinearEquality ( [ \"wheat_flour\" , \"spelt_flour\" , \"sugar\" , \"chocolate\" , \"nuts\" , \"carrot\" , ], rhs = 1 , ) ], data = get_data ( \"cake.csv\" ), ) Fullerenes ( Problem ) Fullerene recation optimization. Source code in opti/problems/datasets.py class Fullerenes ( Problem ): \"\"\"Fullerene recation optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Fullerenes\" , inputs = [ Continuous ( \"reaction_time\" , domain = [ 3.0 , 31.0 ]), Continuous ( \"sultine\" , domain = [ 1.5 , 6.0 ]), Continuous ( \"temperature\" , domain = [ 100.0 , 150.0 ]), ], outputs = [ Continuous ( \"product\" )], objectives = [ Maximize ( \"product\" )], data = get_data ( \"fullerenes.csv\" ), ) HPLC ( Problem ) High-performance liquid chromatography optimization. Source code in opti/problems/datasets.py class HPLC ( Problem ): \"\"\"High-performance liquid chromatography optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"HPLC\" , inputs = [ Continuous ( \"sample_loop\" , domain = [ 0.0 , 0.08 ]), Continuous ( \"additional_volume\" , domain = [ 0.0 , 0.06 ]), Continuous ( \"tubing_volume\" , domain = [ 0.1 , 0.9 ]), Continuous ( \"sample_flow\" , domain = [ 0.5 , 2.5 ]), Continuous ( \"push_speed\" , domain = [ 80.0 , 150 ]), Continuous ( \"wait_time\" , domain = [ 0.5 , 10.0 ]), ], outputs = [ Continuous ( \"peak_area\" )], objectives = [ Maximize ( \"peak_area\" )], data = get_data ( \"hplc.csv\" ), ) Photodegradation ( Problem ) Photodegration minimization. Source code in opti/problems/datasets.py class Photodegradation ( Problem ): \"\"\"Photodegration minimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Photodegradation\" , inputs = [ Continuous ( \"PCE10\" , domain = [ 0 , 1 ]), Continuous ( \"WF3\" , domain = [ 0 , 1 ]), Continuous ( \"P3HT\" , domain = [ 0 , 1 ]), Continuous ( \"PCBM\" , domain = [ 0 , 1 ]), Continuous ( \"oIDTBR\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( \"degradation\" )], objectives = [ Minimize ( \"degradation\" )], constraints = [ LinearEquality ( [ \"PCE10\" , \"WF3\" , \"P3HT\" , \"PCBM\" , \"oIDTBR\" ], rhs = 1 , lhs = 1 ), NChooseK ([ \"PCE10\" , \"WF3\" ], max_active = 1 ), ], data = get_data ( \"photodegradation.csv\" ), ) ReizmanSuzuki ( Problem ) Suzuki reaction optimization, Reizmann 2016. Source code in opti/problems/datasets.py class ReizmanSuzuki ( Problem ): \"\"\"Suzuki reaction optimization, Reizmann 2016.\"\"\" def __init__ ( self , case = 1 ): assert case in [ 1 , 2 , 3 , 4 ] super () . __init__ ( name = f \"Reizman 2016 - Suzuki Case { case } \" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"P1-L1\" , \"P2-L1\" , \"P1-L2\" , \"P1-L3\" , \"P1-L4\" , \"P1-L5\" , \"P1-L6\" , \"P1-L7\" , ], ), Continuous ( \"t_res\" , domain = [ 60 , 600 ]), Continuous ( \"temperature\" , domain = [ 30 , 110 ]), Continuous ( \"catalyst_loading\" , domain = [ 0.496 , 2.515 ]), ], outputs = [ Continuous ( \"ton\" , domain = [ 0 , 100 ]), Continuous ( \"yield\" , domain = [ 0 , 100 ]), ], objectives = [ Maximize ( \"ton\" ), Maximize ( \"yield\" )], data = get_data ( f \"reizman_suzuki { case } .csv\" ), ) SnAr ( Problem ) SnAr reaction optimization. Source code in opti/problems/datasets.py class SnAr ( Problem ): \"\"\"SnAr reaction optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"SnAr\" , inputs = [ Continuous ( \"residence_time\" , domain = [ 0.5 , 2.0 ]), Continuous ( \"ratio\" , domain = [ 1.0 , 5.0 ]), Continuous ( \"concentration\" , domain = [ 0.1 , 0.5 ]), Continuous ( \"temperature\" , domain = [ 60.0 , 140.0 ]), ], outputs = [ Continuous ( \"impurity\" )], objectives = [ Minimize ( \"impurity\" )], data = get_data ( \"snar.csv\" ), ) Suzuki ( Problem ) Suzuki reaction optimization. Source code in opti/problems/datasets.py class Suzuki ( Problem ): \"\"\"Suzuki reaction optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Suzuki\" , inputs = [ Continuous ( \"temperature\" , domain = [ 75.0 , 90.0 ]), Continuous ( \"pd_mol\" , domain = [ 0.5 , 5.0 ]), Continuous ( \"arbpin\" , domain = [ 1.0 , 1.8 ]), Continuous ( \"k3po4\" , domain = [ 1.5 , 3.0 ]), ], outputs = [ Continuous ( \"yield\" )], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"suzuki.csv\" ), ) detergent Detergent ( Problem ) Constrained problem with 5 inputs and 5 outputs. Each output is modeled as a second degree polynomial. The sixth input is a filler (water) and is factored out using the formulation constraint sum x = 1, and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. Source code in opti/problems/detergent.py class Detergent ( Problem ): \"\"\"Constrained problem with 5 inputs and 5 outputs. Each output is modeled as a second degree polynomial. The sixth input is a filler (water) and is factored out using the formulation constraint sum x = 1, and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. \"\"\" def __init__ ( self ): # coefficients for the 2-order polynomial; generated with # base = 3 * np.ones((1, 5)) # scale = PolynomialFeatures(degree=2).fit_transform(base).T # coef = np.random.RandomState(42).normal(scale=scale, size=(len(scale), 5)) # coef = np.clip(coef, 0, None) self . coef = np . array ( [ [ 0.4967 , 0.0 , 0.6477 , 1.523 , 0.0 ], [ 0.0 , 4.7376 , 2.3023 , 0.0 , 1.6277 ], [ 0.0 , 0.0 , 0.7259 , 0.0 , 0.0 ], [ 0.0 , 0.0 , 0.9427 , 0.0 , 0.0 ], [ 4.3969 , 0.0 , 0.2026 , 0.0 , 0.0 ], [ 0.3328 , 0.0 , 1.1271 , 0.0 , 0.0 ], [ 0.0 , 16.6705 , 0.0 , 0.0 , 7.4029 ], [ 0.0 , 1.8798 , 0.0 , 0.0 , 1.7718 ], [ 6.6462 , 1.5423 , 0.0 , 0.0 , 0.0 ], [ 0.0 , 0.0 , 9.5141 , 3.0926 , 0.0 ], [ 2.9168 , 0.0 , 0.0 , 5.5051 , 9.279 ], [ 8.3815 , 0.0 , 0.0 , 2.9814 , 8.7799 ], [ 0.0 , 0.0 , 0.0 , 0.0 , 7.3127 ], [ 12.2062 , 0.0 , 9.0318 , 3.2547 , 0.0 ], [ 3.2526 , 13.8423 , 0.0 , 14.0818 , 0.0 ], [ 7.3971 , 0.7834 , 0.0 , 0.8258 , 0.0 ], [ 0.0 , 3.214 , 13.301 , 0.0 , 0.0 ], [ 0.0 , 8.2386 , 2.9588 , 0.0 , 4.6194 ], [ 0.8737 , 8.7178 , 0.0 , 0.0 , 0.0 ], [ 0.0 , 2.6651 , 2.3495 , 0.046 , 0.0 ], [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ], ] ) super () . __init__ ( name = \"Detergent\" , inputs = [ Continuous ( \"x1\" , domain = [ 0.0 , 0.2 ]), Continuous ( \"x2\" , domain = [ 0.0 , 0.3 ]), Continuous ( \"x3\" , domain = [ 0.02 , 0.2 ]), Continuous ( \"x4\" , domain = [ 0.0 , 0.06 ]), Continuous ( \"x5\" , domain = [ 0.0 , 0.04 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" , domain = [ 0 , 3 ]) for i in range ( 5 )], objectives = [ Maximize ( f \"y { i + 1 } \" ) for i in range ( 5 )], constraints = [ LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"x5\" ], lhs =- 1 , rhs =- 0.2 ), LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"x5\" ], lhs = 1 , rhs = 0.4 ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = np . atleast_2d ( X [ self . inputs . names ]) xp = np . stack ([ _poly2 ( xi ) for xi in x ], axis = 0 ) return pd . DataFrame ( xp @ self . coef , columns = self . outputs . names , index = X . index ) Detergent_NChooseKConstraint ( Problem ) Variant of the Detergent problem with an n-choose-k constraint Source code in opti/problems/detergent.py class Detergent_NChooseKConstraint ( Problem ): \"\"\"Variant of the Detergent problem with an n-choose-k constraint\"\"\" def __init__ ( self ): base = Detergent () super () . __init__ ( name = \"Detergent with n-choose-k constraint\" , inputs = base . inputs , outputs = base . outputs , objectives = base . objectives , constraints = list ( base . constraints ) + [ NChooseK ( names = base . inputs . names , max_active = 3 )], f = base . f , ) Detergent_OutputConstraint ( Problem ) Variant of the Detergent problem with an output constraint. There are 5 inputs, 6 outputs: - 3 outputs are to be maximized - 1 output represents the stability of the formulation (0: not stable, 1: stable) Source code in opti/problems/detergent.py class Detergent_OutputConstraint ( Problem ): \"\"\"Variant of the Detergent problem with an output constraint. There are 5 inputs, 6 outputs: - 3 outputs are to be maximized - 1 output represents the stability of the formulation (0: not stable, 1: stable) \"\"\" def __init__ ( self , discrete = False ): base = Detergent () def f ( X ): Y = base . f ( X ) if discrete : Y [ \"stable\" ] = ( X . sum ( axis = 1 ) < 0.3 ) . astype ( int ) else : Y [ \"stable\" ] = ( 0.4 - X . sum ( axis = 1 )) / 0.2 return Y outputs = list ( base . outputs ) if discrete : outputs += [ Discrete ( \"stable\" , domain = [ 0 , 1 ])] else : outputs += [ Continuous ( \"stable\" , domain = [ 0 , 1 ])] super () . __init__ ( name = \"Detergent with stability constraint\" , inputs = base . inputs , outputs = outputs , objectives = base . objectives , output_constraints = [ Maximize ( \"stable\" , target = 0.5 )], constraints = base . constraints , f = f , ) mixed Mixed variables single and multi-objective test problems. DiscreteFuelInjector ( Problem ) Fuel injector test problem, modified to contain an integer variable. 4 objectives, mixed variables, unconstrained See Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 Source code in opti/problems/mixed.py class DiscreteFuelInjector ( Problem ): \"\"\"Fuel injector test problem, modified to contain an integer variable. * 4 objectives, * mixed variables, unconstrained See: Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Discrete fuel injector test problem\" , inputs = [ Discrete ( \"x1\" , [ 0 , 1 , 2 , 3 ]), Continuous ( \"x2\" , [ - 2 , 2 ]), Continuous ( \"x3\" , [ - 2 , 2 ]), Continuous ( \"x4\" , [ - 2 , 2 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 4 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 = X [ \"x1\" ] . to_numpy () . astype ( float ) x2 = X [ \"x2\" ] . to_numpy () . astype ( float ) x3 = X [ \"x3\" ] . to_numpy () . astype ( float ) x4 = X [ \"x4\" ] . to_numpy () . astype ( float ) x1 *= 0.2 y1 = ( 0.692 + 0.4771 * x1 - 0.687 * x4 - 0.08 * x3 - 0.065 * x2 - 0.167 * x1 ** 2 - 0.0129 * x1 * x4 + 0.0796 * x4 ** 2 - 0.0634 * x1 * x3 - 0.0257 * x3 * x4 + 0.0877 * x3 ** 2 - 0.0521 * x1 * x2 + 0.00156 * x2 * x4 + 0.00198 * x2 * x3 + 0.0184 * x2 ** 2 ) y2 = ( 0.37 - 0.205 * x1 + 0.0307 * x4 + 0.108 * x3 + 1.019 * x2 - 0.135 * x1 ** 2 + 0.0141 * x1 * x4 + 0.0998 * x4 ** 2 + 0.208 * x1 * x3 - 0.0301 * x3 * x4 - 0.226 * x3 ** 2 + 0.353 * x1 * x2 - 0.0497 * x2 * x3 - 0.423 * x2 ** 2 + 0.202 * x1 ** 2 * x4 - 0.281 * x1 ** 2 * x3 - 0.342 * x1 * x4 ** 2 - 0.245 * x3 * x4 ** 2 + 0.281 * x3 ** 2 * x4 - 0.184 * x1 * x2 ** 2 + 0.281 * x1 * x3 * x4 ) y3 = ( 0.153 - 0.322 * x1 + 0.396 * x4 + 0.424 * x3 + 0.0226 * x2 + 0.175 * x1 ** 2 + 0.0185 * x1 * x4 - 0.0701 * x4 ** 2 - 0.251 * x1 * x3 + 0.179 * x3 * x4 + 0.015 * x3 ** 2 + 0.0134 * x1 * x2 + 0.0296 * x2 * x4 + 0.0752 * x2 * x3 + 0.0192 * x2 ** 2 ) y4 = ( 0.758 + 0.358 * x1 - 0.807 * x4 + 0.0925 * x3 - 0.0468 * x2 - 0.172 * x1 ** 2 + 0.0106 * x1 * x4 + 0.0697 * x4 ** 2 - 0.146 * x1 * x3 - 0.0416 * x3 * x4 + 0.102 * x3 ** 2 - 0.0694 * x1 * x2 - 0.00503 * x2 * x4 + 0.0151 * x2 * x3 + 0.0173 * x2 ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 , \"y3\" : y3 , \"y4\" : y4 }) DiscreteVLMOP2 ( Problem ) VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. 2 minimization objectives 1 categorical and n continuous inputs, unconstrained See Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 Source code in opti/problems/mixed.py class DiscreteVLMOP2 ( Problem ): \"\"\"VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. * 2 minimization objectives * 1 categorical and n continuous inputs, unconstrained See: Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 \"\"\" def __init__ ( self , n_inputs : int = 3 ): assert n_inputs >= 2 super () . __init__ ( name = \"Discrete VLMOP2 test problem\" , inputs = [ Categorical ( \"x1\" , [ \"a\" , \"b\" ])] + [ Continuous ( f \"x { i + 1 } \" , [ - 2 , 2 ]) for i in range ( 1 , n_inputs )], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : d = X [ self . inputs . names [ 0 ]] . values x = X [ self . inputs . names [ 1 :]] . values n = self . n_inputs y1 = np . exp ( - np . sum (( x - n ** - 0.5 ) ** 2 , axis = 1 )) y2 = np . exp ( - np . sum (( x + n ** - 0.5 ) ** 2 , axis = 1 )) y1 = np . where ( d == \"a\" , 1 - y1 , 1.25 - y1 ) y2 = np . where ( d == \"a\" , 1 - y2 , 0.75 - y2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) single Single objective benchmark problems. Ackley ( Problem ) Ackley benchmark problem. Source code in opti/problems/single.py class Ackley ( Problem ): \"\"\"Ackley benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Ackley problem\" , inputs = [ Continuous ( f \"x { i } \" , [ - 32.768 , + 32.768 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : a = 20 b = 1 / 5 c = 2 * np . pi n = self . n_inputs x = self . get_X ( X ) part1 = - a * np . exp ( - b * np . sqrt (( 1 / n ) * np . sum ( x ** 2 , axis =- 1 ))) part2 = - np . exp (( 1 / n ) * np . sum ( np . cos ( c * x ), axis =- 1 )) y = part1 + part2 + a + np . exp ( 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Himmelblau ( Problem ) Himmelblau benchmark problem Source code in opti/problems/single.py class Himmelblau ( Problem ): \"\"\"Himmelblau benchmark problem\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Himmelblau function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 6 , 6 ]) for i in range ( 2 )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x0 , x1 = self . get_X ( X ) . T y = ( x0 ** 2 + x1 - 11 ) ** 2 + ( x0 + x1 ** 2 - 7 ) ** 2 return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . array ( [ [ 3.0 , 2.0 ], [ - 2.805118 , 3.131312 ], [ - 3.779310 , - 3.283186 ], [ 3.584428 , - 1.848126 ], ] ) y = np . zeros ( 4 ) return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Rastrigin ( Problem ) Rastrigin benchmark problem. Source code in opti/problems/single.py class Rastrigin ( Problem ): \"\"\"Rastrigin benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Rastrigin function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 5 , 5 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) a = 10 y = a * self . n_inputs + np . sum ( x ** 2 - a * np . cos ( 2 * np . pi * x ), axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Rosenbrock ( Problem ) Rosenbrock benchmark problem. Source code in opti/problems/single.py class Rosenbrock ( Problem ): \"\"\"Rosenbrock benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Rosenbrock function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 2.048 , 2.048 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) . T y = np . sum ( 100 * ( x [ 1 :] - x [: - 1 ] ** 2 ) ** 2 + ( 1 - x [: - 1 ]) ** 2 , axis = 0 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . ones (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Schwefel ( Problem ) Schwefel benchmark problem Source code in opti/problems/single.py class Schwefel ( Problem ): \"\"\"Schwefel benchmark problem\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Schwefel function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 500 , 500 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) y = 418.9829 * self . n_inputs - np . sum ( x * np . sin ( np . abs ( x ) ** 0.5 ), axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . full (( 1 , self . n_inputs ), 420.9687 ) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Sphere ( Problem ) Sphere benchmark problem. Source code in opti/problems/single.py class Sphere ( Problem ): \"\"\"Sphere benchmark problem.\"\"\" def __init__ ( self , n_inputs = 10 ): super () . __init__ ( name = \"Sphere function\" , inputs = [ Continuous ( f \"x { i } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ 0 , 2 ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) y = np . sum (( x - 0.5 ) ** 2 , axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . full (( 1 , self . n_inputs ), 0.5 ) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Zakharov ( Problem ) Zakharov benchmark problem. Source code in opti/problems/single.py class Zakharov ( Problem ): \"\"\"Zakharov benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Zakharov function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 10 , 10 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ): x = self . get_X ( X ) a = 0.5 * np . sum ( np . arange ( 1 , self . n_inputs + 1 ) * x , axis = 1 ) y = np . sum ( x ** 2 , axis = 1 ) + a ** 2 + a ** 4 return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Zakharov_Categorical ( Problem ) Zakharov problem with one categorical input Source code in opti/problems/single.py class Zakharov_Categorical ( Problem ): \"\"\"Zakharov problem with one categorical input\"\"\" def __init__ ( self , n_inputs = 3 ): base = Zakharov ( n_inputs ) super () . __init__ ( name = \"Zakharov function with one categorical input\" , inputs = [ Continuous ( f \"x { i } \" , [ - 10 , 10 ]) for i in range ( n_inputs - 1 )] + [ Categorical ( \"expon_switch\" , [ \"one\" , \"two\" ])], outputs = base . outputs , ) def f ( self , X : pd . DataFrame ): x_conti = X [ self . inputs . names [: - 1 ]] . values # just the continuous inputs a = 0.5 * np . sum ( np . arange ( 1 , self . n_inputs ) * x_conti , axis = 1 ) powers = np . repeat ( np . expand_dims ([ 2.0 , 2.0 , 4.0 ], 0 ), repeats = len ( X ), axis = 0 ) modify_powers = X [ self . inputs . names [ - 1 ]] == \"two\" powers [ modify_powers , :] += powers [ modify_powers , :] res = ( np . sum ( x_conti ** np . expand_dims ( powers [:, 0 ], 1 ), axis = 1 ) + a ** np . expand_dims ( powers [:, 1 ], 0 ) + a ** np . expand_dims ( powers [:, 2 ], 0 ) ) res_float_array = np . array ( res , dtype = np . float64 ) . ravel () y = res_float_array return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = list ( np . zeros ( self . n_inputs - 1 )) + [ \"one\" ] y = [ 0 ] return pd . DataFrame ([ x + y ], columns = self . inputs . names + self . outputs . names ) Zakharov_Constrained ( Problem ) Zakharov problem with one linear constraint Source code in opti/problems/single.py class Zakharov_Constrained ( Problem ): \"\"\"Zakharov problem with one linear constraint\"\"\" def __init__ ( self , n_inputs = 5 ): base = Zakharov ( n_inputs ) self . base = base super () . __init__ ( name = \"Zakharov with one linear constraint\" , inputs = base . inputs , outputs = base . outputs , constraints = [ LinearInequality ( base . inputs . names , lhs = 1 , rhs = 10 )], f = base . f , ) def get_optima ( self ): return self . base . get_optima () Zakharov_NChooseKConstraint ( Problem ) Zakharov problem with an n-choose-k constraint Source code in opti/problems/single.py class Zakharov_NChooseKConstraint ( Problem ): \"\"\"Zakharov problem with an n-choose-k constraint\"\"\" def __init__ ( self , n_inputs = 5 , n_max_active = 3 ): base = Zakharov ( n_inputs ) self . base = base super () . __init__ ( name = \"Zakharov with n-choose-k constraint\" , inputs = base . inputs , outputs = base . outputs , constraints = [ NChooseK ( names = base . inputs . names , max_active = n_max_active )], f = base . f , ) def get_optima ( self ): return self . base . get_optima () univariate Simple 1D problems for assessing probabilistic surrogate models. Note: these problems should be output-noisified, e.g. import opti problem = opti.problems.noisify_problem_with_gaussian( opti.problems.Line1D(), sigma=0.1 ) Line1D ( Problem ) A line. Source code in opti/problems/univariate.py class Line1D ( Problem ): \"\"\"A line.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y\" : X . eval ( \"0.1 * x + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) Parabola1D ( Problem ) A parabola. Source code in opti/problems/univariate.py class Parabola1D ( Problem ): \"\"\"A parabola.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"0.025 * (x - 5) ** 2 + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) Sigmoid1D ( Problem ) A smooth step at x=5. Source code in opti/problems/univariate.py class Sigmoid1D ( Problem ): \"\"\"A smooth step at x=5.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"1 / (1 + exp(-2 * (x - 5))) + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) Sinus1D ( Problem ) A sinus-function with one full period over the domain. Source code in opti/problems/univariate.py class Sinus1D ( Problem ): \"\"\"A sinus-function with one full period over the domain.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"sin(x * 2 * 3.14159 / 10) / 2 + 2\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) Step1D ( Problem ) A discrete step at x=1.1. Source code in opti/problems/univariate.py class Step1D ( Problem ): \"\"\"A discrete step at x=1.1.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y\" : X . eval ( \"x > 1.1\" ) . astype ( float )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Discrete ( \"y\" , [ 0 , 1 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) zdt ZDT benchmark problem suite. All problems are bi-objective, have D continuous inputs and are unconstrained. Zitzler, Deb, Thiele 2000 - Comparison of Multiobjective Evolutionary Algorithms: Empirical Results http://dx.doi.org/10.1162/106365600568202 ZDT1 ( Problem ) ZDT-1 benchmark problem. Source code in opti/problems/zdt.py class ZDT1 ( Problem ): \"\"\"ZDT-1 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-1 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 0.5 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . sqrt ( x )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names ) ZDT2 ( Problem ) ZDT-2 benchmark problem. Source code in opti/problems/zdt.py class ZDT2 ( Problem ): \"\"\"ZDT-2 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-2 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . power ( x , 2 )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names ) ZDT3 ( Problem ) ZDT-3 benchmark problem. Source code in opti/problems/zdt.py class ZDT3 ( Problem ): \"\"\"ZDT-3 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-3 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - np . inf , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 0.5 - ( y1 / g ) * np . sin ( 10 * np . pi * y1 )) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): regions = [ [ 0 , 0.0830015349 ], [ 0.182228780 , 0.2577623634 ], [ 0.4093136748 , 0.4538821041 ], [ 0.6183967944 , 0.6525117038 ], [ 0.8233317983 , 0.8518328654 ], ] pf = [] for r in regions : x1 = np . linspace ( r [ 0 ], r [ 1 ], int ( points / len ( regions ))) x2 = 1 - np . sqrt ( x1 ) - x1 * np . sin ( 10 * np . pi * x1 ) pf . append ( np . stack ([ x1 , x2 ], axis = 1 )) y = np . concatenate ( pf , axis = 0 ) return pd . DataFrame ( y , columns = self . outputs . names ) ZDT4 ( Problem ) ZDT-4 benchmark problem. Source code in opti/problems/zdt.py class ZDT4 ( Problem ): \"\"\"ZDT-4 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 10 ): super () . __init__ ( name = \"ZDT-4 problem\" , inputs = [ Continuous ( \"x1\" , [ 0 , 1 ])] + [ Continuous ( f \"x { i + 1 } \" , [ - 5 , 5 ]) for i in range ( 1 , n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . to_numpy () g = 1 + 10 * ( self . n_inputs - 1 ) for i in range ( 1 , self . n_inputs ): g += x [:, i ] ** 2 - 10 * np . cos ( 4.0 * np . pi * x [:, i ]) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - np . sqrt ( y1 / g )) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . sqrt ( x )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names ) ZDT6 ( Problem ) ZDT-6 benchmark problem. Source code in opti/problems/zdt.py class ZDT6 ( Problem ): \"\"\"ZDT-6 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-6 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - np . inf , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . to_numpy () n = self . n_inputs g = 1 + 9 * ( np . sum ( x [:, 1 :], axis = 1 ) / ( n - 1 )) ** 0.25 y1 = 1 - np . exp ( - 4 * x [:, 0 ]) * ( np . sin ( 6 * np . pi * x [:, 0 ])) ** 6 y2 = g * ( 1 - ( y1 / g ) ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0.2807753191 , 1 , points ) y = np . stack ([ x , 1 - x ** 2 ], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"Test Problems"},{"location":"ref-problems/#test-problems","text":"","title":"Test Problems"},{"location":"ref-problems/#opti.problems.benchmark","text":"","title":"benchmark"},{"location":"ref-problems/#opti.problems.benchmark.Daechert1","text":"Problem with a non-convex Pareto front. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. Source code in opti/problems/benchmark.py class Daechert1 ( Problem ): \"\"\"Problem with a non-convex Pareto front. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-1\" , inputs = [ Continuous ( \"x1\" , domain = [ 0 , np . pi ]), Continuous ( \"x2\" , domain = [ 0 , 10 ]), Continuous ( \"x3\" , domain = [ 1.2 , 10 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], constraints = [ NonlinearInequality ( \"- cos(x1) - exp(-x2) + x3\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y1\" : - X [ \"x1\" ], \"y2\" : - X [ \"x2\" ], \"y3\" : - X [ \"x3\" ] ** 2 })","title":"Daechert1"},{"location":"ref-problems/#opti.problems.benchmark.Daechert2","text":"Unconstrained problem with a Pareto front resembling a comet. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. Source code in opti/problems/benchmark.py class Daechert2 ( Problem ): \"\"\"Unconstrained problem with a Pareto front resembling a comet. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-2\" , inputs = [ Continuous ( \"x1\" , domain = [ 1 , 3.5 ]), Continuous ( \"x2\" , domain = [ - 2 , 2 ]), Continuous ( \"x3\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y1\" : X . eval ( \"(1 + x3) * (x1**3 * x2**2 - 10 * x1 - 4 * x2)\" ), \"y2\" : X . eval ( \"(1 + x3) * (x1**3 * x2**2 - 10 * x1 + 4 * x2)\" ), \"y3\" : X . eval ( \"3 * (1 + x3) * x1**2\" ), } )","title":"Daechert2"},{"location":"ref-problems/#opti.problems.benchmark.Daechert3","text":"Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. Source code in opti/problems/benchmark.py class Daechert3 ( Problem ): \"\"\"Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-3\" , inputs = [ Continuous ( f \"x { i + 1 } \" , domain = [ 0 , 1 ]) for i in range ( 2 )], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . values return pd . DataFrame ( { \"y1\" : X [ \"x1\" ], \"y2\" : X [ \"x2\" ], \"y3\" : 6 - np . sum ( x * ( 1 + np . sin ( 3 * np . pi * x )), axis = 1 ), } )","title":"Daechert3"},{"location":"ref-problems/#opti.problems.benchmark.Hyperellipsoid","text":"Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Parameters: Name Type Description Default n int Dimension of the hyperellipsoid. Defaults to 5. 5 a list-like Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. None Source code in opti/problems/benchmark.py class Hyperellipsoid ( Problem ): \"\"\"Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Args: n (int, optional): Dimension of the hyperellipsoid. Defaults to 5. a (list-like, optional): Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. \"\"\" def __init__ ( self , n : int = 5 , a : Optional [ Union [ list , np . ndarray ]] = None ): if a is None : a = np . ones ( n ) constr = \" + \" . join ([ f \"x { i + 1 } **2\" for i in range ( n )]) + \" - 1\" else : a = np . array ( a ) . squeeze () if len ( a ) != n : raise ValueError ( \"Dimension of half axes doesn't match input dimension\" ) constr = \" + \" . join ([ f \"(x { i + 1 } / { a [ i ] } )**2\" for i in range ( n )]) + \" - 1\" self . a = a super () . __init__ ( name = \"Hyperellipsoid\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - a [ i ], a [ i ]]) for i in range ( n )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - a [ i ], a [ i ]]) for i in range ( n )], constraints = [ NonlinearInequality ( constr )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : y = X [ self . inputs . names ] y . columns = self . outputs . names return y def get_optima ( self , n = 10 ) -> pd . DataFrame : X = opti . sampling . sphere . sample ( self . n_inputs , n , positive = True ) X = np . concatenate ([ - np . eye ( self . n_inputs ), - X ], axis = 0 )[: n ] Y = self . a * X return pd . DataFrame ( data = np . column_stack ([ X , Y ]), columns = self . inputs . names + self . outputs . names , )","title":"Hyperellipsoid"},{"location":"ref-problems/#opti.problems.benchmark.Qapi1","text":"Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. Source code in opti/problems/benchmark.py class Qapi1 ( Problem ): \"\"\"Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Constrained bi-objective problem\" , inputs = [ Continuous ( \"x1\" , [ 0 , 10 ]), Continuous ( \"x2\" , [ - 10 , 10 ])], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], constraints = [ NonlinearInequality ( \"x2 - x1**2\" ), NonlinearInequality ( \"2 - x1 - x2\" ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y1\" : X . eval ( \"(x1 - 2)**2 + (x2 - 1)**2\" ), \"y2\" : X . eval ( \"x1**2 + (x2 - 3)**2\" ), } )","title":"Qapi1"},{"location":"ref-problems/#opti.problems.datasets","text":"Datasets: Problems with observed data but no underlying ground truth.","title":"datasets"},{"location":"ref-problems/#opti.problems.datasets.Alkox","text":"Alkox reaction optimization. Source code in opti/problems/datasets.py class Alkox ( Problem ): \"\"\"Alkox reaction optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Alkox\" , inputs = [ Continuous ( \"residence_time\" , domain = [ 0.05 , 1 ]), Continuous ( \"ratio\" , domain = [ 0.5 , 10 ]), Continuous ( \"concentration\" , domain = [ 2 , 8 ]), Continuous ( \"temperature\" , domain = [ 6 , 8 ]), ], outputs = [ Continuous ( \"conversion\" )], objectives = [ Maximize ( \"conversion\" )], data = get_data ( \"alkox.csv\" ), )","title":"Alkox"},{"location":"ref-problems/#opti.problems.datasets.BaumgartnerAniline","text":"Baumgartner Aniline optimization. Source code in opti/problems/datasets.py class BaumgartnerAniline ( Problem ): \"\"\"Baumgartner Aniline optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Baumgartner 2019 - Aniline Cross-Coupling\" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"tBuXPhos\" , \"tBuBrettPhos\" , \"AlPhos\" ]), Categorical ( \"base\" , domain = [ \"TEA\" , \"TMG\" , \"BTMG\" , \"DBU\" ]), Continuous ( \"base_equivalents\" , domain = [ 1.0 , 2.5 ]), Continuous ( \"temperature\" , domain = [ 30 , 100 ]), Continuous ( \"residence_time\" , domain = [ 60 , 1800 ]), ], outputs = [ Continuous ( \"yield\" , domain = [ 0 , 1 ])], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"baumgartner_aniline.csv\" ), )","title":"BaumgartnerAniline"},{"location":"ref-problems/#opti.problems.datasets.BaumgartnerBenzamide","text":"Benzamide optimization, Baumgartner 2019. Source code in opti/problems/datasets.py class BaumgartnerBenzamide ( Problem ): \"\"\"Benzamide optimization, Baumgartner 2019.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Baumgartner 2019 - Benzamide Cross-Coupling\" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"tBuXPhos\" , \"tBuBrettPhos\" ]), Categorical ( \"base\" , domain = [ \"TMG\" , \"BTMG\" , \"DBU\" , \"MTBD\" ]), Continuous ( \"base_equivalents\" , domain = [ 1.0 , 2.1 ]), Continuous ( \"temperature\" , domain = [ 30 , 100 ]), Continuous ( \"residence_time\" , domain = [ 60 , 1850 ]), ], outputs = [ Continuous ( \"yield\" , domain = [ 0 , 1 ])], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"baumgartner_benzamide.csv\" ), )","title":"BaumgartnerBenzamide"},{"location":"ref-problems/#opti.problems.datasets.Benzylation","text":"Benzylation optimization. Source code in opti/problems/datasets.py class Benzylation ( Problem ): \"\"\"Benzylation optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Benzylation\" , inputs = [ Continuous ( \"flow_rate\" , domain = [ 0.2 , 0.4 ]), Continuous ( \"ratio\" , domain = [ 1.0 , 5.0 ]), Continuous ( \"solvent\" , domain = [ 0.5 , 1.0 ]), Continuous ( \"temperature\" , domain = [ 110.0 , 150.0 ]), ], outputs = [ Continuous ( \"impurity\" )], objectives = [ Minimize ( \"impurity\" )], data = get_data ( \"benzylation.csv\" ), )","title":"Benzylation"},{"location":"ref-problems/#opti.problems.datasets.Cake","text":"Cake recipe optimization with mixed objectives. Source code in opti/problems/datasets.py class Cake ( Problem ): \"\"\"Cake recipe optimization with mixed objectives.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Cake\" , inputs = [ Continuous ( \"wheat_flour\" , domain = [ 0 , 1 ]), Continuous ( \"spelt_flour\" , domain = [ 0 , 1 ]), Continuous ( \"sugar\" , domain = [ 0 , 1 ]), Continuous ( \"chocolate\" , domain = [ 0 , 1 ]), Continuous ( \"nuts\" , domain = [ 0 , 1 ]), Continuous ( \"carrot\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( \"calories\" , domain = [ 300 , 600 ]), Continuous ( \"taste\" , domain = [ 0 , 5 ]), Continuous ( \"browning\" , domain = [ 0 , 2 ]), ], objectives = [ Minimize ( \"calories\" ), Maximize ( \"taste\" ), CloseToTarget ( \"browning\" , target = 1.4 ), ], constraints = [ LinearEquality ( [ \"wheat_flour\" , \"spelt_flour\" , \"sugar\" , \"chocolate\" , \"nuts\" , \"carrot\" , ], rhs = 1 , ) ], data = get_data ( \"cake.csv\" ), )","title":"Cake"},{"location":"ref-problems/#opti.problems.datasets.Fullerenes","text":"Fullerene recation optimization. Source code in opti/problems/datasets.py class Fullerenes ( Problem ): \"\"\"Fullerene recation optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Fullerenes\" , inputs = [ Continuous ( \"reaction_time\" , domain = [ 3.0 , 31.0 ]), Continuous ( \"sultine\" , domain = [ 1.5 , 6.0 ]), Continuous ( \"temperature\" , domain = [ 100.0 , 150.0 ]), ], outputs = [ Continuous ( \"product\" )], objectives = [ Maximize ( \"product\" )], data = get_data ( \"fullerenes.csv\" ), )","title":"Fullerenes"},{"location":"ref-problems/#opti.problems.datasets.HPLC","text":"High-performance liquid chromatography optimization. Source code in opti/problems/datasets.py class HPLC ( Problem ): \"\"\"High-performance liquid chromatography optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"HPLC\" , inputs = [ Continuous ( \"sample_loop\" , domain = [ 0.0 , 0.08 ]), Continuous ( \"additional_volume\" , domain = [ 0.0 , 0.06 ]), Continuous ( \"tubing_volume\" , domain = [ 0.1 , 0.9 ]), Continuous ( \"sample_flow\" , domain = [ 0.5 , 2.5 ]), Continuous ( \"push_speed\" , domain = [ 80.0 , 150 ]), Continuous ( \"wait_time\" , domain = [ 0.5 , 10.0 ]), ], outputs = [ Continuous ( \"peak_area\" )], objectives = [ Maximize ( \"peak_area\" )], data = get_data ( \"hplc.csv\" ), )","title":"HPLC"},{"location":"ref-problems/#opti.problems.datasets.Photodegradation","text":"Photodegration minimization. Source code in opti/problems/datasets.py class Photodegradation ( Problem ): \"\"\"Photodegration minimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Photodegradation\" , inputs = [ Continuous ( \"PCE10\" , domain = [ 0 , 1 ]), Continuous ( \"WF3\" , domain = [ 0 , 1 ]), Continuous ( \"P3HT\" , domain = [ 0 , 1 ]), Continuous ( \"PCBM\" , domain = [ 0 , 1 ]), Continuous ( \"oIDTBR\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( \"degradation\" )], objectives = [ Minimize ( \"degradation\" )], constraints = [ LinearEquality ( [ \"PCE10\" , \"WF3\" , \"P3HT\" , \"PCBM\" , \"oIDTBR\" ], rhs = 1 , lhs = 1 ), NChooseK ([ \"PCE10\" , \"WF3\" ], max_active = 1 ), ], data = get_data ( \"photodegradation.csv\" ), )","title":"Photodegradation"},{"location":"ref-problems/#opti.problems.datasets.ReizmanSuzuki","text":"Suzuki reaction optimization, Reizmann 2016. Source code in opti/problems/datasets.py class ReizmanSuzuki ( Problem ): \"\"\"Suzuki reaction optimization, Reizmann 2016.\"\"\" def __init__ ( self , case = 1 ): assert case in [ 1 , 2 , 3 , 4 ] super () . __init__ ( name = f \"Reizman 2016 - Suzuki Case { case } \" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"P1-L1\" , \"P2-L1\" , \"P1-L2\" , \"P1-L3\" , \"P1-L4\" , \"P1-L5\" , \"P1-L6\" , \"P1-L7\" , ], ), Continuous ( \"t_res\" , domain = [ 60 , 600 ]), Continuous ( \"temperature\" , domain = [ 30 , 110 ]), Continuous ( \"catalyst_loading\" , domain = [ 0.496 , 2.515 ]), ], outputs = [ Continuous ( \"ton\" , domain = [ 0 , 100 ]), Continuous ( \"yield\" , domain = [ 0 , 100 ]), ], objectives = [ Maximize ( \"ton\" ), Maximize ( \"yield\" )], data = get_data ( f \"reizman_suzuki { case } .csv\" ), )","title":"ReizmanSuzuki"},{"location":"ref-problems/#opti.problems.datasets.SnAr","text":"SnAr reaction optimization. Source code in opti/problems/datasets.py class SnAr ( Problem ): \"\"\"SnAr reaction optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"SnAr\" , inputs = [ Continuous ( \"residence_time\" , domain = [ 0.5 , 2.0 ]), Continuous ( \"ratio\" , domain = [ 1.0 , 5.0 ]), Continuous ( \"concentration\" , domain = [ 0.1 , 0.5 ]), Continuous ( \"temperature\" , domain = [ 60.0 , 140.0 ]), ], outputs = [ Continuous ( \"impurity\" )], objectives = [ Minimize ( \"impurity\" )], data = get_data ( \"snar.csv\" ), )","title":"SnAr"},{"location":"ref-problems/#opti.problems.datasets.Suzuki","text":"Suzuki reaction optimization. Source code in opti/problems/datasets.py class Suzuki ( Problem ): \"\"\"Suzuki reaction optimization.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Suzuki\" , inputs = [ Continuous ( \"temperature\" , domain = [ 75.0 , 90.0 ]), Continuous ( \"pd_mol\" , domain = [ 0.5 , 5.0 ]), Continuous ( \"arbpin\" , domain = [ 1.0 , 1.8 ]), Continuous ( \"k3po4\" , domain = [ 1.5 , 3.0 ]), ], outputs = [ Continuous ( \"yield\" )], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"suzuki.csv\" ), )","title":"Suzuki"},{"location":"ref-problems/#opti.problems.detergent","text":"","title":"detergent"},{"location":"ref-problems/#opti.problems.detergent.Detergent","text":"Constrained problem with 5 inputs and 5 outputs. Each output is modeled as a second degree polynomial. The sixth input is a filler (water) and is factored out using the formulation constraint sum x = 1, and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. Source code in opti/problems/detergent.py class Detergent ( Problem ): \"\"\"Constrained problem with 5 inputs and 5 outputs. Each output is modeled as a second degree polynomial. The sixth input is a filler (water) and is factored out using the formulation constraint sum x = 1, and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. \"\"\" def __init__ ( self ): # coefficients for the 2-order polynomial; generated with # base = 3 * np.ones((1, 5)) # scale = PolynomialFeatures(degree=2).fit_transform(base).T # coef = np.random.RandomState(42).normal(scale=scale, size=(len(scale), 5)) # coef = np.clip(coef, 0, None) self . coef = np . array ( [ [ 0.4967 , 0.0 , 0.6477 , 1.523 , 0.0 ], [ 0.0 , 4.7376 , 2.3023 , 0.0 , 1.6277 ], [ 0.0 , 0.0 , 0.7259 , 0.0 , 0.0 ], [ 0.0 , 0.0 , 0.9427 , 0.0 , 0.0 ], [ 4.3969 , 0.0 , 0.2026 , 0.0 , 0.0 ], [ 0.3328 , 0.0 , 1.1271 , 0.0 , 0.0 ], [ 0.0 , 16.6705 , 0.0 , 0.0 , 7.4029 ], [ 0.0 , 1.8798 , 0.0 , 0.0 , 1.7718 ], [ 6.6462 , 1.5423 , 0.0 , 0.0 , 0.0 ], [ 0.0 , 0.0 , 9.5141 , 3.0926 , 0.0 ], [ 2.9168 , 0.0 , 0.0 , 5.5051 , 9.279 ], [ 8.3815 , 0.0 , 0.0 , 2.9814 , 8.7799 ], [ 0.0 , 0.0 , 0.0 , 0.0 , 7.3127 ], [ 12.2062 , 0.0 , 9.0318 , 3.2547 , 0.0 ], [ 3.2526 , 13.8423 , 0.0 , 14.0818 , 0.0 ], [ 7.3971 , 0.7834 , 0.0 , 0.8258 , 0.0 ], [ 0.0 , 3.214 , 13.301 , 0.0 , 0.0 ], [ 0.0 , 8.2386 , 2.9588 , 0.0 , 4.6194 ], [ 0.8737 , 8.7178 , 0.0 , 0.0 , 0.0 ], [ 0.0 , 2.6651 , 2.3495 , 0.046 , 0.0 ], [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ], ] ) super () . __init__ ( name = \"Detergent\" , inputs = [ Continuous ( \"x1\" , domain = [ 0.0 , 0.2 ]), Continuous ( \"x2\" , domain = [ 0.0 , 0.3 ]), Continuous ( \"x3\" , domain = [ 0.02 , 0.2 ]), Continuous ( \"x4\" , domain = [ 0.0 , 0.06 ]), Continuous ( \"x5\" , domain = [ 0.0 , 0.04 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" , domain = [ 0 , 3 ]) for i in range ( 5 )], objectives = [ Maximize ( f \"y { i + 1 } \" ) for i in range ( 5 )], constraints = [ LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"x5\" ], lhs =- 1 , rhs =- 0.2 ), LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"x5\" ], lhs = 1 , rhs = 0.4 ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = np . atleast_2d ( X [ self . inputs . names ]) xp = np . stack ([ _poly2 ( xi ) for xi in x ], axis = 0 ) return pd . DataFrame ( xp @ self . coef , columns = self . outputs . names , index = X . index )","title":"Detergent"},{"location":"ref-problems/#opti.problems.detergent.Detergent_NChooseKConstraint","text":"Variant of the Detergent problem with an n-choose-k constraint Source code in opti/problems/detergent.py class Detergent_NChooseKConstraint ( Problem ): \"\"\"Variant of the Detergent problem with an n-choose-k constraint\"\"\" def __init__ ( self ): base = Detergent () super () . __init__ ( name = \"Detergent with n-choose-k constraint\" , inputs = base . inputs , outputs = base . outputs , objectives = base . objectives , constraints = list ( base . constraints ) + [ NChooseK ( names = base . inputs . names , max_active = 3 )], f = base . f , )","title":"Detergent_NChooseKConstraint"},{"location":"ref-problems/#opti.problems.detergent.Detergent_OutputConstraint","text":"Variant of the Detergent problem with an output constraint. There are 5 inputs, 6 outputs: - 3 outputs are to be maximized - 1 output represents the stability of the formulation (0: not stable, 1: stable) Source code in opti/problems/detergent.py class Detergent_OutputConstraint ( Problem ): \"\"\"Variant of the Detergent problem with an output constraint. There are 5 inputs, 6 outputs: - 3 outputs are to be maximized - 1 output represents the stability of the formulation (0: not stable, 1: stable) \"\"\" def __init__ ( self , discrete = False ): base = Detergent () def f ( X ): Y = base . f ( X ) if discrete : Y [ \"stable\" ] = ( X . sum ( axis = 1 ) < 0.3 ) . astype ( int ) else : Y [ \"stable\" ] = ( 0.4 - X . sum ( axis = 1 )) / 0.2 return Y outputs = list ( base . outputs ) if discrete : outputs += [ Discrete ( \"stable\" , domain = [ 0 , 1 ])] else : outputs += [ Continuous ( \"stable\" , domain = [ 0 , 1 ])] super () . __init__ ( name = \"Detergent with stability constraint\" , inputs = base . inputs , outputs = outputs , objectives = base . objectives , output_constraints = [ Maximize ( \"stable\" , target = 0.5 )], constraints = base . constraints , f = f , )","title":"Detergent_OutputConstraint"},{"location":"ref-problems/#opti.problems.mixed","text":"Mixed variables single and multi-objective test problems.","title":"mixed"},{"location":"ref-problems/#opti.problems.mixed.DiscreteFuelInjector","text":"Fuel injector test problem, modified to contain an integer variable. 4 objectives, mixed variables, unconstrained See Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 Source code in opti/problems/mixed.py class DiscreteFuelInjector ( Problem ): \"\"\"Fuel injector test problem, modified to contain an integer variable. * 4 objectives, * mixed variables, unconstrained See: Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Discrete fuel injector test problem\" , inputs = [ Discrete ( \"x1\" , [ 0 , 1 , 2 , 3 ]), Continuous ( \"x2\" , [ - 2 , 2 ]), Continuous ( \"x3\" , [ - 2 , 2 ]), Continuous ( \"x4\" , [ - 2 , 2 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 4 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 = X [ \"x1\" ] . to_numpy () . astype ( float ) x2 = X [ \"x2\" ] . to_numpy () . astype ( float ) x3 = X [ \"x3\" ] . to_numpy () . astype ( float ) x4 = X [ \"x4\" ] . to_numpy () . astype ( float ) x1 *= 0.2 y1 = ( 0.692 + 0.4771 * x1 - 0.687 * x4 - 0.08 * x3 - 0.065 * x2 - 0.167 * x1 ** 2 - 0.0129 * x1 * x4 + 0.0796 * x4 ** 2 - 0.0634 * x1 * x3 - 0.0257 * x3 * x4 + 0.0877 * x3 ** 2 - 0.0521 * x1 * x2 + 0.00156 * x2 * x4 + 0.00198 * x2 * x3 + 0.0184 * x2 ** 2 ) y2 = ( 0.37 - 0.205 * x1 + 0.0307 * x4 + 0.108 * x3 + 1.019 * x2 - 0.135 * x1 ** 2 + 0.0141 * x1 * x4 + 0.0998 * x4 ** 2 + 0.208 * x1 * x3 - 0.0301 * x3 * x4 - 0.226 * x3 ** 2 + 0.353 * x1 * x2 - 0.0497 * x2 * x3 - 0.423 * x2 ** 2 + 0.202 * x1 ** 2 * x4 - 0.281 * x1 ** 2 * x3 - 0.342 * x1 * x4 ** 2 - 0.245 * x3 * x4 ** 2 + 0.281 * x3 ** 2 * x4 - 0.184 * x1 * x2 ** 2 + 0.281 * x1 * x3 * x4 ) y3 = ( 0.153 - 0.322 * x1 + 0.396 * x4 + 0.424 * x3 + 0.0226 * x2 + 0.175 * x1 ** 2 + 0.0185 * x1 * x4 - 0.0701 * x4 ** 2 - 0.251 * x1 * x3 + 0.179 * x3 * x4 + 0.015 * x3 ** 2 + 0.0134 * x1 * x2 + 0.0296 * x2 * x4 + 0.0752 * x2 * x3 + 0.0192 * x2 ** 2 ) y4 = ( 0.758 + 0.358 * x1 - 0.807 * x4 + 0.0925 * x3 - 0.0468 * x2 - 0.172 * x1 ** 2 + 0.0106 * x1 * x4 + 0.0697 * x4 ** 2 - 0.146 * x1 * x3 - 0.0416 * x3 * x4 + 0.102 * x3 ** 2 - 0.0694 * x1 * x2 - 0.00503 * x2 * x4 + 0.0151 * x2 * x3 + 0.0173 * x2 ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 , \"y3\" : y3 , \"y4\" : y4 })","title":"DiscreteFuelInjector"},{"location":"ref-problems/#opti.problems.mixed.DiscreteVLMOP2","text":"VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. 2 minimization objectives 1 categorical and n continuous inputs, unconstrained See Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 Source code in opti/problems/mixed.py class DiscreteVLMOP2 ( Problem ): \"\"\"VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. * 2 minimization objectives * 1 categorical and n continuous inputs, unconstrained See: Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 \"\"\" def __init__ ( self , n_inputs : int = 3 ): assert n_inputs >= 2 super () . __init__ ( name = \"Discrete VLMOP2 test problem\" , inputs = [ Categorical ( \"x1\" , [ \"a\" , \"b\" ])] + [ Continuous ( f \"x { i + 1 } \" , [ - 2 , 2 ]) for i in range ( 1 , n_inputs )], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : d = X [ self . inputs . names [ 0 ]] . values x = X [ self . inputs . names [ 1 :]] . values n = self . n_inputs y1 = np . exp ( - np . sum (( x - n ** - 0.5 ) ** 2 , axis = 1 )) y2 = np . exp ( - np . sum (( x + n ** - 0.5 ) ** 2 , axis = 1 )) y1 = np . where ( d == \"a\" , 1 - y1 , 1.25 - y1 ) y2 = np . where ( d == \"a\" , 1 - y2 , 0.75 - y2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index )","title":"DiscreteVLMOP2"},{"location":"ref-problems/#opti.problems.single","text":"Single objective benchmark problems.","title":"single"},{"location":"ref-problems/#opti.problems.single.Ackley","text":"Ackley benchmark problem. Source code in opti/problems/single.py class Ackley ( Problem ): \"\"\"Ackley benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Ackley problem\" , inputs = [ Continuous ( f \"x { i } \" , [ - 32.768 , + 32.768 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : a = 20 b = 1 / 5 c = 2 * np . pi n = self . n_inputs x = self . get_X ( X ) part1 = - a * np . exp ( - b * np . sqrt (( 1 / n ) * np . sum ( x ** 2 , axis =- 1 ))) part2 = - np . exp (( 1 / n ) * np . sum ( np . cos ( c * x ), axis =- 1 )) y = part1 + part2 + a + np . exp ( 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Ackley"},{"location":"ref-problems/#opti.problems.single.Himmelblau","text":"Himmelblau benchmark problem Source code in opti/problems/single.py class Himmelblau ( Problem ): \"\"\"Himmelblau benchmark problem\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Himmelblau function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 6 , 6 ]) for i in range ( 2 )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x0 , x1 = self . get_X ( X ) . T y = ( x0 ** 2 + x1 - 11 ) ** 2 + ( x0 + x1 ** 2 - 7 ) ** 2 return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . array ( [ [ 3.0 , 2.0 ], [ - 2.805118 , 3.131312 ], [ - 3.779310 , - 3.283186 ], [ 3.584428 , - 1.848126 ], ] ) y = np . zeros ( 4 ) return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Himmelblau"},{"location":"ref-problems/#opti.problems.single.Rastrigin","text":"Rastrigin benchmark problem. Source code in opti/problems/single.py class Rastrigin ( Problem ): \"\"\"Rastrigin benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Rastrigin function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 5 , 5 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) a = 10 y = a * self . n_inputs + np . sum ( x ** 2 - a * np . cos ( 2 * np . pi * x ), axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Rastrigin"},{"location":"ref-problems/#opti.problems.single.Rosenbrock","text":"Rosenbrock benchmark problem. Source code in opti/problems/single.py class Rosenbrock ( Problem ): \"\"\"Rosenbrock benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Rosenbrock function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 2.048 , 2.048 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) . T y = np . sum ( 100 * ( x [ 1 :] - x [: - 1 ] ** 2 ) ** 2 + ( 1 - x [: - 1 ]) ** 2 , axis = 0 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . ones (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Rosenbrock"},{"location":"ref-problems/#opti.problems.single.Schwefel","text":"Schwefel benchmark problem Source code in opti/problems/single.py class Schwefel ( Problem ): \"\"\"Schwefel benchmark problem\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Schwefel function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 500 , 500 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) y = 418.9829 * self . n_inputs - np . sum ( x * np . sin ( np . abs ( x ) ** 0.5 ), axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . full (( 1 , self . n_inputs ), 420.9687 ) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Schwefel"},{"location":"ref-problems/#opti.problems.single.Sphere","text":"Sphere benchmark problem. Source code in opti/problems/single.py class Sphere ( Problem ): \"\"\"Sphere benchmark problem.\"\"\" def __init__ ( self , n_inputs = 10 ): super () . __init__ ( name = \"Sphere function\" , inputs = [ Continuous ( f \"x { i } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ 0 , 2 ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) y = np . sum (( x - 0.5 ) ** 2 , axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . full (( 1 , self . n_inputs ), 0.5 ) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Sphere"},{"location":"ref-problems/#opti.problems.single.Zakharov","text":"Zakharov benchmark problem. Source code in opti/problems/single.py class Zakharov ( Problem ): \"\"\"Zakharov benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Zakharov function\" , inputs = [ Continuous ( f \"x { i } \" , [ - 10 , 10 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ - np . inf , np . inf ])], ) def f ( self , X : pd . DataFrame ): x = self . get_X ( X ) a = 0.5 * np . sum ( np . arange ( 1 , self . n_inputs + 1 ) * x , axis = 1 ) y = np . sum ( x ** 2 , axis = 1 ) + a ** 2 + a ** 4 return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Zakharov"},{"location":"ref-problems/#opti.problems.single.Zakharov_Categorical","text":"Zakharov problem with one categorical input Source code in opti/problems/single.py class Zakharov_Categorical ( Problem ): \"\"\"Zakharov problem with one categorical input\"\"\" def __init__ ( self , n_inputs = 3 ): base = Zakharov ( n_inputs ) super () . __init__ ( name = \"Zakharov function with one categorical input\" , inputs = [ Continuous ( f \"x { i } \" , [ - 10 , 10 ]) for i in range ( n_inputs - 1 )] + [ Categorical ( \"expon_switch\" , [ \"one\" , \"two\" ])], outputs = base . outputs , ) def f ( self , X : pd . DataFrame ): x_conti = X [ self . inputs . names [: - 1 ]] . values # just the continuous inputs a = 0.5 * np . sum ( np . arange ( 1 , self . n_inputs ) * x_conti , axis = 1 ) powers = np . repeat ( np . expand_dims ([ 2.0 , 2.0 , 4.0 ], 0 ), repeats = len ( X ), axis = 0 ) modify_powers = X [ self . inputs . names [ - 1 ]] == \"two\" powers [ modify_powers , :] += powers [ modify_powers , :] res = ( np . sum ( x_conti ** np . expand_dims ( powers [:, 0 ], 1 ), axis = 1 ) + a ** np . expand_dims ( powers [:, 1 ], 0 ) + a ** np . expand_dims ( powers [:, 2 ], 0 ) ) res_float_array = np . array ( res , dtype = np . float64 ) . ravel () y = res_float_array return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = list ( np . zeros ( self . n_inputs - 1 )) + [ \"one\" ] y = [ 0 ] return pd . DataFrame ([ x + y ], columns = self . inputs . names + self . outputs . names )","title":"Zakharov_Categorical"},{"location":"ref-problems/#opti.problems.single.Zakharov_Constrained","text":"Zakharov problem with one linear constraint Source code in opti/problems/single.py class Zakharov_Constrained ( Problem ): \"\"\"Zakharov problem with one linear constraint\"\"\" def __init__ ( self , n_inputs = 5 ): base = Zakharov ( n_inputs ) self . base = base super () . __init__ ( name = \"Zakharov with one linear constraint\" , inputs = base . inputs , outputs = base . outputs , constraints = [ LinearInequality ( base . inputs . names , lhs = 1 , rhs = 10 )], f = base . f , ) def get_optima ( self ): return self . base . get_optima ()","title":"Zakharov_Constrained"},{"location":"ref-problems/#opti.problems.single.Zakharov_NChooseKConstraint","text":"Zakharov problem with an n-choose-k constraint Source code in opti/problems/single.py class Zakharov_NChooseKConstraint ( Problem ): \"\"\"Zakharov problem with an n-choose-k constraint\"\"\" def __init__ ( self , n_inputs = 5 , n_max_active = 3 ): base = Zakharov ( n_inputs ) self . base = base super () . __init__ ( name = \"Zakharov with n-choose-k constraint\" , inputs = base . inputs , outputs = base . outputs , constraints = [ NChooseK ( names = base . inputs . names , max_active = n_max_active )], f = base . f , ) def get_optima ( self ): return self . base . get_optima ()","title":"Zakharov_NChooseKConstraint"},{"location":"ref-problems/#opti.problems.univariate","text":"Simple 1D problems for assessing probabilistic surrogate models. Note: these problems should be output-noisified, e.g. import opti problem = opti.problems.noisify_problem_with_gaussian( opti.problems.Line1D(), sigma=0.1 )","title":"univariate"},{"location":"ref-problems/#opti.problems.univariate.Line1D","text":"A line. Source code in opti/problems/univariate.py class Line1D ( Problem ): \"\"\"A line.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y\" : X . eval ( \"0.1 * x + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Line1D"},{"location":"ref-problems/#opti.problems.univariate.Parabola1D","text":"A parabola. Source code in opti/problems/univariate.py class Parabola1D ( Problem ): \"\"\"A parabola.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"0.025 * (x - 5) ** 2 + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Parabola1D"},{"location":"ref-problems/#opti.problems.univariate.Sigmoid1D","text":"A smooth step at x=5. Source code in opti/problems/univariate.py class Sigmoid1D ( Problem ): \"\"\"A smooth step at x=5.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"1 / (1 + exp(-2 * (x - 5))) + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Sigmoid1D"},{"location":"ref-problems/#opti.problems.univariate.Sinus1D","text":"A sinus-function with one full period over the domain. Source code in opti/problems/univariate.py class Sinus1D ( Problem ): \"\"\"A sinus-function with one full period over the domain.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"sin(x * 2 * 3.14159 / 10) / 2 + 2\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Sinus1D"},{"location":"ref-problems/#opti.problems.univariate.Step1D","text":"A discrete step at x=1.1. Source code in opti/problems/univariate.py class Step1D ( Problem ): \"\"\"A discrete step at x=1.1.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y\" : X . eval ( \"x > 1.1\" ) . astype ( float )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Discrete ( \"y\" , [ 0 , 1 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Step1D"},{"location":"ref-problems/#opti.problems.zdt","text":"ZDT benchmark problem suite. All problems are bi-objective, have D continuous inputs and are unconstrained. Zitzler, Deb, Thiele 2000 - Comparison of Multiobjective Evolutionary Algorithms: Empirical Results http://dx.doi.org/10.1162/106365600568202","title":"zdt"},{"location":"ref-problems/#opti.problems.zdt.ZDT1","text":"ZDT-1 benchmark problem. Source code in opti/problems/zdt.py class ZDT1 ( Problem ): \"\"\"ZDT-1 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-1 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 0.5 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . sqrt ( x )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT1"},{"location":"ref-problems/#opti.problems.zdt.ZDT2","text":"ZDT-2 benchmark problem. Source code in opti/problems/zdt.py class ZDT2 ( Problem ): \"\"\"ZDT-2 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-2 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . power ( x , 2 )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT2"},{"location":"ref-problems/#opti.problems.zdt.ZDT3","text":"ZDT-3 benchmark problem. Source code in opti/problems/zdt.py class ZDT3 ( Problem ): \"\"\"ZDT-3 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-3 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - np . inf , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 0.5 - ( y1 / g ) * np . sin ( 10 * np . pi * y1 )) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): regions = [ [ 0 , 0.0830015349 ], [ 0.182228780 , 0.2577623634 ], [ 0.4093136748 , 0.4538821041 ], [ 0.6183967944 , 0.6525117038 ], [ 0.8233317983 , 0.8518328654 ], ] pf = [] for r in regions : x1 = np . linspace ( r [ 0 ], r [ 1 ], int ( points / len ( regions ))) x2 = 1 - np . sqrt ( x1 ) - x1 * np . sin ( 10 * np . pi * x1 ) pf . append ( np . stack ([ x1 , x2 ], axis = 1 )) y = np . concatenate ( pf , axis = 0 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT3"},{"location":"ref-problems/#opti.problems.zdt.ZDT4","text":"ZDT-4 benchmark problem. Source code in opti/problems/zdt.py class ZDT4 ( Problem ): \"\"\"ZDT-4 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 10 ): super () . __init__ ( name = \"ZDT-4 problem\" , inputs = [ Continuous ( \"x1\" , [ 0 , 1 ])] + [ Continuous ( f \"x { i + 1 } \" , [ - 5 , 5 ]) for i in range ( 1 , n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . to_numpy () g = 1 + 10 * ( self . n_inputs - 1 ) for i in range ( 1 , self . n_inputs ): g += x [:, i ] ** 2 - 10 * np . cos ( 4.0 * np . pi * x [:, i ]) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - np . sqrt ( y1 / g )) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . sqrt ( x )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT4"},{"location":"ref-problems/#opti.problems.zdt.ZDT6","text":"ZDT-6 benchmark problem. Source code in opti/problems/zdt.py class ZDT6 ( Problem ): \"\"\"ZDT-6 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-6 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - np . inf , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . to_numpy () n = self . n_inputs g = 1 + 9 * ( np . sum ( x [:, 1 :], axis = 1 ) / ( n - 1 )) ** 0.25 y1 = 1 - np . exp ( - 4 * x [:, 0 ]) * ( np . sin ( 6 * np . pi * x [:, 0 ])) ** 6 y2 = g * ( 1 - ( y1 / g ) ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0.2807753191 , 1 , points ) y = np . stack ([ x , 1 - x ** 2 ], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT6"},{"location":"ref-sampling/","text":"Sampling base apply_nchoosek ( samples , constraint ) Apply an n-choose-k constraint in-place Source code in opti/sampling/base.py def apply_nchoosek ( samples : pd . DataFrame , constraint : NChooseK ): \"\"\"Apply an n-choose-k constraint in-place\"\"\" n_zeros = len ( constraint . names ) - constraint . max_active for i in samples . index : s = np . random . choice ( constraint . names , size = n_zeros , replace = False ) samples . loc [ i , s ] = 0 constrained_sampling ( n_samples , parameters , constraints ) Uniform sampling from a constrained space. Source code in opti/sampling/base.py def constrained_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints ) -> pd . DataFrame : \"\"\"Uniform sampling from a constrained space.\"\"\" nchoosek_constraints , other_constraints = split_nchoosek ( constraints ) try : samples = rejection_sampling ( n_samples , parameters , other_constraints ) except Exception : samples = polytope_sampling ( n_samples , parameters , other_constraints ) if len ( nchoosek_constraints ) == 0 : return samples for c in nchoosek_constraints : apply_nchoosek ( samples , c ) # check if other constraints are still satisfied if not constraints . satisfied ( samples ) . all (): raise Exception ( \"Applying the n-choose-k constraint(s) violated another constraint.\" ) return samples rejection_sampling ( n_samples , parameters , constraints , max_iters = 1000 ) Uniformly distributed samples from a constrained space via rejection sampling. Source code in opti/sampling/base.py def rejection_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , max_iters : int = 1000 , ) -> pd . DataFrame : \"\"\"Uniformly distributed samples from a constrained space via rejection sampling.\"\"\" if constraints is None : return parameters . sample ( n_samples ) # check for equality constraints in combination with continuous parameters for c in constraints : if isinstance ( c , ( LinearEquality , NonlinearEquality )): for p in parameters : if isinstance ( p , Continuous ): raise Exception ( \"Rejection sampling doesn't work for equality constraints over continuous variables.\" ) n_iters = 0 n_found = 0 points_found = [] while n_found < n_samples : n_iters += 1 if n_iters > max_iters : raise Exception ( \"Maximum iterations exceeded in rejection sampling\" ) points = parameters . sample ( 10000 ) valid = constraints . satisfied ( points ) n_found += np . sum ( valid ) points_found . append ( points [ valid ]) return pd . concat ( points_found , ignore_index = True ) . iloc [: n_samples ] sobol_sampling ( n_samples , parameters ) Super-uniform sampling from an unconstrained space using a Sobol sequence. Source code in opti/sampling/base.py def sobol_sampling ( n_samples : int , parameters : Parameters ) -> pd . DataFrame : \"\"\"Super-uniform sampling from an unconstrained space using a Sobol sequence.\"\"\" d = len ( parameters ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( d ) . random ( n_samples ) res = [] for i , p in enumerate ( parameters ): if isinstance ( p , Continuous ): x = p . from_unit_range ( X [:, i ]) else : bins = np . linspace ( 0 , 1 , len ( p . domain ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( p . domain )[ idx ] res . append ( pd . Series ( x , name = p . name )) return pd . concat ( res , axis = 1 ) split_nchoosek ( constraints ) Split constraints in n-choose-k constraint and all other constraints. Source code in opti/sampling/base.py def split_nchoosek ( constraints : Optional [ Constraints ], ) -> Tuple [ Constraints , Constraints ]: \"\"\"Split constraints in n-choose-k constraint and all other constraints.\"\"\" if constraints is None : return Constraints ([]), Constraints ([]) nchoosek_constraints = Constraints ( [ c for c in constraints if isinstance ( c , NChooseK )] ) other_constraints = Constraints ( [ c for c in constraints if not isinstance ( c , NChooseK )] ) return nchoosek_constraints , other_constraints polytope This module provides functions to uniformly sample points subject to a system of linear inequality constraints, :math: Ax <= b (convex polytope), and linear equality constraints, :math: Ax = b (affine projection). A comparison of MCMC algorithms to generate uniform samples over a convex polytope is given in [Chen2018] . Here, we use the Hit & Run algorithm described in [Smith1984] . The R-package hitandrun _ provides similar functionality to this module. References .. [Chen2018] Chen Y., Dwivedi, R., Wainwright, M., Yu B. (2018) Fast MCMC Sampling Algorithms on Polytopes. JMLR, 19(55):1\u221286 https://arxiv.org/abs/1710.08165 .. [Smith1984] Smith, R. (1984). Efficient Monte Carlo Procedures for Generating Points Uniformly Distributed Over Bounded Regions. Operations Research, 32(6), 1296-1308. www.jstor.org/stable/170949 .. _ hitandrun : https://cran.r-project.org/web/packages/hitandrun/index.html polytope_sampling ( n_samples , parameters , constraints , thin = 100 ) Hit-and-run method to sample uniformly under linear constraints. Parameters: Name Type Description Default n_samples int Number of samples. required parameters opti.Parameters Parameter space. required constraints opti.Constraints Constraints on the parameters. required thin int Thinning factor of the generated samples. 100 Returns: Type Description array, shape=(n_samples, dimension) Randomly sampled points. Source code in opti/sampling/polytope.py def polytope_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , thin : int = 100 ) -> pd . DataFrame : \"\"\"Hit-and-run method to sample uniformly under linear constraints. Args: n_samples (int): Number of samples. parameters (opti.Parameters): Parameter space. constraints (opti.Constraints): Constraints on the parameters. thin (int, optional): Thinning factor of the generated samples. Returns: array, shape=(n_samples, dimension): Randomly sampled points. \"\"\" for c in constraints : if not isinstance ( c , ( LinearEquality , LinearInequality )): raise Exception ( \"Polytope sampling only works for linear constraints.\" ) At , bt , N , xp = _get_AbNx ( parameters , constraints ) # hit & run sampling x0 = _chebyshev_center ( At , bt ) sampler = _hitandrun ( At , bt , x0 ) X = np . empty (( n_samples , At . shape [ 1 ])) for i in range ( n_samples ): for _ in range ( thin - 1 ): next ( sampler ) X [ i ] = next ( sampler ) # project back X = X @ N . T + xp return pd . DataFrame ( columns = parameters . names , data = X ) simplex grid ( dimension , levels ) Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Parameters: Name Type Description Default dimension int Number of variables. required levels int Number of levels for each variable. required Returns: Type Description array Regularily spaced points on the unit simplex. Examples: >>> simplex_grid ( 3 , 3 ) array ([ [ 0. , 0. , 1. ], [ 0. , 0.5 , 0.5 ], [ 0. , 1. , 0. ], [ 0.5 , 0. , 0.5 ], [ 0.5 , 0.5 , 0. ], [ 1. , 0. , 0. ] ]) References Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. Source code in opti/sampling/simplex.py def grid ( dimension : int , levels : int ) -> np . ndarray : \"\"\"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Args: dimension (int): Number of variables. levels (int): Number of levels for each variable. Returns: array: Regularily spaced points on the unit simplex. Examples: >>> simplex_grid(3, 3) array([ [0. , 0. , 1. ], [0. , 0.5, 0.5], [0. , 1. , 0. ], [0.5, 0. , 0.5], [0.5, 0.5, 0. ], [1. , 0. , 0. ] ]) References: Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. \"\"\" m = dimension n = levels - 1 L = int ( comb ( dimension - 1 + levels - 1 , dimension - 1 , exact = True )) x = np . zeros ( m , dtype = int ) x [ - 1 ] = n out = np . empty (( L , m ), dtype = int ) out [ 0 ] = x h = m for i in range ( 1 , L ): h -= 1 val = x [ h ] x [ h ] = 0 x [ h - 1 ] += 1 x [ - 1 ] = val - 1 if val != 1 : h = m out [ i ] = x return out / n sample ( dimension , n_samples = 1 ) Sample uniformly from the unit simplex. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/simplex.py def sample ( dimension : int , n_samples : int = 1 ) -> np . ndarray : \"\"\"Sample uniformly from the unit simplex. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" s = np . random . standard_exponential (( n_samples , dimension )) return ( s . T / s . sum ( axis = 1 )) . T sphere sample ( dimension , n_samples = 1 , positive = False ) Sample uniformly from the unit hypersphere. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 positive bool Sample from the non-negative unit-sphere. False Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/sphere.py def sample ( dimension : int , n_samples : int = 1 , positive : bool = False ) -> np . ndarray : \"\"\"Sample uniformly from the unit hypersphere. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. positive (bool): Sample from the non-negative unit-sphere. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" x = np . random . normal ( 0 , 1 , size = ( n_samples , dimension )) x = x / np . sum ( x ** 2 , axis = 1 , keepdims = True ) ** 0.5 if positive : x *= - 2 * ( x < 0 ) + 1 return x","title":"Sampling"},{"location":"ref-sampling/#sampling","text":"","title":"Sampling"},{"location":"ref-sampling/#opti.sampling.base","text":"","title":"base"},{"location":"ref-sampling/#opti.sampling.base.apply_nchoosek","text":"Apply an n-choose-k constraint in-place Source code in opti/sampling/base.py def apply_nchoosek ( samples : pd . DataFrame , constraint : NChooseK ): \"\"\"Apply an n-choose-k constraint in-place\"\"\" n_zeros = len ( constraint . names ) - constraint . max_active for i in samples . index : s = np . random . choice ( constraint . names , size = n_zeros , replace = False ) samples . loc [ i , s ] = 0","title":"apply_nchoosek()"},{"location":"ref-sampling/#opti.sampling.base.constrained_sampling","text":"Uniform sampling from a constrained space. Source code in opti/sampling/base.py def constrained_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints ) -> pd . DataFrame : \"\"\"Uniform sampling from a constrained space.\"\"\" nchoosek_constraints , other_constraints = split_nchoosek ( constraints ) try : samples = rejection_sampling ( n_samples , parameters , other_constraints ) except Exception : samples = polytope_sampling ( n_samples , parameters , other_constraints ) if len ( nchoosek_constraints ) == 0 : return samples for c in nchoosek_constraints : apply_nchoosek ( samples , c ) # check if other constraints are still satisfied if not constraints . satisfied ( samples ) . all (): raise Exception ( \"Applying the n-choose-k constraint(s) violated another constraint.\" ) return samples","title":"constrained_sampling()"},{"location":"ref-sampling/#opti.sampling.base.rejection_sampling","text":"Uniformly distributed samples from a constrained space via rejection sampling. Source code in opti/sampling/base.py def rejection_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , max_iters : int = 1000 , ) -> pd . DataFrame : \"\"\"Uniformly distributed samples from a constrained space via rejection sampling.\"\"\" if constraints is None : return parameters . sample ( n_samples ) # check for equality constraints in combination with continuous parameters for c in constraints : if isinstance ( c , ( LinearEquality , NonlinearEquality )): for p in parameters : if isinstance ( p , Continuous ): raise Exception ( \"Rejection sampling doesn't work for equality constraints over continuous variables.\" ) n_iters = 0 n_found = 0 points_found = [] while n_found < n_samples : n_iters += 1 if n_iters > max_iters : raise Exception ( \"Maximum iterations exceeded in rejection sampling\" ) points = parameters . sample ( 10000 ) valid = constraints . satisfied ( points ) n_found += np . sum ( valid ) points_found . append ( points [ valid ]) return pd . concat ( points_found , ignore_index = True ) . iloc [: n_samples ]","title":"rejection_sampling()"},{"location":"ref-sampling/#opti.sampling.base.sobol_sampling","text":"Super-uniform sampling from an unconstrained space using a Sobol sequence. Source code in opti/sampling/base.py def sobol_sampling ( n_samples : int , parameters : Parameters ) -> pd . DataFrame : \"\"\"Super-uniform sampling from an unconstrained space using a Sobol sequence.\"\"\" d = len ( parameters ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( d ) . random ( n_samples ) res = [] for i , p in enumerate ( parameters ): if isinstance ( p , Continuous ): x = p . from_unit_range ( X [:, i ]) else : bins = np . linspace ( 0 , 1 , len ( p . domain ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( p . domain )[ idx ] res . append ( pd . Series ( x , name = p . name )) return pd . concat ( res , axis = 1 )","title":"sobol_sampling()"},{"location":"ref-sampling/#opti.sampling.base.split_nchoosek","text":"Split constraints in n-choose-k constraint and all other constraints. Source code in opti/sampling/base.py def split_nchoosek ( constraints : Optional [ Constraints ], ) -> Tuple [ Constraints , Constraints ]: \"\"\"Split constraints in n-choose-k constraint and all other constraints.\"\"\" if constraints is None : return Constraints ([]), Constraints ([]) nchoosek_constraints = Constraints ( [ c for c in constraints if isinstance ( c , NChooseK )] ) other_constraints = Constraints ( [ c for c in constraints if not isinstance ( c , NChooseK )] ) return nchoosek_constraints , other_constraints","title":"split_nchoosek()"},{"location":"ref-sampling/#opti.sampling.polytope","text":"This module provides functions to uniformly sample points subject to a system of linear inequality constraints, :math: Ax <= b (convex polytope), and linear equality constraints, :math: Ax = b (affine projection). A comparison of MCMC algorithms to generate uniform samples over a convex polytope is given in [Chen2018] . Here, we use the Hit & Run algorithm described in [Smith1984] . The R-package hitandrun _ provides similar functionality to this module.","title":"polytope"},{"location":"ref-sampling/#opti.sampling.polytope--references","text":".. [Chen2018] Chen Y., Dwivedi, R., Wainwright, M., Yu B. (2018) Fast MCMC Sampling Algorithms on Polytopes. JMLR, 19(55):1\u221286 https://arxiv.org/abs/1710.08165 .. [Smith1984] Smith, R. (1984). Efficient Monte Carlo Procedures for Generating Points Uniformly Distributed Over Bounded Regions. Operations Research, 32(6), 1296-1308. www.jstor.org/stable/170949 .. _ hitandrun : https://cran.r-project.org/web/packages/hitandrun/index.html","title":"References"},{"location":"ref-sampling/#opti.sampling.polytope.polytope_sampling","text":"Hit-and-run method to sample uniformly under linear constraints. Parameters: Name Type Description Default n_samples int Number of samples. required parameters opti.Parameters Parameter space. required constraints opti.Constraints Constraints on the parameters. required thin int Thinning factor of the generated samples. 100 Returns: Type Description array, shape=(n_samples, dimension) Randomly sampled points. Source code in opti/sampling/polytope.py def polytope_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , thin : int = 100 ) -> pd . DataFrame : \"\"\"Hit-and-run method to sample uniformly under linear constraints. Args: n_samples (int): Number of samples. parameters (opti.Parameters): Parameter space. constraints (opti.Constraints): Constraints on the parameters. thin (int, optional): Thinning factor of the generated samples. Returns: array, shape=(n_samples, dimension): Randomly sampled points. \"\"\" for c in constraints : if not isinstance ( c , ( LinearEquality , LinearInequality )): raise Exception ( \"Polytope sampling only works for linear constraints.\" ) At , bt , N , xp = _get_AbNx ( parameters , constraints ) # hit & run sampling x0 = _chebyshev_center ( At , bt ) sampler = _hitandrun ( At , bt , x0 ) X = np . empty (( n_samples , At . shape [ 1 ])) for i in range ( n_samples ): for _ in range ( thin - 1 ): next ( sampler ) X [ i ] = next ( sampler ) # project back X = X @ N . T + xp return pd . DataFrame ( columns = parameters . names , data = X )","title":"polytope_sampling()"},{"location":"ref-sampling/#opti.sampling.simplex","text":"","title":"simplex"},{"location":"ref-sampling/#opti.sampling.simplex.grid","text":"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Parameters: Name Type Description Default dimension int Number of variables. required levels int Number of levels for each variable. required Returns: Type Description array Regularily spaced points on the unit simplex. Examples: >>> simplex_grid ( 3 , 3 ) array ([ [ 0. , 0. , 1. ], [ 0. , 0.5 , 0.5 ], [ 0. , 1. , 0. ], [ 0.5 , 0. , 0.5 ], [ 0.5 , 0.5 , 0. ], [ 1. , 0. , 0. ] ]) References Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. Source code in opti/sampling/simplex.py def grid ( dimension : int , levels : int ) -> np . ndarray : \"\"\"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Args: dimension (int): Number of variables. levels (int): Number of levels for each variable. Returns: array: Regularily spaced points on the unit simplex. Examples: >>> simplex_grid(3, 3) array([ [0. , 0. , 1. ], [0. , 0.5, 0.5], [0. , 1. , 0. ], [0.5, 0. , 0.5], [0.5, 0.5, 0. ], [1. , 0. , 0. ] ]) References: Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. \"\"\" m = dimension n = levels - 1 L = int ( comb ( dimension - 1 + levels - 1 , dimension - 1 , exact = True )) x = np . zeros ( m , dtype = int ) x [ - 1 ] = n out = np . empty (( L , m ), dtype = int ) out [ 0 ] = x h = m for i in range ( 1 , L ): h -= 1 val = x [ h ] x [ h ] = 0 x [ h - 1 ] += 1 x [ - 1 ] = val - 1 if val != 1 : h = m out [ i ] = x return out / n","title":"grid()"},{"location":"ref-sampling/#opti.sampling.simplex.sample","text":"Sample uniformly from the unit simplex. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/simplex.py def sample ( dimension : int , n_samples : int = 1 ) -> np . ndarray : \"\"\"Sample uniformly from the unit simplex. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" s = np . random . standard_exponential (( n_samples , dimension )) return ( s . T / s . sum ( axis = 1 )) . T","title":"sample()"},{"location":"ref-sampling/#opti.sampling.sphere","text":"","title":"sphere"},{"location":"ref-sampling/#opti.sampling.sphere.sample","text":"Sample uniformly from the unit hypersphere. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 positive bool Sample from the non-negative unit-sphere. False Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/sphere.py def sample ( dimension : int , n_samples : int = 1 , positive : bool = False ) -> np . ndarray : \"\"\"Sample uniformly from the unit hypersphere. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. positive (bool): Sample from the non-negative unit-sphere. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" x = np . random . normal ( 0 , 1 , size = ( n_samples , dimension )) x = x / np . sum ( x ** 2 , axis = 1 , keepdims = True ) ** 0.5 if positive : x *= - 2 * ( x < 0 ) + 1 return x","title":"sample()"},{"location":"ref-tools/","text":"Tools modde MipFile File reader for MODDE investigation files (.mip) Source code in opti/tools/modde.py class MipFile : \"\"\"File reader for MODDE investigation files (.mip)\"\"\" def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data () def _get_experimental_data ( self ): \"\"\"Parse the experimental data. Returns: pd.DataFrame: dataframe of experimental data \"\"\" part = [ p for p in self . parts if p . startswith ( \"ExpNo\" )][ 0 ] return pd . read_csv ( io . StringIO ( part ), delimiter = \" \\t \" , index_col = \"ExpNo\" ) def _get_design_settings ( self ): \"\"\"Parse the design settings. Returns: dict of dicts: dictionary with 'Factors', 'Responses', 'Options' as well as the individual variables. \"\"\" part = [ p for p in self . parts if p . startswith ( \"[Status]\" )][ 0 ] settings = {} for line in part . split ( \" \\n \" ): if len ( line ) == 0 : continue if line . startswith ( \"[\" ): thing = line . strip ( \"[]\" ) settings [ thing ] = {} else : key , value = line . split ( \"=\" ) settings [ thing ][ key ] = value return settings __init__ ( self , filename ) special Read in a MODDE file. Parameters: Name Type Description Default filename str or path path to MODDE file required Source code in opti/tools/modde.py def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data () read_modde ( filepath ) Read a problem specification from a MODDE file. Parameters: Name Type Description Default filepath path-like path to MODDE .mip file required Returns: Type Description opti.Problem problem specification Source code in opti/tools/modde.py def read_modde ( filepath ): \"\"\"Read a problem specification from a MODDE file. Args: filepath (path-like): path to MODDE .mip file Returns: opti.Problem: problem specification \"\"\" mip = MipFile ( filepath ) inputs = [] outputs = [] constraints = [] formulation_parameters = [] # build input space for name , props in mip . factors . items (): if props [ \"Use\" ] == \"Uncontrolled\" : print ( f \"Uncontrolled factors not supported. Skipping { name } \" ) continue if props [ \"Type\" ] == \"Formulation\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) formulation_parameters . append ( name ) elif props [ \"Type\" ] == \"Quantitative\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Multilevel\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Discrete ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Qualitative\" : domain = props [ \"Settings\" ] . split ( \",\" ) inputs . append ( opti . Categorical ( name = name , domain = domain )) inputs = opti . Parameters ( inputs ) # build formulation constraint constraints . append ( opti . constraint . LinearEquality ( names = formulation_parameters , lhs = np . ones ( len ( formulation_parameters )), rhs = 1 , ) ) # build output space for name , props in mip . responses . items (): # check if data available that allows to infer the domain vmin = mip . data [ name ] . min () vmax = mip . data [ name ] . max () if np . isfinite ( vmin ) or np . isfinite ( vmax ) and vmin < vmax : domain = [ vmin , vmax ] else : domain = [ 0 , 1 ] dim = opti . Continuous ( name = name , domain = domain ) outputs . append ( dim ) outputs = opti . Parameters ( outputs ) # data data = mip . data . drop ( columns = [ \"ExpName\" , \"InOut\" ]) return opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , data = data ) noisify noisify_problem ( problem , noisifiers ) Creates a new problem that is based on the given one plus noise on the outputs. Parameters: Name Type Description Default problem Problem given problem where we will add noise required noisifiers List[Callable] list of functions that add noise to the outputs required Returns: new problem with noise on the output Source code in opti/tools/noisify.py def noisify_problem ( problem : Problem , noisifiers : List [ Callable ], ) -> Problem : \"\"\"Creates a new problem that is based on the given one plus noise on the outputs. Args: problem: given problem where we will add noise noisifiers: list of functions that add noise to the outputs Returns: new problem with noise on the output \"\"\" def noisy_f ( X ): return _add_noise_to_data ( problem . f ( X ), noisifiers , problem . outputs ) if problem . data is not None : data = problem . get_data () X = data [ problem . inputs . names ] Yn = _add_noise_to_data ( data [ problem . outputs . names ], noisifiers , problem . outputs ) data = pd . concat ([ X , Yn ], axis = 1 ) else : data = None return Problem ( inputs = problem . inputs , outputs = problem . outputs , objectives = problem . objectives , constraints = problem . constraints , output_constraints = problem . output_constraints , f = noisy_f , data = data , name = problem . name , ) noisify_problem_with_gaussian ( problem , mu = 0 , sigma = 0.05 ) Given an instance of a problem, this returns the problem with additive Gaussian noise Parameters: Name Type Description Default problem Problem problem instance where we add the noise required mu float mean of the Gaussian noise to be added 0 sigma float standard deviation of the Gaussian noise to be added 0.05 Returns: input problem with additive Gaussian noise Source code in opti/tools/noisify.py def noisify_problem_with_gaussian ( problem : Problem , mu : float = 0 , sigma : float = 0.05 ): \"\"\" Given an instance of a problem, this returns the problem with additive Gaussian noise Args: problem: problem instance where we add the noise mu: mean of the Gaussian noise to be added sigma: standard deviation of the Gaussian noise to be added Returns: input problem with additive Gaussian noise \"\"\" def noisify ( y ): rv = norm ( loc = mu , scale = sigma ) return y + rv . rvs ( len ( y )) return noisify_problem ( problem , noisifiers = [ noisify ] * len ( problem . outputs )) sanitize sanitize_problem ( problem ) This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives. More precisely, the resulting problem has the following properties: - Inputs are named input_0 , input_1 , .... Outputs are named analogously. - The data is scaled per feature to [0, 1] . - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of f are dropped if there are any. Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints. Parameters: Name Type Description Default problem Problem to be sanitized required Exceptions: Type Description TypeError in case there are unsupported constraints, data is None, or there are output constraints Returns: Type Description Problem Problem instance with sanitized labels and normalized data Source code in opti/tools/sanitize.py def sanitize_problem ( problem : Problem ) -> Problem : \"\"\" This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives. More precisely, the resulting problem has the following properties: - Inputs are named `input_0`, `input_1`, .... Outputs are named analogously. - The data is scaled per feature to `[0, 1]`. - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of `f` are dropped if there are any. Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints. Args: problem: to be sanitized Raises: TypeError: in case there are unsupported constraints, data is None, or there are output constraints Returns: Problem instance with sanitized labels and normalized data \"\"\" if problem . data is None : raise TypeError ( \"we cannot sanitize a problem without data\" ) if problem . output_constraints is not None : raise TypeError ( \"output constraints are currently not supported\" ) if getattr ( problem , \"f\" , None ) is not None : warnings . warn ( \"f is not sanitized but dropped\" ) if problem . models is not None : warnings . warn ( \"models are not sanitized but dropped\" ) inputs = _sanitize_params ( problem . inputs , \"input\" ) input_name_map = { pi . name : i . name for pi , i in zip ( problem . inputs , inputs )} normalized_in_data , xmin , \u0394x = _normalize_parameters_data ( problem . data , problem . inputs ) outputs = _sanitize_params ( problem . outputs , \"output\" ) output_name_map = { pi . name : i . name for pi , i in zip ( problem . outputs , outputs )} normalized_out_data , ymin , \u0394y = _normalize_parameters_data ( problem . data , problem . outputs ) normalized_in_data . columns = inputs . names normalized_out_data . columns = outputs . names normalized_data = pd . concat ([ normalized_in_data , normalized_out_data ], axis = 1 ) normalized_data . reset_index ( inplace = True , drop = True ) objectives = deepcopy ( problem . objectives ) for obj in objectives : sanitized_name = output_name_map [ obj . name ] i = outputs . names . index ( sanitized_name ) obj . name = sanitized_name obj . parameter = sanitized_name obj . target = ( obj . target - ymin [ i ]) / \u0394y [ i ] if hasattr ( obj , \"tolerance\" ): obj . tolerance /= \u0394y [ i ] constraints = deepcopy ( problem . constraints ) if constraints is not None : for c in constraints : c . names = [ input_name_map [ n ] for n in c . names ] if isinstance ( c , ( LinearEquality , LinearInequality )): c . lhs = ( c . lhs + xmin ) * \u0394x if c . rhs > 1e-5 : c . lhs = c . lhs / c . rhs c . rhs = 1.0 elif isinstance ( c , NChooseK ): pass else : raise TypeError ( \"sanitizer only supports linear and n-choose-k constraints\" ) normalized_problem = Problem ( inputs = inputs , outputs = outputs , objectives = objectives , constraints = constraints , data = normalized_data , ) return normalized_problem","title":"Tools"},{"location":"ref-tools/#tools","text":"","title":"Tools"},{"location":"ref-tools/#opti.tools.modde","text":"","title":"modde"},{"location":"ref-tools/#opti.tools.modde.MipFile","text":"File reader for MODDE investigation files (.mip) Source code in opti/tools/modde.py class MipFile : \"\"\"File reader for MODDE investigation files (.mip)\"\"\" def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data () def _get_experimental_data ( self ): \"\"\"Parse the experimental data. Returns: pd.DataFrame: dataframe of experimental data \"\"\" part = [ p for p in self . parts if p . startswith ( \"ExpNo\" )][ 0 ] return pd . read_csv ( io . StringIO ( part ), delimiter = \" \\t \" , index_col = \"ExpNo\" ) def _get_design_settings ( self ): \"\"\"Parse the design settings. Returns: dict of dicts: dictionary with 'Factors', 'Responses', 'Options' as well as the individual variables. \"\"\" part = [ p for p in self . parts if p . startswith ( \"[Status]\" )][ 0 ] settings = {} for line in part . split ( \" \\n \" ): if len ( line ) == 0 : continue if line . startswith ( \"[\" ): thing = line . strip ( \"[]\" ) settings [ thing ] = {} else : key , value = line . split ( \"=\" ) settings [ thing ][ key ] = value return settings","title":"MipFile"},{"location":"ref-tools/#opti.tools.modde.MipFile.__init__","text":"Read in a MODDE file. Parameters: Name Type Description Default filename str or path path to MODDE file required Source code in opti/tools/modde.py def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data ()","title":"__init__()"},{"location":"ref-tools/#opti.tools.modde.read_modde","text":"Read a problem specification from a MODDE file. Parameters: Name Type Description Default filepath path-like path to MODDE .mip file required Returns: Type Description opti.Problem problem specification Source code in opti/tools/modde.py def read_modde ( filepath ): \"\"\"Read a problem specification from a MODDE file. Args: filepath (path-like): path to MODDE .mip file Returns: opti.Problem: problem specification \"\"\" mip = MipFile ( filepath ) inputs = [] outputs = [] constraints = [] formulation_parameters = [] # build input space for name , props in mip . factors . items (): if props [ \"Use\" ] == \"Uncontrolled\" : print ( f \"Uncontrolled factors not supported. Skipping { name } \" ) continue if props [ \"Type\" ] == \"Formulation\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) formulation_parameters . append ( name ) elif props [ \"Type\" ] == \"Quantitative\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Multilevel\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Discrete ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Qualitative\" : domain = props [ \"Settings\" ] . split ( \",\" ) inputs . append ( opti . Categorical ( name = name , domain = domain )) inputs = opti . Parameters ( inputs ) # build formulation constraint constraints . append ( opti . constraint . LinearEquality ( names = formulation_parameters , lhs = np . ones ( len ( formulation_parameters )), rhs = 1 , ) ) # build output space for name , props in mip . responses . items (): # check if data available that allows to infer the domain vmin = mip . data [ name ] . min () vmax = mip . data [ name ] . max () if np . isfinite ( vmin ) or np . isfinite ( vmax ) and vmin < vmax : domain = [ vmin , vmax ] else : domain = [ 0 , 1 ] dim = opti . Continuous ( name = name , domain = domain ) outputs . append ( dim ) outputs = opti . Parameters ( outputs ) # data data = mip . data . drop ( columns = [ \"ExpName\" , \"InOut\" ]) return opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , data = data )","title":"read_modde()"},{"location":"ref-tools/#opti.tools.noisify","text":"","title":"noisify"},{"location":"ref-tools/#opti.tools.noisify.noisify_problem","text":"Creates a new problem that is based on the given one plus noise on the outputs. Parameters: Name Type Description Default problem Problem given problem where we will add noise required noisifiers List[Callable] list of functions that add noise to the outputs required Returns: new problem with noise on the output Source code in opti/tools/noisify.py def noisify_problem ( problem : Problem , noisifiers : List [ Callable ], ) -> Problem : \"\"\"Creates a new problem that is based on the given one plus noise on the outputs. Args: problem: given problem where we will add noise noisifiers: list of functions that add noise to the outputs Returns: new problem with noise on the output \"\"\" def noisy_f ( X ): return _add_noise_to_data ( problem . f ( X ), noisifiers , problem . outputs ) if problem . data is not None : data = problem . get_data () X = data [ problem . inputs . names ] Yn = _add_noise_to_data ( data [ problem . outputs . names ], noisifiers , problem . outputs ) data = pd . concat ([ X , Yn ], axis = 1 ) else : data = None return Problem ( inputs = problem . inputs , outputs = problem . outputs , objectives = problem . objectives , constraints = problem . constraints , output_constraints = problem . output_constraints , f = noisy_f , data = data , name = problem . name , )","title":"noisify_problem()"},{"location":"ref-tools/#opti.tools.noisify.noisify_problem_with_gaussian","text":"Given an instance of a problem, this returns the problem with additive Gaussian noise Parameters: Name Type Description Default problem Problem problem instance where we add the noise required mu float mean of the Gaussian noise to be added 0 sigma float standard deviation of the Gaussian noise to be added 0.05 Returns: input problem with additive Gaussian noise Source code in opti/tools/noisify.py def noisify_problem_with_gaussian ( problem : Problem , mu : float = 0 , sigma : float = 0.05 ): \"\"\" Given an instance of a problem, this returns the problem with additive Gaussian noise Args: problem: problem instance where we add the noise mu: mean of the Gaussian noise to be added sigma: standard deviation of the Gaussian noise to be added Returns: input problem with additive Gaussian noise \"\"\" def noisify ( y ): rv = norm ( loc = mu , scale = sigma ) return y + rv . rvs ( len ( y )) return noisify_problem ( problem , noisifiers = [ noisify ] * len ( problem . outputs ))","title":"noisify_problem_with_gaussian()"},{"location":"ref-tools/#opti.tools.sanitize","text":"","title":"sanitize"},{"location":"ref-tools/#opti.tools.sanitize.sanitize_problem","text":"This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives. More precisely, the resulting problem has the following properties: - Inputs are named input_0 , input_1 , .... Outputs are named analogously. - The data is scaled per feature to [0, 1] . - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of f are dropped if there are any. Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints. Parameters: Name Type Description Default problem Problem to be sanitized required Exceptions: Type Description TypeError in case there are unsupported constraints, data is None, or there are output constraints Returns: Type Description Problem Problem instance with sanitized labels and normalized data Source code in opti/tools/sanitize.py def sanitize_problem ( problem : Problem ) -> Problem : \"\"\" This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives. More precisely, the resulting problem has the following properties: - Inputs are named `input_0`, `input_1`, .... Outputs are named analogously. - The data is scaled per feature to `[0, 1]`. - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of `f` are dropped if there are any. Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints. Args: problem: to be sanitized Raises: TypeError: in case there are unsupported constraints, data is None, or there are output constraints Returns: Problem instance with sanitized labels and normalized data \"\"\" if problem . data is None : raise TypeError ( \"we cannot sanitize a problem without data\" ) if problem . output_constraints is not None : raise TypeError ( \"output constraints are currently not supported\" ) if getattr ( problem , \"f\" , None ) is not None : warnings . warn ( \"f is not sanitized but dropped\" ) if problem . models is not None : warnings . warn ( \"models are not sanitized but dropped\" ) inputs = _sanitize_params ( problem . inputs , \"input\" ) input_name_map = { pi . name : i . name for pi , i in zip ( problem . inputs , inputs )} normalized_in_data , xmin , \u0394x = _normalize_parameters_data ( problem . data , problem . inputs ) outputs = _sanitize_params ( problem . outputs , \"output\" ) output_name_map = { pi . name : i . name for pi , i in zip ( problem . outputs , outputs )} normalized_out_data , ymin , \u0394y = _normalize_parameters_data ( problem . data , problem . outputs ) normalized_in_data . columns = inputs . names normalized_out_data . columns = outputs . names normalized_data = pd . concat ([ normalized_in_data , normalized_out_data ], axis = 1 ) normalized_data . reset_index ( inplace = True , drop = True ) objectives = deepcopy ( problem . objectives ) for obj in objectives : sanitized_name = output_name_map [ obj . name ] i = outputs . names . index ( sanitized_name ) obj . name = sanitized_name obj . parameter = sanitized_name obj . target = ( obj . target - ymin [ i ]) / \u0394y [ i ] if hasattr ( obj , \"tolerance\" ): obj . tolerance /= \u0394y [ i ] constraints = deepcopy ( problem . constraints ) if constraints is not None : for c in constraints : c . names = [ input_name_map [ n ] for n in c . names ] if isinstance ( c , ( LinearEquality , LinearInequality )): c . lhs = ( c . lhs + xmin ) * \u0394x if c . rhs > 1e-5 : c . lhs = c . lhs / c . rhs c . rhs = 1.0 elif isinstance ( c , NChooseK ): pass else : raise TypeError ( \"sanitizer only supports linear and n-choose-k constraints\" ) normalized_problem = Problem ( inputs = inputs , outputs = outputs , objectives = objectives , constraints = constraints , data = normalized_data , ) return normalized_problem","title":"sanitize_problem()"}]}