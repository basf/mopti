{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The primary purpose of opti is to define tasks or problems in a number of closely related fields, including experimental design, multiobjective optimization and decision making and Bayesian optimization.</p> <p>Opti specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.</p>"},{"location":"#experimental-design","title":"Experimental design","text":"<p>In the context of experimental design opti allows to define a design space</p> \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] <p>where the design parameters may take values depending on their type and domain, e.g.</p> <ul> <li>continuous: \\(x_1 \\in [0, 1]\\)</li> <li>discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\)</li> <li>categorical: \\(x_3 \\in \\{A, B, C\\}\\)</li> </ul> <p>and a set of equations define additional experimental constraints, e.g.</p> <ul> <li>linear equality: \\(\\sum x_i = 1\\)</li> <li>linear inequality: \\(2 x_1 \\leq x_2\\)</li> <li>non-linear inequality: \\(\\sum x_i^2 \\leq 1\\)</li> <li>n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.</li> </ul>"},{"location":"#multiobjective-optimization","title":"Multiobjective optimization","text":"<p>In the context of multiobjective optimization opti allows to define a vector-valued optimization problem</p> \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] <p>where</p> <ul> <li>\\(x \\in \\mathbb{X}\\) is again the experimental design space</li> <li>\\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and</li> <li>\\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized.</li> </ul> <p>Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.</p>"},{"location":"#bayesian-optimization","title":"Bayesian optimization","text":"<p>In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\). An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation</p> \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] <p>and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.</p>"},{"location":"install/","title":"Install","text":"<p>Note: the package name is <code>mopti</code> while the import name is <code>opti</code>. <pre><code>pip install mopti\n</code></pre> or install the latest version with <pre><code>pip install git+https://github.com/basf/mopti.git\n</code></pre></p>"},{"location":"problem/","title":"Getting started","text":"<p>Opti problems consist of a definition of </p> <ul> <li>the input parameters \\(x \\in \\mathbb{X}\\), </li> <li>the output parameters \\(y \\in \\mathbb{Y}\\), </li> <li>the objectives \\(s(y)\\) (optional), </li> <li>the input constraints \\(g(x) \\leq 0\\) (optional), </li> <li>the output constraints \\(h(y)\\) (optional),</li> <li>a data set of previous function evaluations \\(\\{x, y\\}\\) (optional)</li> <li>and the function \\(f(x)\\) to be optimized (optional).</li> </ul>"},{"location":"problem/#parameters","title":"Parameters","text":"<p>Input and output spaces are defined using <code>Parameters</code> objects.  For example a mixed input space of three continuous, one discrete and one categorical parameter(s), along with an output space of continuous parameters can be defined as:</p> <p><pre><code>import opti\n\ninputs = opti.Parameters([\n    opti.Continuous(\"x1\", domain=[0, 1]),\n    opti.Continuous(\"x2\", domain=[0, 1]),\n    opti.Continuous(\"x3\", domain=[0, 1]),\n    opti.Discrete(\"x4\", domain=[1, 2, 5, 7.5]),\n    opti.Categorical(\"x5\", domain=[\"A\", \"B\", \"C\"])\n])\n\noutputs = opti.Parameters([\n    opti.Continuous(\"y1\", domain=[0, None]),\n    opti.Continuous(\"y2\", domain=[None, None]),\n    opti.Continuous(\"y3\", domain=[0, 100])\n])\n</code></pre> Note that for some of the outputs we didn't specify bounds as we may not know them.</p> <p>Individual parameters can be indexed by name. <pre><code>inputs[\"x5\"]\n&gt;&gt;&gt; Categorical(\"x5\", domain=[\"A\", \"B\", \"C\"])\n</code></pre> and all parameter names can retrieved with <pre><code>inputs.names\n&gt;&gt;&gt; [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"]\n</code></pre></p> <p>We can sample from individual parameters, parameter spaces or parameter spaces including constraints (more on that later) <pre><code>x5 = inputs[\"x1\"].sample(3)\nprint(x5.values)\n&gt;&gt;&gt; array([0.72405216, 0.14914942, 0.46051132])\n\nX = inputs.sample(5)\nprint(X)\n&gt;&gt;&gt;      x1        x2        x3   x4 x5\n0  0.760116  0.063584  0.518885  7.5  A\n1  0.807928  0.496213  0.885545  1.0  C\n2  0.351253  0.993993  0.340414  5.0  B\n3  0.385825  0.857306  0.355267  1.0  C\n4  0.191907  0.993494  0.384322  2.0  A\n</code></pre></p> <p>We can also check for each point in a dataframe, whether it is contained in the space. <pre><code>inputs.contains(X)\n&gt;&gt;&gt; array([ True,  True,  True,  True,  True])\n</code></pre></p> <p>In general all opti functions operate on dataframes and thus use the parameter name to identify corresponding column.  Hence, a dataframe may contain additional columns and columns may be in arbitrary order. The index of a dataframe is preserved, meaning that the returned dataframe will have the same indices as the original dataframe.</p>"},{"location":"problem/#constraints","title":"Constraints","text":"<p>Input constraints are defined separately from the input space. The following constraints are supported.</p> <p>Linear constraints (<code>LinearEquality</code> and <code>LinearInequality</code>) are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input parameters they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\). <pre><code># A mixture: x1 + x2 + x3 = 1\nconstr1 = opti.LinearEquality([\"x1\", \"x2\", \"x3\"], lhs=1, rhs=1)\n\n# x1 + 2 * x3 &lt; 0.8\nconstr2 = opti.LinearInequality([\"x1\", \"x3\"], lhs=[1, 2], rhs=0.8)\n</code></pre> Because of the product \\(a_i x_i\\), linear constraints cannot operate on categorical parameters.</p> <p>Nonlinear constraints (<code>NonlinearEquality</code> and <code>NonlinearInequality</code>) take any expression that can be evaluated by pandas.eval, including mathematical operators such as <code>sin</code>, <code>exp</code>, <code>log10</code> or exponentiation. <pre><code># The unit circle: x1**2 + x2**2 = 1\nconstr3 = opti.NonlinearEquality(\"x1**2 + x2**2 - 1\")\n</code></pre> Nonlinear constraints can also operate on categorical parameters and support conditional statements. <pre><code># Require x1 &lt; 0.5 if x5 == \"A\"\nconstr4 = opti.NonlinearInequality(\"(x1 - 0.5) * (x5 =='A')\")\n</code></pre></p> <p>Finally, there is a combinatorical constraint (<code>NChooseK</code>) to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe. <pre><code># Only 2 out of 3 parameters can be greater than zero\nconstr5 = opti.NChooseK([\"x1\", \"x2\", \"x3\"], max_active=2)\n</code></pre></p> <p>Constraints can be grouped in a container which acts as the union constraints. <pre><code>constraints = opti.Constraints([constr1, constr2, constr3, constr4, constr5])\n</code></pre></p> <p>We can check whether a point satisfies individual constraints or the list of constraints. <pre><code>constr2.satisfied(X).values\n&gt;&gt;&gt; array([False, False, True, True, True])\n</code></pre></p> <p>The distance to the constraint boundary can also be evaluated for use in numerical optimization methods, where values \\(\\leq 0\\) correspond to a satisified constraint. <pre><code>constr2(X).values\n&gt;&gt;&gt; array([ 0.479001  ,  0.89347371, -0.10833372, -0.05890873, -0.22377122])\n</code></pre></p> <p>Opti contains a number of methods to draw random samples from constrained spaces, see the sampling reference.</p>"},{"location":"problem/#objectives","title":"Objectives","text":"<p>In an optimization problem we need to define the target direction or target value individually for each output. This is done using objectives \\(s_m(y_m)\\) so that a mixed objective optimization becomes a minimization problem. <pre><code>objectives = opti.Objectives([\n    opti.Minimize(\"y1\"),\n    opti.Maximize(\"y2\"),\n    opti.CloseToTarget(\"y3\", target=7)\n])\n</code></pre></p> <p>We can compute objective values from output values to see the objective transformation applied. <pre><code>Y = pd.DataFrame({\n    \"y1\": [1, 2, 3],\n    \"y2\": [7, 4, 5],\n    \"y3\": [5, 6.9, 12]\n})\nobjectives(Y)\n&gt;&gt;&gt; minimize_y1  maximize_y2  closetotarget_y3\n0            1           -7              4.00\n1            2           -4              0.01\n2            3           -5             25.00\n</code></pre></p> <p>Objectives can also be used as output constraints.  This is different from an objective in that we want the constraint to be satisfied and not explore possible tradeoffs.</p>"},{"location":"problem/#problem","title":"Problem","text":"<p>Finally, a problem is the combination of inputs, outputs, objectives, constraints, output_constraints, (true) function and data.</p> <p><pre><code>problem = opti.Problem(\n    inputs=inputs,\n    outputs=outputs,\n    constraints=constraints,\n    objectives=objectives\n)\n</code></pre> Problems can be serialized to and from a dictionary <pre><code>config = problem.to_config()\nproblem = opti.Problem(**config)\n</code></pre> or to a json file <pre><code>problem.to_json(\"problem.json\")\nproblem = opti.read_json(\"problem.json\")\n</code></pre></p>"},{"location":"ref-constraint/","title":"Constraints","text":""},{"location":"ref-constraint/#opti.constraint.Constraint","title":"<code> Constraint        </code>","text":"<p>Base class to define constraints on the input space, g(x) == 0 or g(x) &lt;= 0.</p> Source code in <code>opti/constraint.py</code> <pre><code>class Constraint:\n\"\"\"Base class to define constraints on the input space, g(x) == 0 or g(x) &lt;= 0.\"\"\"\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Numerically evaluate the constraint g(x).\"\"\"\n        raise NotImplementedError\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Numerically evaluate the jacobian of the constraint J_g(x)\"\"\"\n        raise NotImplementedError\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.\"\"\"\n        raise NotImplementedError\n\n    def to_config(self) -&gt; Dict:\n        raise NotImplementedError\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraint.__call__","title":"<code>__call__(self, data)</code>  <code>special</code>","text":"<p>Numerically evaluate the constraint g(x).</p> Source code in <code>opti/constraint.py</code> <pre><code>def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Numerically evaluate the constraint g(x).\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraint.jacobian","title":"<code>jacobian(self, data)</code>","text":"<p>Numerically evaluate the jacobian of the constraint J_g(x)</p> Source code in <code>opti/constraint.py</code> <pre><code>def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Numerically evaluate the jacobian of the constraint J_g(x)\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraint.satisfied","title":"<code>satisfied(self, data)</code>","text":"<p>Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints","title":"<code> Constraints        </code>","text":"<p>List of input constraints</p> Source code in <code>opti/constraint.py</code> <pre><code>class Constraints:\n\"\"\"List of input constraints\"\"\"\n\n    def __init__(self, constraints: Sequence):\n        self.constraints = []\n        for c in constraints:\n            if not isinstance(c, Constraint):\n                if \"names\" in c and len(c[\"names\"]) == 0:\n                    continue  # skip empty constraints\n                c = make_constraint(**c)\n            self.constraints.append(c)\n\n    def __repr__(self):\n        return \"Constraints(\\n\" + pprint.pformat(self.constraints) + \"\\n)\"\n\n    def __iter__(self):\n        return iter(self.constraints)\n\n    def __len__(self):\n        return len(self.constraints)\n\n    def __getitem__(self, i):\n        return self.constraints[i]\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Numerically evaluate all constraints.\n\n        Args:\n            data: Data to evaluate the constraints on.\n\n        Returns:\n            Constraint evaluation g(x) for each of the constraints.\n        \"\"\"\n        return pd.concat([c(data) for c in self.constraints], axis=1)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; List:\n\"\"\"Numerically evaluate all constraint gradients.\n\n        Args:\n            data: Data to evaluate the constraint gradients on.\n\n        Returns:\n            Jacobian evaluation J_g(x) for each of the constraints as a list of dataframes.\n        \"\"\"\n        return [c.jacobian(data) for c in self.constraints]\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if all constraints are satisfied.\n\n        Args:\n            data: Data to evaluate the constraints on.\n\n        Returns:\n            Series of booleans indicating if all constraints are satisfied.\n        \"\"\"\n        return pd.concat([c.satisfied(data) for c in self.constraints], axis=1).all(\n            axis=1\n        )\n\n    def to_config(self) -&gt; List[Dict]:\n        return [obj.to_config() for obj in self.constraints]\n\n    def get(self, types) -&gt; \"Constraints\":\n\"\"\"Get all constraints of the given type(s).\"\"\"\n        return Constraints([c for c in self if isinstance(c, types)])\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints.__call__","title":"<code>__call__(self, data)</code>  <code>special</code>","text":"<p>Numerically evaluate all constraints.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to evaluate the constraints on.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Constraint evaluation g(x) for each of the constraints.</p> Source code in <code>opti/constraint.py</code> <pre><code>def __call__(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Numerically evaluate all constraints.\n\n    Args:\n        data: Data to evaluate the constraints on.\n\n    Returns:\n        Constraint evaluation g(x) for each of the constraints.\n    \"\"\"\n    return pd.concat([c(data) for c in self.constraints], axis=1)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints.get","title":"<code>get(self, types)</code>","text":"<p>Get all constraints of the given type(s).</p> Source code in <code>opti/constraint.py</code> <pre><code>def get(self, types) -&gt; \"Constraints\":\n\"\"\"Get all constraints of the given type(s).\"\"\"\n    return Constraints([c for c in self if isinstance(c, types)])\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints.jacobian","title":"<code>jacobian(self, data)</code>","text":"<p>Numerically evaluate all constraint gradients.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to evaluate the constraint gradients on.</p> required <p>Returns:</p> Type Description <code>List</code> <p>Jacobian evaluation J_g(x) for each of the constraints as a list of dataframes.</p> Source code in <code>opti/constraint.py</code> <pre><code>def jacobian(self, data: pd.DataFrame) -&gt; List:\n\"\"\"Numerically evaluate all constraint gradients.\n\n    Args:\n        data: Data to evaluate the constraint gradients on.\n\n    Returns:\n        Jacobian evaluation J_g(x) for each of the constraints as a list of dataframes.\n    \"\"\"\n    return [c.jacobian(data) for c in self.constraints]\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints.satisfied","title":"<code>satisfied(self, data)</code>","text":"<p>Check if all constraints are satisfied.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to evaluate the constraints on.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series of booleans indicating if all constraints are satisfied.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if all constraints are satisfied.\n\n    Args:\n        data: Data to evaluate the constraints on.\n\n    Returns:\n        Series of booleans indicating if all constraints are satisfied.\n    \"\"\"\n    return pd.concat([c.satisfied(data) for c in self.constraints], axis=1).all(\n        axis=1\n    )\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearEquality","title":"<code> LinearEquality            (Constraint)         </code>","text":"Source code in <code>opti/constraint.py</code> <pre><code>class LinearEquality(Constraint):\n    def __init__(\n        self,\n        names: List[str],\n        lhs: Union[float, List[float], np.ndarray] = 1,\n        rhs: float = 0,\n    ):\n\"\"\"Linear / affine inequality of the form 'lhs * x == rhs'.\n\n        Args:\n            names: Parameter names that the constraint works on.\n            lhs: Left-hand side / coefficients of the constraint.\n            rhs: Right-hand side of the constraint.\n\n        Examples:\n            A mixture constraint where A, B and C need to add up to 100 can be defined as\n            ```\n            LinearEquality([\"A\", \"B\", \"C\"], rhs=100)\n            ```\n            If the coefficients of A, B and C are not 1 they are passed explicitly.\n            ```\n            LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n            ```\n        \"\"\"\n        self.names = names\n        if np.isscalar(lhs):\n            self.lhs = lhs * np.ones(len(names))\n        else:\n            self.lhs = np.asarray(lhs)\n        if self.lhs.shape != (len(names),):\n            raise ValueError(\"Number of parameters and coefficients/lhs don't match.\")\n        self.rhs = rhs\n        self.is_equality = True\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        return (data[self.names] @ self.lhs - self.rhs) / np.linalg.norm(self.lhs)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            np.tile(self.lhs / np.linalg.norm(self.lhs), [data.shape[0], 1]),\n            columns=[\"dg/d\" + name for name in self.names],\n        )\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return pd.Series(np.isclose(self(data), 0), index=data.index)\n\n    def __repr__(self):\n        return (\n            f\"LinearEquality(names={self.names}, lhs={list(self.lhs)}, rhs={self.rhs})\"\n        )\n\n    def to_config(self) -&gt; Dict:\n        return dict(\n            type=\"linear-equality\",\n            names=self.names,\n            lhs=self.lhs.tolist(),\n            rhs=self.rhs,\n        )\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearEquality.__init__","title":"<code>__init__(self, names, lhs=1, rhs=0)</code>  <code>special</code>","text":"<p>Linear / affine inequality of the form 'lhs * x == rhs'.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>Parameter names that the constraint works on.</p> required <code>lhs</code> <code>Union[float, List[float], numpy.ndarray]</code> <p>Left-hand side / coefficients of the constraint.</p> <code>1</code> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint.</p> <code>0</code> <p>Examples:</p> <p>A mixture constraint where A, B and C need to add up to 100 can be defined as <pre><code>LinearEquality([\"A\", \"B\", \"C\"], rhs=100)\n</code></pre> If the coefficients of A, B and C are not 1 they are passed explicitly. <pre><code>LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(\n    self,\n    names: List[str],\n    lhs: Union[float, List[float], np.ndarray] = 1,\n    rhs: float = 0,\n):\n\"\"\"Linear / affine inequality of the form 'lhs * x == rhs'.\n\n    Args:\n        names: Parameter names that the constraint works on.\n        lhs: Left-hand side / coefficients of the constraint.\n        rhs: Right-hand side of the constraint.\n\n    Examples:\n        A mixture constraint where A, B and C need to add up to 100 can be defined as\n        ```\n        LinearEquality([\"A\", \"B\", \"C\"], rhs=100)\n        ```\n        If the coefficients of A, B and C are not 1 they are passed explicitly.\n        ```\n        LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n        ```\n    \"\"\"\n    self.names = names\n    if np.isscalar(lhs):\n        self.lhs = lhs * np.ones(len(names))\n    else:\n        self.lhs = np.asarray(lhs)\n    if self.lhs.shape != (len(names),):\n        raise ValueError(\"Number of parameters and coefficients/lhs don't match.\")\n    self.rhs = rhs\n    self.is_equality = True\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearEquality.jacobian","title":"<code>jacobian(self, data)</code>","text":"<p>Numerically evaluate the jacobian of the constraint J_g(x)</p> Source code in <code>opti/constraint.py</code> <pre><code>def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    return pd.DataFrame(\n        np.tile(self.lhs / np.linalg.norm(self.lhs), [data.shape[0], 1]),\n        columns=[\"dg/d\" + name for name in self.names],\n    )\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearEquality.satisfied","title":"<code>satisfied(self, data)</code>","text":"<p>Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n    return pd.Series(np.isclose(self(data), 0), index=data.index)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearInequality","title":"<code> LinearInequality            (Constraint)         </code>","text":"Source code in <code>opti/constraint.py</code> <pre><code>class LinearInequality(Constraint):\n    def __init__(\n        self,\n        names: List[str],\n        lhs: Union[float, List[float], np.ndarray] = 1,\n        rhs: float = 0,\n    ):\n\"\"\"Linear / affine inequality of the form 'lhs * x &lt;= rhs'.\n\n        Args:\n            names: Parameter names that the constraint works on.\n            lhs: Left-hand side / coefficients of the constraint.\n            rhs: Right-hand side of the constraint.\n\n        Examples:\n            A mixture constraint where the values of A, B and C may not exceed 100 can be defined as\n            ```\n            LinearInequality([\"A\", \"B\", \"C\"], rhs=100)\n            ```\n            If the coefficients are not 1, they need to be passed explicitly.\n            ```\n            LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n            ```\n            Inequalities are alway of the form g(x) &lt;= 0. To define a the constraint g(x) &gt;=0 0, both `lhs` and `rhs` need to be multiplied by -1.\n            ```\n            LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100)\n            LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100)\n            ```\n        \"\"\"\n        self.names = names\n        if np.isscalar(lhs):\n            self.lhs = lhs * np.ones(len(names))\n        else:\n            self.lhs = np.asarray(lhs)\n        if self.lhs.shape != (len(names),):\n            raise ValueError(\"Number of parameters and coefficients/lhs don't match.\")\n        self.rhs = rhs\n        self.is_equality = False\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        return (data[self.names] @ self.lhs - self.rhs) / np.linalg.norm(self.lhs)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            np.tile(self.lhs / np.linalg.norm(self.lhs), [data.shape[0], 1]),\n            columns=[\"dg/d\" + name for name in self.names],\n        )\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return self(data) &lt;= 0\n\n    def __repr__(self):\n        return f\"LinearInequality(names={self.names}, lhs={list(self.lhs)}, rhs={self.rhs})\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(\n            type=\"linear-inequality\",\n            names=self.names,\n            lhs=self.lhs.tolist(),\n            rhs=self.rhs,\n        )\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearInequality.__init__","title":"<code>__init__(self, names, lhs=1, rhs=0)</code>  <code>special</code>","text":"<p>Linear / affine inequality of the form 'lhs * x &lt;= rhs'.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>Parameter names that the constraint works on.</p> required <code>lhs</code> <code>Union[float, List[float], numpy.ndarray]</code> <p>Left-hand side / coefficients of the constraint.</p> <code>1</code> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint.</p> <code>0</code> <p>Examples:</p> <p>A mixture constraint where the values of A, B and C may not exceed 100 can be defined as <pre><code>LinearInequality([\"A\", \"B\", \"C\"], rhs=100)\n</code></pre> If the coefficients are not 1, they need to be passed explicitly. <pre><code>LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n</code></pre> Inequalities are alway of the form g(x) &lt;= 0. To define a the constraint g(x) &gt;=0 0, both <code>lhs</code> and <code>rhs</code> need to be multiplied by -1. <pre><code>LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100)\nLinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100)\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(\n    self,\n    names: List[str],\n    lhs: Union[float, List[float], np.ndarray] = 1,\n    rhs: float = 0,\n):\n\"\"\"Linear / affine inequality of the form 'lhs * x &lt;= rhs'.\n\n    Args:\n        names: Parameter names that the constraint works on.\n        lhs: Left-hand side / coefficients of the constraint.\n        rhs: Right-hand side of the constraint.\n\n    Examples:\n        A mixture constraint where the values of A, B and C may not exceed 100 can be defined as\n        ```\n        LinearInequality([\"A\", \"B\", \"C\"], rhs=100)\n        ```\n        If the coefficients are not 1, they need to be passed explicitly.\n        ```\n        LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n        ```\n        Inequalities are alway of the form g(x) &lt;= 0. To define a the constraint g(x) &gt;=0 0, both `lhs` and `rhs` need to be multiplied by -1.\n        ```\n        LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100)\n        LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100)\n        ```\n    \"\"\"\n    self.names = names\n    if np.isscalar(lhs):\n        self.lhs = lhs * np.ones(len(names))\n    else:\n        self.lhs = np.asarray(lhs)\n    if self.lhs.shape != (len(names),):\n        raise ValueError(\"Number of parameters and coefficients/lhs don't match.\")\n    self.rhs = rhs\n    self.is_equality = False\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearInequality.jacobian","title":"<code>jacobian(self, data)</code>","text":"<p>Numerically evaluate the jacobian of the constraint J_g(x)</p> Source code in <code>opti/constraint.py</code> <pre><code>def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    return pd.DataFrame(\n        np.tile(self.lhs / np.linalg.norm(self.lhs), [data.shape[0], 1]),\n        columns=[\"dg/d\" + name for name in self.names],\n    )\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearInequality.satisfied","title":"<code>satisfied(self, data)</code>","text":"<p>Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n    return self(data) &lt;= 0\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NChooseK","title":"<code> NChooseK            (Constraint)         </code>","text":"Source code in <code>opti/constraint.py</code> <pre><code>class NChooseK(Constraint):\n    def __init__(self, names: List[str], max_active: int):\n\"\"\"Only k out of n values are allowed to take nonzero values.\n\n        Args:\n            names: Parameter names that the constraint works on.\n            max_active: Maximium number of non-zero parameter values.\n\n        Examples:\n            A choice of 2 or less from A, B, C, D or E can be defined as\n            ```\n            NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2)\n            ```\n        \"\"\"\n        self.names = names\n        self.max_active = max_active\n        self.is_equality = False\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        x = np.abs(data[self.names].values)\n        num_zeros = x.shape[1] - self.max_active\n        violation = np.apply_along_axis(\n            func1d=lambda r: sum(sorted(r)[:num_zeros]), axis=1, arr=x\n        )\n        return pd.Series(violation, index=data.index)\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return pd.Series(self(data) &lt;= 0, index=data.index)\n\n    def __repr__(self):\n        return f\"NChooseK(names={self.names}, max_active={self.max_active})\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(type=\"n-choose-k\", names=self.names, max_active=self.max_active)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NChooseK.__init__","title":"<code>__init__(self, names, max_active)</code>  <code>special</code>","text":"<p>Only k out of n values are allowed to take nonzero values.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>Parameter names that the constraint works on.</p> required <code>max_active</code> <code>int</code> <p>Maximium number of non-zero parameter values.</p> required <p>Examples:</p> <p>A choice of 2 or less from A, B, C, D or E can be defined as <pre><code>NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2)\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(self, names: List[str], max_active: int):\n\"\"\"Only k out of n values are allowed to take nonzero values.\n\n    Args:\n        names: Parameter names that the constraint works on.\n        max_active: Maximium number of non-zero parameter values.\n\n    Examples:\n        A choice of 2 or less from A, B, C, D or E can be defined as\n        ```\n        NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2)\n        ```\n    \"\"\"\n    self.names = names\n    self.max_active = max_active\n    self.is_equality = False\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NChooseK.satisfied","title":"<code>satisfied(self, data)</code>","text":"<p>Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n    return pd.Series(self(data) &lt;= 0, index=data.index)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality","title":"<code> NonlinearEquality            (Constraint)         </code>","text":"Source code in <code>opti/constraint.py</code> <pre><code>class NonlinearEquality(Constraint):\n    def __init__(\n        self,\n        expression: str,\n        jacobian: Optional[str] = None,\n        names: Optional[List[str]] = None,\n    ):\n\"\"\"Equality of the form 'expression == 0'.\n\n        Args:\n            expression: Mathematical expression that can be evaluated by `pandas.eval`.\n            jacobian: List of mathematical expressions that can be evaluated by `pandas.eval`.\n                The i-th expression should correspond to the partial derivative with respect to\n                the i-th variable. If `names` attribute is provided, the order of the variables should\n                correspond to the order of the variables in `names`. Optional.\n            names: List of variable names present in `expression`. Optional.\n\n        Examples:\n            You can pass any expression that can be evaluated by `pd.eval`.\n            To define x1**2 + x2**2 = 1, use\n            ```\n            NonlinearEquality(\"x1**2 + x2**2 - 1\")\n            ```\n            Standard mathematical operators are supported.\n            ```\n            NonlinearEquality(\"sin(A) / (exp(B) - 1)\")\n            ```\n            Parameter names with special characters or spaces need to be enclosed in backticks.\n            ```\n            NonlinearEquality(\"1 - `weight A` / `weight B`\")\n            ```\n        \"\"\"\n        self.expression = expression\n        self.is_equality = True\n        self.jacobian_expression = jacobian\n        self.names = names\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        return data.eval(self.expression)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\n        if self.jacobian_expression is not None:\n            res = data.eval(self.jacobian_expression)\n            for i, col in enumerate(res):\n                if not hasattr(col, \"__iter__\"):\n                    res[i] = pd.Series(np.repeat(col, data.shape[0]))\n\n            if self.names is not None:\n                return pd.DataFrame(\n                    res, index=[\"dg/d\" + name for name in self.names]\n                ).transpose()\n            else:\n                return pd.DataFrame(\n                    res, index=[f\"dg/dx{i}\" for i in range(data.shape[1])]\n                ).transpose()\n\n        return super().jacobian(data)\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return pd.Series(np.isclose(self(data), 0), index=data.index)\n\n    def __repr__(self):\n        return f\"NonlinearEquality('{self.expression}')\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(type=\"nonlinear-equality\", expression=self.expression)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.__init__","title":"<code>__init__(self, expression, jacobian=None, names=None)</code>  <code>special</code>","text":"<p>Equality of the form 'expression == 0'.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Mathematical expression that can be evaluated by <code>pandas.eval</code>.</p> required <code>jacobian</code> <code>Optional[str]</code> <p>List of mathematical expressions that can be evaluated by <code>pandas.eval</code>. The i-th expression should correspond to the partial derivative with respect to the i-th variable. If <code>names</code> attribute is provided, the order of the variables should correspond to the order of the variables in <code>names</code>. Optional.</p> <code>None</code> <code>names</code> <code>Optional[List[str]]</code> <p>List of variable names present in <code>expression</code>. Optional.</p> <code>None</code> <p>Examples:</p> <p>You can pass any expression that can be evaluated by <code>pd.eval</code>. To define x12 + x22 = 1, use <pre><code>NonlinearEquality(\"x1**2 + x2**2 - 1\")\n</code></pre> Standard mathematical operators are supported. <pre><code>NonlinearEquality(\"sin(A) / (exp(B) - 1)\")\n</code></pre> Parameter names with special characters or spaces need to be enclosed in backticks. <pre><code>NonlinearEquality(\"1 - `weight A` / `weight B`\")\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(\n    self,\n    expression: str,\n    jacobian: Optional[str] = None,\n    names: Optional[List[str]] = None,\n):\n\"\"\"Equality of the form 'expression == 0'.\n\n    Args:\n        expression: Mathematical expression that can be evaluated by `pandas.eval`.\n        jacobian: List of mathematical expressions that can be evaluated by `pandas.eval`.\n            The i-th expression should correspond to the partial derivative with respect to\n            the i-th variable. If `names` attribute is provided, the order of the variables should\n            correspond to the order of the variables in `names`. Optional.\n        names: List of variable names present in `expression`. Optional.\n\n    Examples:\n        You can pass any expression that can be evaluated by `pd.eval`.\n        To define x1**2 + x2**2 = 1, use\n        ```\n        NonlinearEquality(\"x1**2 + x2**2 - 1\")\n        ```\n        Standard mathematical operators are supported.\n        ```\n        NonlinearEquality(\"sin(A) / (exp(B) - 1)\")\n        ```\n        Parameter names with special characters or spaces need to be enclosed in backticks.\n        ```\n        NonlinearEquality(\"1 - `weight A` / `weight B`\")\n        ```\n    \"\"\"\n    self.expression = expression\n    self.is_equality = True\n    self.jacobian_expression = jacobian\n    self.names = names\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.jacobian","title":"<code>jacobian(self, data)</code>","text":"<p>Numerically evaluate the jacobian of the constraint J_g(x)</p> Source code in <code>opti/constraint.py</code> <pre><code>def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\n    if self.jacobian_expression is not None:\n        res = data.eval(self.jacobian_expression)\n        for i, col in enumerate(res):\n            if not hasattr(col, \"__iter__\"):\n                res[i] = pd.Series(np.repeat(col, data.shape[0]))\n\n        if self.names is not None:\n            return pd.DataFrame(\n                res, index=[\"dg/d\" + name for name in self.names]\n            ).transpose()\n        else:\n            return pd.DataFrame(\n                res, index=[f\"dg/dx{i}\" for i in range(data.shape[1])]\n            ).transpose()\n\n    return super().jacobian(data)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.satisfied","title":"<code>satisfied(self, data)</code>","text":"<p>Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n    return pd.Series(np.isclose(self(data), 0), index=data.index)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality","title":"<code> NonlinearInequality            (Constraint)         </code>","text":"Source code in <code>opti/constraint.py</code> <pre><code>class NonlinearInequality(Constraint):\n    def __init__(\n        self,\n        expression: str,\n        jacobian: Optional[str] = None,\n        names: Optional[List[str]] = None,\n    ):\n\"\"\"Inequality of the form 'expression &lt;= 0'.\n\n        Args:\n            expression: Mathematical expression that can be evaluated by `pandas.eval`.\n            jacobian: List of mathematical expressions that can be evaluated by `pandas.eval`.\n                The i-th expression should correspond to the partial derivative with respect to\n                the i-th variable. If `names` attribute is provided, the order of the variables should\n                correspond to the order of the variables in `names`. Optional.\n            names: List of variable names present in `expression`. Optional.\n\n        Examples:\n            You can pass any expression that can be evaluated by `pd.eval`.\n            To define x1**2 + x2**2 &lt; 1, use\n            ```\n            NonlinearInequality(\"x1**2 + x2**2 - 1\")\n            ```\n            Standard mathematical operators are supported.\n            ```\n            NonlinearInequality(\"sin(A) / (exp(B) - 1)\")\n            ```\n            Parameter names with special characters or spaces need to be enclosed in backticks.\n            ```\n            NonlinearInequality(\"1 - `weight A` / `weight B`\")\n            ```\n        \"\"\"\n        self.expression = expression\n        self.is_equality = False\n        self.jacobian_expression = jacobian\n        self.names = names\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        return data.eval(self.expression)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\n        if self.jacobian_expression is not None:\n            res = data.eval(self.jacobian_expression)\n            for i, col in enumerate(res):\n                if not hasattr(col, \"__iter__\"):\n                    res[i] = pd.Series(np.repeat(col, data.shape[0]))\n\n            if self.names is not None:\n                return pd.DataFrame(\n                    res, index=[\"dg/d\" + name for name in self.names]\n                ).transpose()\n            else:\n                return pd.DataFrame(\n                    res, index=[f\"dg/dx{i}\" for i in range(data.shape[1])]\n                ).transpose()\n\n        return super().jacobian(data)\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return self(data) &lt;= 0\n\n    def __repr__(self):\n        return f\"NonlinearInequality('{self.expression}')\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(type=\"nonlinear-inequality\", expression=self.expression)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.__init__","title":"<code>__init__(self, expression, jacobian=None, names=None)</code>  <code>special</code>","text":"<p>Inequality of the form 'expression &lt;= 0'.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Mathematical expression that can be evaluated by <code>pandas.eval</code>.</p> required <code>jacobian</code> <code>Optional[str]</code> <p>List of mathematical expressions that can be evaluated by <code>pandas.eval</code>. The i-th expression should correspond to the partial derivative with respect to the i-th variable. If <code>names</code> attribute is provided, the order of the variables should correspond to the order of the variables in <code>names</code>. Optional.</p> <code>None</code> <code>names</code> <code>Optional[List[str]]</code> <p>List of variable names present in <code>expression</code>. Optional.</p> <code>None</code> <p>Examples:</p> <p>You can pass any expression that can be evaluated by <code>pd.eval</code>. To define x12 + x22 &lt; 1, use <pre><code>NonlinearInequality(\"x1**2 + x2**2 - 1\")\n</code></pre> Standard mathematical operators are supported. <pre><code>NonlinearInequality(\"sin(A) / (exp(B) - 1)\")\n</code></pre> Parameter names with special characters or spaces need to be enclosed in backticks. <pre><code>NonlinearInequality(\"1 - `weight A` / `weight B`\")\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(\n    self,\n    expression: str,\n    jacobian: Optional[str] = None,\n    names: Optional[List[str]] = None,\n):\n\"\"\"Inequality of the form 'expression &lt;= 0'.\n\n    Args:\n        expression: Mathematical expression that can be evaluated by `pandas.eval`.\n        jacobian: List of mathematical expressions that can be evaluated by `pandas.eval`.\n            The i-th expression should correspond to the partial derivative with respect to\n            the i-th variable. If `names` attribute is provided, the order of the variables should\n            correspond to the order of the variables in `names`. Optional.\n        names: List of variable names present in `expression`. Optional.\n\n    Examples:\n        You can pass any expression that can be evaluated by `pd.eval`.\n        To define x1**2 + x2**2 &lt; 1, use\n        ```\n        NonlinearInequality(\"x1**2 + x2**2 - 1\")\n        ```\n        Standard mathematical operators are supported.\n        ```\n        NonlinearInequality(\"sin(A) / (exp(B) - 1)\")\n        ```\n        Parameter names with special characters or spaces need to be enclosed in backticks.\n        ```\n        NonlinearInequality(\"1 - `weight A` / `weight B`\")\n        ```\n    \"\"\"\n    self.expression = expression\n    self.is_equality = False\n    self.jacobian_expression = jacobian\n    self.names = names\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.jacobian","title":"<code>jacobian(self, data)</code>","text":"<p>Numerically evaluate the jacobian of the constraint J_g(x)</p> Source code in <code>opti/constraint.py</code> <pre><code>def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\n    if self.jacobian_expression is not None:\n        res = data.eval(self.jacobian_expression)\n        for i, col in enumerate(res):\n            if not hasattr(col, \"__iter__\"):\n                res[i] = pd.Series(np.repeat(col, data.shape[0]))\n\n        if self.names is not None:\n            return pd.DataFrame(\n                res, index=[\"dg/d\" + name for name in self.names]\n            ).transpose()\n        else:\n            return pd.DataFrame(\n                res, index=[f\"dg/dx{i}\" for i in range(data.shape[1])]\n            ).transpose()\n\n    return super().jacobian(data)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.satisfied","title":"<code>satisfied(self, data)</code>","text":"<p>Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n    return self(data) &lt;= 0\n</code></pre>"},{"location":"ref-metric/","title":"Metrics","text":""},{"location":"ref-metric/#opti.metric.crowding_distance","title":"<code>crowding_distance(A)</code>","text":"<p>Crowding distance indicator.</p> <p>The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points.</p> <p>Reference</p> <p>Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array</code> <p>Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Crowding distance indicator for each point in the front.</p> Source code in <code>opti/metric.py</code> <pre><code>def crowding_distance(A):\n\"\"\"Crowding distance indicator.\n\n    The crowding distance is defined for each point in a Pareto front as the average\n    side length of the cuboid formed by the neighbouring points.\n\n    Reference:\n        [Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II](https://link.springer.com/chapter/10.1007/3-540-45356-3_83)\n\n    Args:\n        A (2D-array): Set of points representing a Pareto front.\n            Pareto-efficiency is assumed but not checked.\n\n    Returns:\n        array: Crowding distance indicator for each point in the front.\n    \"\"\"\n    A = pareto_front(A)\n    N, m = A.shape\n\n    # no crowding distance for 2 points\n    if N &lt;= 2:\n        return np.full(N, np.inf)\n\n    # sort points along each objective\n    sort = np.argsort(A, axis=0)\n    A = A[sort, np.arange(m)]\n\n    # normalize all objectives\n    norm = np.max(A, axis=0) - np.min(A, axis=0)\n    A = A / norm\n    A[:, norm == 0] = 0  # handle min = max\n\n    # distance to previous and to next point along each objective\n    d = np.diff(A, axis=0)\n    inf = np.full((1, m), np.inf)\n    d0 = np.concatenate([inf, d])\n    d1 = np.concatenate([d, inf])\n\n    # TODO: handle cases with duplicate objective values leading to 0 distances\n\n    # cuboid side length = distance between previous and next point\n    unsort = np.argsort(sort, axis=0)\n    cuboid = d0[unsort, np.arange(m)] + d1[unsort, np.arange(m)]\n    return np.mean(cuboid, axis=1)\n</code></pre>"},{"location":"ref-metric/#opti.metric.generational_distance","title":"<code>generational_distance(A, R, p=1, clip=True)</code>","text":"<p>Generational distance indicator.</p> <p>The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence.</p> <p>Reference</p> <p>David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array</code> <p>Set of points representing an approximate Pareto front.</p> required <code>R</code> <code>2D-array</code> <p>Set of points representing a reference Pareto front.</p> required <code>p</code> <code>int</code> <p>Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average.</p> <code>1</code> <code>clip</code> <code>bool</code> <p>Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>Generational distance indicator.</p> Source code in <code>opti/metric.py</code> <pre><code>def generational_distance(\n    A: np.ndarray, R: np.ndarray, p: float = 1, clip: bool = True\n) -&gt; float:\nr\"\"\"Generational distance indicator.\n\n    The generational distance (GD) is defined as the average distance of points in the\n    approximate front A to a reference front R.\n    .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A\n    where d(a, R) is the euclidean distance of point a to the reference front, N_A is\n    the number of points in A. GD is a measure of convergence.\n\n    Reference:\n        [David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art](http://dx.doi.org/10.1162/106365600568158)\n\n    Args:\n        A (2D-array): Set of points representing an approximate Pareto front.\n        R (2D-array): Set of points representing a reference Pareto front.\n        p (int, optional): Order of the p-norm for averaging over distances.\n            Defaults to 1, yielding the standard average.\n        clip (bool, optional): Flag for using the modfied generational distance, which\n            prevents negative values for points that are non-dominated by the reference\n            front.\n\n    Returns:\n        float: Generational distance indicator.\n    \"\"\"\n    A = pareto_front(A)\n    distances = A[:, np.newaxis] - R[np.newaxis]\n    if clip:\n        distances = distances.clip(0, None)\n    distances = np.linalg.norm(distances, axis=2).min(axis=1)\n    return np.linalg.norm(distances, p) / len(A)\n</code></pre>"},{"location":"ref-metric/#opti.metric.inverted_generational_distance","title":"<code>inverted_generational_distance(A, R, p=1)</code>","text":"<p>Inverted generational distance indicator.</p> <p>The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front.</p> <p>Reference</p> <p>CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array</code> <p>Set of points representing an approximate Pareto front.</p> required <code>R</code> <code>2D-array</code> <p>Set of points representing a reference Pareto front.</p> required <code>p</code> <code>int</code> <p>Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average.</p> <code>1</code> <p>Returns:</p> Type Description <code>float</code> <p>Inverted generational distance indicator.</p> Source code in <code>opti/metric.py</code> <pre><code>def inverted_generational_distance(A: np.ndarray, R: np.ndarray, p: float = 1) -&gt; float:\n\"\"\"Inverted generational distance indicator.\n\n    The inverted generational distance (IGD) is defined as the average distance of\n    points in the reference front R to an approximate front A.\n    IGD is a measure of convergence, spread and distribution of the approximate front.\n\n    Reference:\n        [CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm](https://doi.org/10.1007/978-3-540-24694-7_71)\n\n    Args:\n        A (2D-array): Set of points representing an approximate Pareto front.\n        R (2D-array): Set of points representing a reference Pareto front.\n        p (int, optional): Order of the p-norm for averaging over distances.\n            Defaults to 1, yielding the standard average.\n\n    Returns:\n        float: Inverted generational distance indicator.\n    \"\"\"\n    return generational_distance(R, A, p, clip=False)\n</code></pre>"},{"location":"ref-metric/#opti.metric.is_pareto_efficient","title":"<code>is_pareto_efficient(A)</code>","text":"<p>Find the Pareto-efficient points in a set of objective vectors.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array, shape=(samples, dimension</code> <p>Objective vectors.</p> required <p>Returns:</p> Type Description <code>1D-array of bools</code> <p>Boolean mask for the Pareto efficient points in A.</p> Source code in <code>opti/metric.py</code> <pre><code>def is_pareto_efficient(A: np.ndarray) -&gt; np.ndarray:\n\"\"\"Find the Pareto-efficient points in a set of objective vectors.\n\n    Args:\n        A (2D-array, shape=(samples, dimension)): Objective vectors.\n\n    Returns:\n        1D-array of bools: Boolean mask for the Pareto efficient points in A.\n    \"\"\"\n    efficient = np.ones(len(A), dtype=bool)\n    idx = np.arange(len(A))\n    for i, a in enumerate(A):\n        if not efficient[i]:\n            continue\n        # set all *other* efficent points to False, if they are not strictly better in at least one objective\n        efficient[efficient] = np.any(A[efficient] &lt; a, axis=1) | (i == idx[efficient])\n    return efficient\n</code></pre>"},{"location":"ref-metric/#opti.metric.pareto_front","title":"<code>pareto_front(A)</code>","text":"<p>Find the Pareto-efficient points in a set of objective vectors.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array, shape=(samples, dimension</code> <p>Objective vectors.</p> required <p>Returns:</p> Type Description <code>2D-array</code> <p>Pareto efficient points in A.</p> Source code in <code>opti/metric.py</code> <pre><code>def pareto_front(A: np.ndarray) -&gt; np.ndarray:\n\"\"\"Find the Pareto-efficient points in a set of objective vectors.\n\n    Args:\n        A (2D-array, shape=(samples, dimension)): Objective vectors.\n\n    Returns:\n        2D-array: Pareto efficient points in A.\n    \"\"\"\n    return A[is_pareto_efficient(A)]\n</code></pre>"},{"location":"ref-model/","title":"Models","text":""},{"location":"ref-model/#opti.model.CustomModel","title":"<code> CustomModel            (Model)         </code>","text":"Source code in <code>opti/model.py</code> <pre><code>class CustomModel(Model):\n    def __init__(self, names: List[str], f: Callable):\n\"\"\"Custom model for arbitrary functions.\n\n        Args:\n            names: names of the modeled outputs.\n            f: Callable that takes a DataFrame of inputs and returns a DataFrame of outputs.\n        \"\"\"\n        super().__init__(names)\n        self.f = f\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        return self.f(df)\n\n    def __repr__(self):\n        return f\"CustomModel({self.names}, f={self.f})\"\n</code></pre>"},{"location":"ref-model/#opti.model.CustomModel.__init__","title":"<code>__init__(self, names, f)</code>  <code>special</code>","text":"<p>Custom model for arbitrary functions.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>names of the modeled outputs.</p> required <code>f</code> <code>Callable</code> <p>Callable that takes a DataFrame of inputs and returns a DataFrame of outputs.</p> required Source code in <code>opti/model.py</code> <pre><code>def __init__(self, names: List[str], f: Callable):\n\"\"\"Custom model for arbitrary functions.\n\n    Args:\n        names: names of the modeled outputs.\n        f: Callable that takes a DataFrame of inputs and returns a DataFrame of outputs.\n    \"\"\"\n    super().__init__(names)\n    self.f = f\n</code></pre>"},{"location":"ref-model/#opti.model.LinearModel","title":"<code> LinearModel            (Model)         </code>","text":"Source code in <code>opti/model.py</code> <pre><code>class LinearModel(Model):\n    def __init__(\n        self,\n        names: List[str],\n        coefficients: Dict[str, float],\n        offset: float = 0,\n    ):\n\"\"\"Model to compute an output as a linear/affine function of the inputs, $y = ax + b$.\n\n        Args:\n            names: name of the modeled output.\n            coefficients: dictionary mapping input name to the corresponding coefficient a.\n            offset: the offset b.\n        \"\"\"\n        super().__init__(names)\n        if len(names) &gt; 1:\n            raise ValueError(\"LinearModel can only describe a single output.\")\n        self.coefficients = coefficients\n        self.offset = offset\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        coefficients_values = list(self.coefficients.values())\n        coefficients_names = list(self.coefficients.keys())\n        y = df[coefficients_names].to_numpy() @ coefficients_values + self.offset\n        return pd.DataFrame(y, columns=self.names)\n\n    def __repr__(self):\n        return f\"LinearModel({self.names}, coefficients={self.coefficients}, offset={self.offset})\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(\n            type=\"linear-model\",\n            names=self.names,\n            coefficients=self.coefficients,\n            offset=self.offset,\n        )\n</code></pre>"},{"location":"ref-model/#opti.model.LinearModel.__init__","title":"<code>__init__(self, names, coefficients, offset=0)</code>  <code>special</code>","text":"<p>Model to compute an output as a linear/affine function of the inputs, \\(y = ax + b\\).</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>name of the modeled output.</p> required <code>coefficients</code> <code>Dict[str, float]</code> <p>dictionary mapping input name to the corresponding coefficient a.</p> required <code>offset</code> <code>float</code> <p>the offset b.</p> <code>0</code> Source code in <code>opti/model.py</code> <pre><code>def __init__(\n    self,\n    names: List[str],\n    coefficients: Dict[str, float],\n    offset: float = 0,\n):\n\"\"\"Model to compute an output as a linear/affine function of the inputs, $y = ax + b$.\n\n    Args:\n        names: name of the modeled output.\n        coefficients: dictionary mapping input name to the corresponding coefficient a.\n        offset: the offset b.\n    \"\"\"\n    super().__init__(names)\n    if len(names) &gt; 1:\n        raise ValueError(\"LinearModel can only describe a single output.\")\n    self.coefficients = coefficients\n    self.offset = offset\n</code></pre>"},{"location":"ref-model/#opti.model.LinearModel.to_config","title":"<code>to_config(self)</code>","text":"<p>Return a json-serializable dictionary of the objective.</p> Source code in <code>opti/model.py</code> <pre><code>def to_config(self) -&gt; Dict:\n    return dict(\n        type=\"linear-model\",\n        names=self.names,\n        coefficients=self.coefficients,\n        offset=self.offset,\n    )\n</code></pre>"},{"location":"ref-model/#opti.model.Model","title":"<code> Model        </code>","text":"Source code in <code>opti/model.py</code> <pre><code>class Model:\n    def __init__(self, names: List[str]):\n\"\"\"Base class for models of outputs as function of inputs.\n\n        Args:\n            names: names of the modeled outputs.\n        \"\"\"\n        for name in names:\n            if not isinstance(name, str):\n                TypeError(\"Model: names must be a list of strings.\")\n        self.names = list(names)\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Evaluate the objective values for a given DataFrame.\"\"\"\n        raise NotImplementedError\n\n    def to_config(self) -&gt; None:\n\"\"\"Return a json-serializable dictionary of the objective.\"\"\"\n        pass  # non-serializable models should be ommited without raising an error\n</code></pre>"},{"location":"ref-model/#opti.model.Model.__call__","title":"<code>__call__(self, df)</code>  <code>special</code>","text":"<p>Evaluate the objective values for a given DataFrame.</p> Source code in <code>opti/model.py</code> <pre><code>def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Evaluate the objective values for a given DataFrame.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-model/#opti.model.Model.__init__","title":"<code>__init__(self, names)</code>  <code>special</code>","text":"<p>Base class for models of outputs as function of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>names of the modeled outputs.</p> required Source code in <code>opti/model.py</code> <pre><code>def __init__(self, names: List[str]):\n\"\"\"Base class for models of outputs as function of inputs.\n\n    Args:\n        names: names of the modeled outputs.\n    \"\"\"\n    for name in names:\n        if not isinstance(name, str):\n            TypeError(\"Model: names must be a list of strings.\")\n    self.names = list(names)\n</code></pre>"},{"location":"ref-model/#opti.model.Model.to_config","title":"<code>to_config(self)</code>","text":"<p>Return a json-serializable dictionary of the objective.</p> Source code in <code>opti/model.py</code> <pre><code>def to_config(self) -&gt; None:\n\"\"\"Return a json-serializable dictionary of the objective.\"\"\"\n    pass  # non-serializable models should be ommited without raising an error\n</code></pre>"},{"location":"ref-model/#opti.model.Models","title":"<code> Models        </code>","text":"Source code in <code>opti/model.py</code> <pre><code>class Models:\n    def __init__(self, models: Union[List[Model], List[Dict]]):\n\"\"\"Container for models.\n\n        Args:\n            models: list of models or model configurations.\n        \"\"\"\n        _models = []\n        for m in models:\n            if isinstance(m, Model):\n                _models.append(m)\n            else:\n                _models.append(make_model(**m))\n        self.models = _models\n\n    def __call__(self, y: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.concat([model(y) for model in self.models], axis=1)\n\n    def __repr__(self):\n        return \"Models(\\n\" + pprint.pformat(self.models) + \"\\n)\"\n\n    def __iter__(self):\n        return iter(self.models)\n\n    def __len__(self):\n        return len(self.models)\n\n    def __getitem__(self, i: int) -&gt; Model:\n        return self.models[i]\n\n    @property\n    def names(self):\n        names = []\n        for model in self.models:\n            names += model.names\n        return names\n\n    def to_config(self) -&gt; List[Dict]:\n        return [\n            model.to_config() for model in self.models if model.to_config() is not None\n        ]\n</code></pre>"},{"location":"ref-model/#opti.model.Models.__init__","title":"<code>__init__(self, models)</code>  <code>special</code>","text":"<p>Container for models.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Union[List[opti.model.Model], List[Dict]]</code> <p>list of models or model configurations.</p> required Source code in <code>opti/model.py</code> <pre><code>def __init__(self, models: Union[List[Model], List[Dict]]):\n\"\"\"Container for models.\n\n    Args:\n        models: list of models or model configurations.\n    \"\"\"\n    _models = []\n    for m in models:\n        if isinstance(m, Model):\n            _models.append(m)\n        else:\n            _models.append(make_model(**m))\n    self.models = _models\n</code></pre>"},{"location":"ref-model/#opti.model.make_model","title":"<code>make_model(type, **kwargs)</code>","text":"<p>Make a model object from a configuration dict.</p> Source code in <code>opti/model.py</code> <pre><code>def make_model(type, **kwargs):\n\"\"\"Make a model object from a configuration dict.\"\"\"\n    t = type.lower()\n    if t == \"linear-model\":\n        return LinearModel(**kwargs)\n    raise ValueError(f\"Unknown model type: {t}.\")\n</code></pre>"},{"location":"ref-objective/","title":"Objectives","text":""},{"location":"ref-objective/#opti.objective.CloseToTarget","title":"<code> CloseToTarget            (Objective)         </code>","text":"Source code in <code>opti/objective.py</code> <pre><code>class CloseToTarget(Objective):\n    def __init__(\n        self,\n        name: str,\n        target: float = 0,\n        exponent: float = 1,\n        tolerance: float = 0,\n    ):\n\"\"\"Objective for getting as close as possible to a given value.\n\n        s(y) = |y - target| ** exponent - tolerance ** exponent\n\n        Args:\n            name: output to optimize\n            target: target value\n            exponent: exponent of the difference\n            tolerance: only when used as output constraint. distance to target below which no further improvement is required\n        \"\"\"\n        super().__init__(name)\n        self.target = target\n        self.exponent = exponent\n        self.tolerance = tolerance\n\n    def __call__(self, y: pd.Series) -&gt; pd.Series:\n        return (\n            y - self.target\n        ).abs() ** self.exponent - self.tolerance**self.exponent\n\n    def __repr__(self):\n        return f\"CloseToTarget('{self.name}', target={self.target})\"\n\n    def to_config(self) -&gt; Dict:\n        config = dict(name=self.name, type=\"close-to-target\", target=self.target)\n        if self.exponent != 1:\n            config[\"exponent\"] = self.exponent\n        if self.tolerance != 0:\n            config[\"tolerance\"] = self.tolerance\n        return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.CloseToTarget.__init__","title":"<code>__init__(self, name, target=0, exponent=1, tolerance=0)</code>  <code>special</code>","text":"<p>Objective for getting as close as possible to a given value.</p> <p>s(y) = |y - target|  exponent - tolerance  exponent</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>output to optimize</p> required <code>target</code> <code>float</code> <p>target value</p> <code>0</code> <code>exponent</code> <code>float</code> <p>exponent of the difference</p> <code>1</code> <code>tolerance</code> <code>float</code> <p>only when used as output constraint. distance to target below which no further improvement is required</p> <code>0</code> Source code in <code>opti/objective.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    target: float = 0,\n    exponent: float = 1,\n    tolerance: float = 0,\n):\n\"\"\"Objective for getting as close as possible to a given value.\n\n    s(y) = |y - target| ** exponent - tolerance ** exponent\n\n    Args:\n        name: output to optimize\n        target: target value\n        exponent: exponent of the difference\n        tolerance: only when used as output constraint. distance to target below which no further improvement is required\n    \"\"\"\n    super().__init__(name)\n    self.target = target\n    self.exponent = exponent\n    self.tolerance = tolerance\n</code></pre>"},{"location":"ref-objective/#opti.objective.CloseToTarget.to_config","title":"<code>to_config(self)</code>","text":"<p>Return a json-serializable dictionary of the objective.</p> Source code in <code>opti/objective.py</code> <pre><code>def to_config(self) -&gt; Dict:\n    config = dict(name=self.name, type=\"close-to-target\", target=self.target)\n    if self.exponent != 1:\n        config[\"exponent\"] = self.exponent\n    if self.tolerance != 0:\n        config[\"tolerance\"] = self.tolerance\n    return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.Maximize","title":"<code> Maximize            (Objective)         </code>","text":"Source code in <code>opti/objective.py</code> <pre><code>class Maximize(Objective):\n    def __init__(self, name: str, target: float = 0):\n\"\"\"Maximization objective\n\n        s(y) = target - y\n\n        Args:\n            name: output to maximize\n            target: only when used as output constraint. value above which no further improvement is required\n        \"\"\"\n        super().__init__(name)\n        self.target = target\n\n    def __call__(self, y: pd.Series) -&gt; pd.Series:\n        return self.target - y\n\n    def untransform(self, y: pd.Series) -&gt; pd.Series:\n\"\"\"Undo the transformation from output to objective value\"\"\"\n        return self.target - y\n\n    def __repr__(self):\n        return f\"Maximize('{self.name}')\"\n\n    def to_config(self) -&gt; Dict:\n        config = dict(name=self.name, type=\"maximize\")\n        if self.target != 0:\n            config[\"target\"] = str(self.target)\n        return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.Maximize.__init__","title":"<code>__init__(self, name, target=0)</code>  <code>special</code>","text":"<p>Maximization objective</p> <p>s(y) = target - y</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>output to maximize</p> required <code>target</code> <code>float</code> <p>only when used as output constraint. value above which no further improvement is required</p> <code>0</code> Source code in <code>opti/objective.py</code> <pre><code>def __init__(self, name: str, target: float = 0):\n\"\"\"Maximization objective\n\n    s(y) = target - y\n\n    Args:\n        name: output to maximize\n        target: only when used as output constraint. value above which no further improvement is required\n    \"\"\"\n    super().__init__(name)\n    self.target = target\n</code></pre>"},{"location":"ref-objective/#opti.objective.Maximize.to_config","title":"<code>to_config(self)</code>","text":"<p>Return a json-serializable dictionary of the objective.</p> Source code in <code>opti/objective.py</code> <pre><code>def to_config(self) -&gt; Dict:\n    config = dict(name=self.name, type=\"maximize\")\n    if self.target != 0:\n        config[\"target\"] = str(self.target)\n    return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.Maximize.untransform","title":"<code>untransform(self, y)</code>","text":"<p>Undo the transformation from output to objective value</p> Source code in <code>opti/objective.py</code> <pre><code>def untransform(self, y: pd.Series) -&gt; pd.Series:\n\"\"\"Undo the transformation from output to objective value\"\"\"\n    return self.target - y\n</code></pre>"},{"location":"ref-objective/#opti.objective.Minimize","title":"<code> Minimize            (Objective)         </code>","text":"Source code in <code>opti/objective.py</code> <pre><code>class Minimize(Objective):\n    def __init__(self, name: str, target: float = 0):\n\"\"\"Minimization objective\n\n        s(y) = y - target\n\n        Args:\n            name: output to minimize\n            target: only when used as output constraint. value below which no further improvement is required\n        \"\"\"\n        super().__init__(name)\n        self.target = target\n\n    def __call__(self, y: pd.Series) -&gt; pd.Series:\n        return y - self.target\n\n    def untransform(self, y: pd.Series) -&gt; pd.Series:\n\"\"\"Undo the transformation from output to objective value\"\"\"\n        return y + self.target\n\n    def __repr__(self):\n        return f\"Minimize('{self.name}')\"\n\n    def to_config(self) -&gt; Dict:\n        config = dict(name=self.name, type=\"minimize\")\n        if self.target != 0:\n            config[\"target\"] = str(self.target)\n        return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.Minimize.__init__","title":"<code>__init__(self, name, target=0)</code>  <code>special</code>","text":"<p>Minimization objective</p> <p>s(y) = y - target</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>output to minimize</p> required <code>target</code> <code>float</code> <p>only when used as output constraint. value below which no further improvement is required</p> <code>0</code> Source code in <code>opti/objective.py</code> <pre><code>def __init__(self, name: str, target: float = 0):\n\"\"\"Minimization objective\n\n    s(y) = y - target\n\n    Args:\n        name: output to minimize\n        target: only when used as output constraint. value below which no further improvement is required\n    \"\"\"\n    super().__init__(name)\n    self.target = target\n</code></pre>"},{"location":"ref-objective/#opti.objective.Minimize.to_config","title":"<code>to_config(self)</code>","text":"<p>Return a json-serializable dictionary of the objective.</p> Source code in <code>opti/objective.py</code> <pre><code>def to_config(self) -&gt; Dict:\n    config = dict(name=self.name, type=\"minimize\")\n    if self.target != 0:\n        config[\"target\"] = str(self.target)\n    return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.Minimize.untransform","title":"<code>untransform(self, y)</code>","text":"<p>Undo the transformation from output to objective value</p> Source code in <code>opti/objective.py</code> <pre><code>def untransform(self, y: pd.Series) -&gt; pd.Series:\n\"\"\"Undo the transformation from output to objective value\"\"\"\n    return y + self.target\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objective","title":"<code> Objective        </code>","text":"Source code in <code>opti/objective.py</code> <pre><code>class Objective:\n    def __init__(self, name: str):\n\"\"\"Base class for optimzation objectives.\"\"\"\n        self.name = name\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Evaluate the objective values for given output values.\"\"\"\n        raise NotImplementedError\n\n    def to_config(self) -&gt; Dict:\n\"\"\"Return a json-serializable dictionary of the objective.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objective.__call__","title":"<code>__call__(self, df)</code>  <code>special</code>","text":"<p>Evaluate the objective values for given output values.</p> Source code in <code>opti/objective.py</code> <pre><code>def __call__(self, df: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Evaluate the objective values for given output values.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objective.__init__","title":"<code>__init__(self, name)</code>  <code>special</code>","text":"<p>Base class for optimzation objectives.</p> Source code in <code>opti/objective.py</code> <pre><code>def __init__(self, name: str):\n\"\"\"Base class for optimzation objectives.\"\"\"\n    self.name = name\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objective.to_config","title":"<code>to_config(self)</code>","text":"<p>Return a json-serializable dictionary of the objective.</p> Source code in <code>opti/objective.py</code> <pre><code>def to_config(self) -&gt; Dict:\n\"\"\"Return a json-serializable dictionary of the objective.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objectives","title":"<code> Objectives        </code>","text":"<p>Container for optimization objectives.</p> <p>Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint)</p> Source code in <code>opti/objective.py</code> <pre><code>class Objectives:\n\"\"\"Container for optimization objectives.\n\n    Objectives can be either used to quantify the optimility or as a constraint on the\n    viability of output values (chance / feasibility constraint)\n    \"\"\"\n\n    def __init__(self, objectives: Union[List[Objective], List[Dict]]):\n        _objectives = []\n        for m in objectives:\n            if isinstance(m, Objective):\n                _objectives.append(m)\n            else:\n                _objectives.append(make_objective(**m))\n        self.objectives = _objectives\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.concat([obj(df[obj.name]) for obj in self.objectives], axis=1)\n\n    def __repr__(self):\n        return \"Objectives(\\n\" + pprint.pformat(self.objectives) + \"\\n)\"\n\n    def __iter__(self):\n        return iter(self.objectives)\n\n    def __len__(self):\n        return len(self.objectives)\n\n    def __getitem__(self, i: int) -&gt; Objective:\n        return self.objectives[i]\n\n    @property\n    def names(self):\n        return [obj.name for obj in self]\n\n    def bounds(self, outputs: Parameters) -&gt; pd.DataFrame:\n\"\"\"Compute the bounds in objective space based on the output space bounds.\n\n        The bounds can be interpreted as the ideal and nadir points.\n\n        Examples for continuous parameters:\n            min y for y in [0, 10] -&gt; ideal = 0, nadir = 10\n            max y for y in [0, 10] -&gt; ideal = -10, nadir = 0\n            min (y - 7)**2 for y in [0, 10] -&gt; ideal = 0, nadir = 7**2\n\n        Args:\n            outputs: Output parameters.\n        \"\"\"\n        Z = self(outputs.bounds)\n        bounds = pd.DataFrame(columns=self.names, dtype=float)\n        bounds.loc[\"min\"] = Z.min(axis=0)\n        bounds.loc[\"max\"] = Z.max(axis=0)\n\n        for name, obj in zip(self.names, self):\n            if isinstance(obj, CloseToTarget):\n                bounds.loc[\"min\", name] = 0\n\n        return bounds\n\n    def to_config(self) -&gt; List[Dict]:\n        return [obj.to_config() for obj in self.objectives]\n\n    def get(self, types) -&gt; \"Objectives\":\n\"\"\"Get all parameters of the given type(s).\"\"\"\n        return Objectives([o for o in self if isinstance(o, types)])\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objectives.bounds","title":"<code>bounds(self, outputs)</code>","text":"<p>Compute the bounds in objective space based on the output space bounds.</p> <p>The bounds can be interpreted as the ideal and nadir points.</p> <p>Examples for continuous parameters:     min y for y in [0, 10] -&gt; ideal = 0, nadir = 10     max y for y in [0, 10] -&gt; ideal = -10, nadir = 0     min (y - 7)2 for y in [0, 10] -&gt; ideal = 0, nadir = 72</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Parameters</code> <p>Output parameters.</p> required Source code in <code>opti/objective.py</code> <pre><code>def bounds(self, outputs: Parameters) -&gt; pd.DataFrame:\n\"\"\"Compute the bounds in objective space based on the output space bounds.\n\n    The bounds can be interpreted as the ideal and nadir points.\n\n    Examples for continuous parameters:\n        min y for y in [0, 10] -&gt; ideal = 0, nadir = 10\n        max y for y in [0, 10] -&gt; ideal = -10, nadir = 0\n        min (y - 7)**2 for y in [0, 10] -&gt; ideal = 0, nadir = 7**2\n\n    Args:\n        outputs: Output parameters.\n    \"\"\"\n    Z = self(outputs.bounds)\n    bounds = pd.DataFrame(columns=self.names, dtype=float)\n    bounds.loc[\"min\"] = Z.min(axis=0)\n    bounds.loc[\"max\"] = Z.max(axis=0)\n\n    for name, obj in zip(self.names, self):\n        if isinstance(obj, CloseToTarget):\n            bounds.loc[\"min\", name] = 0\n\n    return bounds\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objectives.get","title":"<code>get(self, types)</code>","text":"<p>Get all parameters of the given type(s).</p> Source code in <code>opti/objective.py</code> <pre><code>def get(self, types) -&gt; \"Objectives\":\n\"\"\"Get all parameters of the given type(s).\"\"\"\n    return Objectives([o for o in self if isinstance(o, types)])\n</code></pre>"},{"location":"ref-objective/#opti.objective.make_objective","title":"<code>make_objective(type, name, **kwargs)</code>","text":"<p>Make an objective from a configuration.</p> <pre><code>obj = make_objective(**config)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>objective type</p> required <code>name</code> <code>str</code> <p>output to optimize</p> required Source code in <code>opti/objective.py</code> <pre><code>def make_objective(type: str, name: str, **kwargs) -&gt; Objective:\n\"\"\"Make an objective from a configuration.\n\n    ```\n    obj = make_objective(**config)\n    ```\n\n    Args:\n        type: objective type\n        name: output to optimize\n    \"\"\"\n    objective = {\n        \"minimize\": Minimize,\n        \"maximize\": Maximize,\n        \"close-to-target\": CloseToTarget,\n    }[type.lower()]\n    return objective(name, **kwargs)\n</code></pre>"},{"location":"ref-parameter/","title":"Parameters","text":""},{"location":"ref-parameter/#opti.parameter.Categorical","title":"<code> Categorical            (Parameter)         </code>","text":"<p>Variable with a categorical domain (nominal scale, values cannot be put into order).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the parameter</p> <code>domain</code> <code>list</code> <p>list possible values</p> Source code in <code>opti/parameter.py</code> <pre><code>class Categorical(Parameter):\n\"\"\"Variable with a categorical domain (nominal scale, values cannot be put into order).\n\n    Attributes:\n        name (str): name of the parameter\n        domain (list): list possible values\n    \"\"\"\n\n    def __init__(self, name: str, domain: List[str], **kwargs):\n        if not isinstance(domain, list):\n            raise TypeError(f\"{name}: Domain must be of type list.\")\n        if len(domain) &lt; 2:\n            raise ValueError(f\"{name}: Domain must a least contain 2 values.\")\n        if len(set(domain)) != len(domain):\n            raise ValueError(f\"{name}: Domain contains duplicates.\")\n        super().__init__(name, domain, type=\"categorical\", **kwargs)\n\n    def __repr__(self):\n        return f\"Categorical('{self.name}', domain={self.domain})\"\n\n    @property\n    def bounds(self) -&gt; Tuple[float, float]:\n\"\"\"Return the domain bounds.\"\"\"\n        return np.nan, np.nan\n\n    def contains(self, point):\n\"\"\"Check if a point is in contained in the domain.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with boolean datatype.\n        \"\"\"\n        if not np.isscalar(point):\n            point = np.array(point)\n        return np.isin(point, self.domain)\n\n    def round(self, point):\n\"\"\"Round a point to the closest contained values.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point`.\n        \"\"\"\n        if not np.all(self.contains(point)):\n            raise ValueError(\n                f\"{self.name}: Cannot round values for categorical parameter.\"\n            )\n        return point\n\n    def sample(self, n: int = 1) -&gt; pd.Series:\n\"\"\"Draw random samples from the domain.\"\"\"\n        return pd.Series(name=self.name, data=np.random.choice(self.domain, n))\n\n    def to_onehot_encoding(self, points: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Convert points to a one-hot encoding.\"\"\"\n        return pd.DataFrame(\n            {f\"{self.name}{_CAT_SEP}{c}\": points == c for c in self.domain}, dtype=float\n        )\n\n    def from_onehot_encoding(self, points: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Convert points back from one-hot encoding.\"\"\"\n        cat_cols = [f\"{self.name}{_CAT_SEP}{c}\" for c in self.domain]\n        if np.any([c not in cat_cols for c in points.columns]):\n            raise ValueError(\n                f\"{self.name}: Column names don't match categorical levels: {points.columns}, {cat_cols}.\"\n            )\n        s = points.idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n        s.name = self.name\n        return s\n\n    def to_dummy_encoding(self, points: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\"\n        return pd.DataFrame(\n            {f\"{self.name}{_CAT_SEP}{c}\": points == c for c in self.domain[1:]},\n            dtype=float,\n        )\n\n    def from_dummy_encoding(self, points: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Convert points back from dummy encoding.\"\"\"\n        cat_cols = [f\"{self.name}{_CAT_SEP}{c}\" for c in self.domain]\n        if np.any([c not in cat_cols[1:] for c in points.columns]):\n            raise ValueError(\n                f\"{self.name}: Column names don't match categorical levels: {points.columns}, {cat_cols}.\"\n            )\n        points = points.copy()\n        points[cat_cols[0]] = 1 - points[cat_cols[1:]].sum(axis=1)\n        s = points.idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n        s.name = self.name\n        return s\n\n    def to_label_encoding(self, points: pd.Series) -&gt; pd.Series:\n\"\"\"Convert points to label-encoding.\"\"\"\n        enc = pd.Series(range(len(self.domain)), index=list(self.domain))\n        s = enc[points]\n        s.index = points.index\n        s.name = self.name\n        return s\n\n    def from_label_encoding(self, points: pd.Series) -&gt; pd.Series:\n\"\"\"Convert points back from label-encoding.\"\"\"\n        enc = np.array(self.domain)\n        return pd.Series(enc[points], index=points.index)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.bounds","title":"<code>bounds: Tuple[float, float]</code>  <code>property</code> <code>readonly</code>","text":"<p>Return the domain bounds.</p>"},{"location":"ref-parameter/#opti.parameter.Categorical.contains","title":"<code>contains(self, point)</code>","text":"<p>Check if a point is in contained in the domain.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>float, np.ndarray, pd.Series or pd.Dataframe</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with boolean datatype.</p> Source code in <code>opti/parameter.py</code> <pre><code>def contains(self, point):\n\"\"\"Check if a point is in contained in the domain.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with boolean datatype.\n    \"\"\"\n    if not np.isscalar(point):\n        point = np.array(point)\n    return np.isin(point, self.domain)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.from_dummy_encoding","title":"<code>from_dummy_encoding(self, points)</code>","text":"<p>Convert points back from dummy encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_dummy_encoding(self, points: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Convert points back from dummy encoding.\"\"\"\n    cat_cols = [f\"{self.name}{_CAT_SEP}{c}\" for c in self.domain]\n    if np.any([c not in cat_cols[1:] for c in points.columns]):\n        raise ValueError(\n            f\"{self.name}: Column names don't match categorical levels: {points.columns}, {cat_cols}.\"\n        )\n    points = points.copy()\n    points[cat_cols[0]] = 1 - points[cat_cols[1:]].sum(axis=1)\n    s = points.idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n    s.name = self.name\n    return s\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.from_label_encoding","title":"<code>from_label_encoding(self, points)</code>","text":"<p>Convert points back from label-encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_label_encoding(self, points: pd.Series) -&gt; pd.Series:\n\"\"\"Convert points back from label-encoding.\"\"\"\n    enc = np.array(self.domain)\n    return pd.Series(enc[points], index=points.index)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.from_onehot_encoding","title":"<code>from_onehot_encoding(self, points)</code>","text":"<p>Convert points back from one-hot encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_onehot_encoding(self, points: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Convert points back from one-hot encoding.\"\"\"\n    cat_cols = [f\"{self.name}{_CAT_SEP}{c}\" for c in self.domain]\n    if np.any([c not in cat_cols for c in points.columns]):\n        raise ValueError(\n            f\"{self.name}: Column names don't match categorical levels: {points.columns}, {cat_cols}.\"\n        )\n    s = points.idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n    s.name = self.name\n    return s\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.round","title":"<code>round(self, point)</code>","text":"<p>Round a point to the closest contained values.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>float, np.ndarray, pd.Series or pd.Dataframe</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code>.</p> Source code in <code>opti/parameter.py</code> <pre><code>def round(self, point):\n\"\"\"Round a point to the closest contained values.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point`.\n    \"\"\"\n    if not np.all(self.contains(point)):\n        raise ValueError(\n            f\"{self.name}: Cannot round values for categorical parameter.\"\n        )\n    return point\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.sample","title":"<code>sample(self, n=1)</code>","text":"<p>Draw random samples from the domain.</p> Source code in <code>opti/parameter.py</code> <pre><code>def sample(self, n: int = 1) -&gt; pd.Series:\n\"\"\"Draw random samples from the domain.\"\"\"\n    return pd.Series(name=self.name, data=np.random.choice(self.domain, n))\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.to_dummy_encoding","title":"<code>to_dummy_encoding(self, points)</code>","text":"<p>Convert points to a dummy-hot encoding, dropping the first categorical level.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_dummy_encoding(self, points: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\"\n    return pd.DataFrame(\n        {f\"{self.name}{_CAT_SEP}{c}\": points == c for c in self.domain[1:]},\n        dtype=float,\n    )\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.to_label_encoding","title":"<code>to_label_encoding(self, points)</code>","text":"<p>Convert points to label-encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_label_encoding(self, points: pd.Series) -&gt; pd.Series:\n\"\"\"Convert points to label-encoding.\"\"\"\n    enc = pd.Series(range(len(self.domain)), index=list(self.domain))\n    s = enc[points]\n    s.index = points.index\n    s.name = self.name\n    return s\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.to_onehot_encoding","title":"<code>to_onehot_encoding(self, points)</code>","text":"<p>Convert points to a one-hot encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_onehot_encoding(self, points: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Convert points to a one-hot encoding.\"\"\"\n    return pd.DataFrame(\n        {f\"{self.name}{_CAT_SEP}{c}\": points == c for c in self.domain}, dtype=float\n    )\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous","title":"<code> Continuous            (Parameter)         </code>","text":"<p>Variable that can take on any real value in the specified domain.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the parameter</p> <code>domain</code> <code>list</code> <p>[lower bound, upper bound]</p> Source code in <code>opti/parameter.py</code> <pre><code>class Continuous(Parameter):\n\"\"\"Variable that can take on any real value in the specified domain.\n\n    Attributes:\n        name (str): name of the parameter\n        domain (list): [lower bound, upper bound]\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        domain: Optional[Sequence] = None,\n        **kwargs,\n    ):\n        if domain is None:\n            domain = [-np.inf, np.inf]\n        else:\n            if len(domain) != 2:\n                raise ValueError(\n                    f\"{name}: Domain must consist of two values [low, high].\"\n                )\n        # convert None to +/- inf and string to float\n        low = -np.inf if domain[0] is None else float(domain[0])\n        high = np.inf if domain[1] is None else float(domain[1])\n\n        if high &lt; low:\n            raise ValueError(\n                f\"{name}: Lower bound {low} must be less than upper bound {high}.\"\n            )\n        self.low = low\n        self.high = high\n        super().__init__(name=name, domain=[low, high], type=\"continuous\", **kwargs)\n\n    def __repr__(self):\n        if np.isfinite(self.low) or np.isfinite(self.high):\n            return f\"Continuous('{self.name}', domain={self.domain})\"\n        else:\n            return f\"Continuous('{self.name}')\"\n\n    @property\n    def bounds(self) -&gt; Tuple[float, float]:\n\"\"\"Return the domain bounds.\"\"\"\n        return self.low, self.high\n\n    def contains(self, point):\n\"\"\"Check if a point is in contained in the domain.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with boolean datatype.\n        \"\"\"\n        return (self.low &lt;= point) &amp; (point &lt;= self.high)\n\n    def round(self, point):\n\"\"\"Round a point to the closest contained values.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with values clipped to parameter bounds.\n        \"\"\"\n        return np.clip(point, self.low, self.high)\n\n    def sample(self, n: int = 1) -&gt; pd.Series:\n\"\"\"Draw random samples from the domain.\"\"\"\n        low = max(self.low, np.finfo(np.float32).min)\n        high = min(self.high, np.finfo(np.float32).max)\n        return pd.Series(name=self.name, data=np.random.uniform(low, high, n))\n\n    def to_config(self) -&gt; dict:\n\"\"\"Return a json-serializable configuration dict.\"\"\"\n        conf = dict(name=self.name, type=self.type)\n        low = None if np.isinf(self.low) else float(self.low)\n        high = None if np.isinf(self.high) else float(self.high)\n        if low is not None or high is not None:\n            conf.update({\"domain\": [low, high]})\n        conf.update(self.extra_fields)\n        return conf\n\n    def to_unit_range(self, points):\n\"\"\"Transform points to the unit range: [low, high] -&gt; [0, 1].\n\n        Points outside of the domain will transform to outside of [0, 1].\n        Nothing is done if low == high.\n        \"\"\"\n        if np.isclose(self.low, self.high):\n            return points\n        else:\n            return (points - self.low) / (self.high - self.low)\n\n    def from_unit_range(self, points):\n\"\"\"Transform points from the unit range: [0, 1] -&gt; [low, high].\n\n        A rounding is applied to correct for numerical precision.\n        Nothing is done if low == high.\n        \"\"\"\n        if np.isclose(self.low, self.high):\n            return points\n        else:\n            return points * (self.high - self.low) + self.low\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.bounds","title":"<code>bounds: Tuple[float, float]</code>  <code>property</code> <code>readonly</code>","text":"<p>Return the domain bounds.</p>"},{"location":"ref-parameter/#opti.parameter.Continuous.contains","title":"<code>contains(self, point)</code>","text":"<p>Check if a point is in contained in the domain.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>float, np.ndarray, pd.Series or pd.Dataframe</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with boolean datatype.</p> Source code in <code>opti/parameter.py</code> <pre><code>def contains(self, point):\n\"\"\"Check if a point is in contained in the domain.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with boolean datatype.\n    \"\"\"\n    return (self.low &lt;= point) &amp; (point &lt;= self.high)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.from_unit_range","title":"<code>from_unit_range(self, points)</code>","text":"<p>Transform points from the unit range: [0, 1] -&gt; [low, high].</p> <p>A rounding is applied to correct for numerical precision. Nothing is done if low == high.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_unit_range(self, points):\n\"\"\"Transform points from the unit range: [0, 1] -&gt; [low, high].\n\n    A rounding is applied to correct for numerical precision.\n    Nothing is done if low == high.\n    \"\"\"\n    if np.isclose(self.low, self.high):\n        return points\n    else:\n        return points * (self.high - self.low) + self.low\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.round","title":"<code>round(self, point)</code>","text":"<p>Round a point to the closest contained values.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>float, np.ndarray, pd.Series or pd.Dataframe</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with values clipped to parameter bounds.</p> Source code in <code>opti/parameter.py</code> <pre><code>def round(self, point):\n\"\"\"Round a point to the closest contained values.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with values clipped to parameter bounds.\n    \"\"\"\n    return np.clip(point, self.low, self.high)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.sample","title":"<code>sample(self, n=1)</code>","text":"<p>Draw random samples from the domain.</p> Source code in <code>opti/parameter.py</code> <pre><code>def sample(self, n: int = 1) -&gt; pd.Series:\n\"\"\"Draw random samples from the domain.\"\"\"\n    low = max(self.low, np.finfo(np.float32).min)\n    high = min(self.high, np.finfo(np.float32).max)\n    return pd.Series(name=self.name, data=np.random.uniform(low, high, n))\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.to_config","title":"<code>to_config(self)</code>","text":"<p>Return a json-serializable configuration dict.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_config(self) -&gt; dict:\n\"\"\"Return a json-serializable configuration dict.\"\"\"\n    conf = dict(name=self.name, type=self.type)\n    low = None if np.isinf(self.low) else float(self.low)\n    high = None if np.isinf(self.high) else float(self.high)\n    if low is not None or high is not None:\n        conf.update({\"domain\": [low, high]})\n    conf.update(self.extra_fields)\n    return conf\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.to_unit_range","title":"<code>to_unit_range(self, points)</code>","text":"<p>Transform points to the unit range: [low, high] -&gt; [0, 1].</p> <p>Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_unit_range(self, points):\n\"\"\"Transform points to the unit range: [low, high] -&gt; [0, 1].\n\n    Points outside of the domain will transform to outside of [0, 1].\n    Nothing is done if low == high.\n    \"\"\"\n    if np.isclose(self.low, self.high):\n        return points\n    else:\n        return (points - self.low) / (self.high - self.low)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete","title":"<code> Discrete            (Parameter)         </code>","text":"<p>Variable with a discrete domain (ordinal scale).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the parameter</p> <code>domain</code> <code>list</code> <p>list of possible numeric values</p> Source code in <code>opti/parameter.py</code> <pre><code>class Discrete(Parameter):\n\"\"\"Variable with a discrete domain (ordinal scale).\n\n    Attributes:\n        name (str): name of the parameter\n        domain (list): list of possible numeric values\n    \"\"\"\n\n    def __init__(self, name: str, domain: Sequence, **kwargs):\n        if len(domain) &lt; 1:\n            raise ValueError(f\"{name}: Domain must contain at least one value.\")\n        try:\n            # convert to a sorted list of floats\n            domain = np.sort(np.array(domain).astype(float)).tolist()\n        except ValueError:\n            raise ValueError(f\"{name}: Domain contains non-numeric values.\")\n        if len(set(domain)) != len(domain):\n            raise ValueError(f\"{name}: Domain contains duplicates.\")\n        self.low = min(domain)\n        self.high = max(domain)\n        super().__init__(name, domain, type=\"discrete\", **kwargs)\n\n    def __repr__(self):\n        return f\"Discrete('{self.name}', domain={self.domain})\"\n\n    @property\n    def bounds(self) -&gt; Tuple[float, float]:\n\"\"\"Return the domain bounds.\"\"\"\n        return self.low, self.high\n\n    def contains(self, point):\n\"\"\"Check if a point is in contained in the domain.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with boolean datatype.\n        \"\"\"\n        if not np.isscalar(point):\n            point = np.array(point)\n        return np.isin(point, self.domain)\n\n    def round(self, point):\n\"\"\"Round a point to the closest contained values.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with values clipped to parameter bounds.\n        \"\"\"\n        if np.isscalar(point):\n            i = np.argmin(np.abs(np.array(self.domain) - point))\n            return self.domain[i]\n        closest = [np.argmin(np.abs(np.array(self.domain) - p)) for p in point]\n        rounded = np.array(self.domain)[closest]\n        if isinstance(point, np.ndarray):\n            return rounded\n        elif isinstance(point, pd.Series):\n            return pd.Series(name=self.name, data=rounded, index=point.index)\n        elif isinstance(point, pd.DataFrame):\n            return pd.DataFrame({self.name: rounded}, index=point.index)\n\n    def sample(self, n: int = 1) -&gt; pd.Series:\n\"\"\"Draw random samples from the domain.\"\"\"\n        return pd.Series(name=self.name, data=np.random.choice(self.domain, n))\n\n    def to_unit_range(self, points):\n\"\"\"Transform points to the unit range: [low, high] -&gt; [0, 1].\n\n        Points outside of the domain will transform to outside of [0, 1].\n        Nothing is done if low == high.\n        \"\"\"\n        if np.isclose(self.low, self.high):\n            return points\n        else:\n            return (points - self.low) / (self.high - self.low)\n\n    def from_unit_range(self, points):\n\"\"\"Transform points from the unit range: [0, 1] -&gt; [low, high].\n\n        A rounding is applied to correct for numerical precision.\n        Nothing is done if low == high.\n        \"\"\"\n        if np.isclose(self.low, self.high):\n            return points\n        else:\n            points = points * (self.high - self.low) + self.low\n            return self.round(points)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.bounds","title":"<code>bounds: Tuple[float, float]</code>  <code>property</code> <code>readonly</code>","text":"<p>Return the domain bounds.</p>"},{"location":"ref-parameter/#opti.parameter.Discrete.contains","title":"<code>contains(self, point)</code>","text":"<p>Check if a point is in contained in the domain.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>float, np.ndarray, pd.Series or pd.Dataframe</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with boolean datatype.</p> Source code in <code>opti/parameter.py</code> <pre><code>def contains(self, point):\n\"\"\"Check if a point is in contained in the domain.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with boolean datatype.\n    \"\"\"\n    if not np.isscalar(point):\n        point = np.array(point)\n    return np.isin(point, self.domain)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.from_unit_range","title":"<code>from_unit_range(self, points)</code>","text":"<p>Transform points from the unit range: [0, 1] -&gt; [low, high].</p> <p>A rounding is applied to correct for numerical precision. Nothing is done if low == high.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_unit_range(self, points):\n\"\"\"Transform points from the unit range: [0, 1] -&gt; [low, high].\n\n    A rounding is applied to correct for numerical precision.\n    Nothing is done if low == high.\n    \"\"\"\n    if np.isclose(self.low, self.high):\n        return points\n    else:\n        points = points * (self.high - self.low) + self.low\n        return self.round(points)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.round","title":"<code>round(self, point)</code>","text":"<p>Round a point to the closest contained values.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>float, np.ndarray, pd.Series or pd.Dataframe</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with values clipped to parameter bounds.</p> Source code in <code>opti/parameter.py</code> <pre><code>def round(self, point):\n\"\"\"Round a point to the closest contained values.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with values clipped to parameter bounds.\n    \"\"\"\n    if np.isscalar(point):\n        i = np.argmin(np.abs(np.array(self.domain) - point))\n        return self.domain[i]\n    closest = [np.argmin(np.abs(np.array(self.domain) - p)) for p in point]\n    rounded = np.array(self.domain)[closest]\n    if isinstance(point, np.ndarray):\n        return rounded\n    elif isinstance(point, pd.Series):\n        return pd.Series(name=self.name, data=rounded, index=point.index)\n    elif isinstance(point, pd.DataFrame):\n        return pd.DataFrame({self.name: rounded}, index=point.index)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.sample","title":"<code>sample(self, n=1)</code>","text":"<p>Draw random samples from the domain.</p> Source code in <code>opti/parameter.py</code> <pre><code>def sample(self, n: int = 1) -&gt; pd.Series:\n\"\"\"Draw random samples from the domain.\"\"\"\n    return pd.Series(name=self.name, data=np.random.choice(self.domain, n))\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.to_unit_range","title":"<code>to_unit_range(self, points)</code>","text":"<p>Transform points to the unit range: [low, high] -&gt; [0, 1].</p> <p>Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_unit_range(self, points):\n\"\"\"Transform points to the unit range: [low, high] -&gt; [0, 1].\n\n    Points outside of the domain will transform to outside of [0, 1].\n    Nothing is done if low == high.\n    \"\"\"\n    if np.isclose(self.low, self.high):\n        return points\n    else:\n        return (points - self.low) / (self.high - self.low)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameter","title":"<code> Parameter        </code>","text":"<p>Parameter base class.</p> Source code in <code>opti/parameter.py</code> <pre><code>class Parameter:\n\"\"\"Parameter base class.\"\"\"\n\n    def __init__(self, name: str, domain: Sequence, type: str = None, **kwargs):\n        self.name = name\n        self.domain = domain\n        self.type = type\n        self.extra_fields = kwargs\n\n    def to_config(self) -&gt; dict:\n\"\"\"Return a json-serializable configuration dict.\"\"\"\n        conf = dict(name=self.name, type=self.type, domain=self.domain)\n        conf.update(self.extra_fields)\n        return conf\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameter.to_config","title":"<code>to_config(self)</code>","text":"<p>Return a json-serializable configuration dict.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_config(self) -&gt; dict:\n\"\"\"Return a json-serializable configuration dict.\"\"\"\n    conf = dict(name=self.name, type=self.type, domain=self.domain)\n    conf.update(self.extra_fields)\n    return conf\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters","title":"<code> Parameters        </code>","text":"<p>Set of parameters representing either the input or the output parameter space.</p> Source code in <code>opti/parameter.py</code> <pre><code>class Parameters:\n\"\"\"Set of parameters representing either the input or the output parameter space.\"\"\"\n\n    def __init__(self, parameters):\n\"\"\"\n        It can be constructed either from a list / tuple (of at least one) Parameter objects\n        ```\n        Parameters([\n            Continuous(name=\"foo\", domain=[1, 10]),\n            Discrete(name=\"bar\", domain=[1, 2, 3, 4]),\n            Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]),\n        ])\n        ```\n        or from a list / tuple of dicts\n        ```\n        Parameters([\n            {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]},\n            {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]},\n            {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]},\n            {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"},\n        ])\n        ```\n        In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses.\n        Parameters(conf).to_config() == conf\n        \"\"\"\n        if not isinstance(parameters, (list, tuple)):\n            raise TypeError(\"Parameters expects a list or tuple of parameters.\")\n\n        self.parameters = {}\n        for d in parameters:\n            if not isinstance(d, Parameter):\n                d = make_parameter(**d)\n            if d.name in self.parameters:\n                raise ValueError(f\"Duplicate parameter name {d.name}\")\n            self.parameters[d.name] = d\n\n    def __repr__(self):\n        return \"Parameters(\\n\" + pprint.pformat(list(self.parameters.values())) + \"\\n)\"\n\n    def __iter__(self):\n        return iter(self.parameters.values())\n\n    def __getitem__(self, name):\n        return self.parameters[name]\n\n    def __len__(self):\n        return len(self.parameters)\n\n    def __add__(self, other):\n        parameters = list(self.parameters.values()) + list(other.parameters.values())\n        return Parameters(parameters)\n\n    @property\n    def names(self):\n        return list(self.parameters.keys())\n\n    @property\n    def bounds(self) -&gt; pd.DataFrame:\n\"\"\"Return the parameter bounds.\"\"\"\n        return pd.DataFrame({p.name: p.bounds for p in self}, index=[\"min\", \"max\"])\n\n    def contains(self, points: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if points are inside the domain of each parameter.\"\"\"\n        if isinstance(points, pd.DataFrame):\n            points = points[self.names]\n        b = np.stack([self[k].contains(v) for k, v in points.iteritems()], axis=1)\n        return b.all(axis=1)\n\n    def round(self, points: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Round points to the closest contained values.\"\"\"\n        return pd.concat([self[k].round(v) for k, v in points.iteritems()], axis=1)\n\n    def sample(self, n: int = 1) -&gt; pd.DataFrame:\n\"\"\"Draw uniformly distributed random samples.\"\"\"\n        return pd.concat([param.sample(n) for param in self], axis=1)\n\n    def transform(\n        self,\n        points: pd.DataFrame,\n        continuous: str = \"none\",\n        discrete: str = \"none\",\n        categorical: str = \"onehot-encode\",\n    ) -&gt; pd.DataFrame:\n\"\"\"Transfrom the given dataframe according to a set of transformation rules.\n\n        Args:\n            points (pd.DataFrame): Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns.\n            continuous (str, optional): Transform for continuous parameters. Options are\n                - \"none\" (default): keep values unchanged.\n                - \"normalize\": transforms the domain bounds to [0, 1]\n            discrete (str, optional): Transform for discrete parameters. Options are\n                - \"none\" (default): keep values unchanged.\n                - \"normalize\": transforms the domain bounds to [0, 1]\n            categorical (str, optional): Transform for categoricals. Options are\n                - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1]\n                - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1]\n                - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2]\n                - \"none\": keep values unchanged\n\n        Raises:\n            ValueError: Unknown transform.\n\n        Returns:\n            pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped.\n        \"\"\"\n        transformed = []\n        for p in self:\n            s = points[p.name]\n            if isinstance(p, Continuous):\n                if continuous == \"none\":\n                    transformed.append(s)\n                elif continuous == \"normalize\":\n                    transformed.append(p.to_unit_range(s))\n                else:\n                    raise ValueError(f\"Unknown continuous transform {continuous}.\")\n            if isinstance(p, Discrete):\n                if discrete == \"none\":\n                    transformed.append(s)\n                elif discrete == \"normalize\":\n                    transformed.append(p.to_unit_range(s))\n                else:\n                    raise ValueError(f\"Unknown discrete transform {discrete}.\")\n            if isinstance(p, Categorical):\n                if categorical == \"none\":\n                    transformed.append(s)\n                elif categorical == \"onehot-encode\":\n                    transformed.append(p.to_onehot_encoding(s))\n                elif categorical == \"dummy-encode\":\n                    transformed.append(p.to_dummy_encoding(s))\n                elif categorical == \"label-encode\":\n                    transformed.append(p.to_label_encoding(s))\n                else:\n                    raise ValueError(f\"Unknown categorical transform {categorical}.\")\n        return pd.concat(transformed, axis=1)\n\n    def to_config(self) -&gt; List[dict]:\n\"\"\"Configuration of the parameter space.\"\"\"\n        return [param.to_config() for param in self.parameters.values()]\n\n    def get(self, types) -&gt; \"Parameters\":\n\"\"\"Get all parameters of the given type(s).\"\"\"\n        return Parameters([p for p in self if isinstance(p, types)])\n\n    def to_df(self, x: np.ndarray, to_numeric=False) -&gt; pd.DataFrame:\n\"\"\"Create a dataframe for a given numpy array of parameter values.\"\"\"\n        X = pd.DataFrame(np.atleast_2d(x), columns=self.names)\n        if to_numeric:\n            for n in self.get((Continuous, Discrete)).names:\n                X[n] = pd.to_numeric(X[n])\n        return X\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.bounds","title":"<code>bounds: DataFrame</code>  <code>property</code> <code>readonly</code>","text":"<p>Return the parameter bounds.</p>"},{"location":"ref-parameter/#opti.parameter.Parameters.__init__","title":"<code>__init__(self, parameters)</code>  <code>special</code>","text":"<p>It can be constructed either from a list / tuple (of at least one) Parameter objects <pre><code>Parameters([\n    Continuous(name=\"foo\", domain=[1, 10]),\n    Discrete(name=\"bar\", domain=[1, 2, 3, 4]),\n    Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]),\n])\n</code></pre> or from a list / tuple of dicts <pre><code>Parameters([\n    {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]},\n    {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]},\n    {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]},\n    {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"},\n])\n</code></pre> In particular, Parameters().init and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf</p> Source code in <code>opti/parameter.py</code> <pre><code>def __init__(self, parameters):\n\"\"\"\n    It can be constructed either from a list / tuple (of at least one) Parameter objects\n    ```\n    Parameters([\n        Continuous(name=\"foo\", domain=[1, 10]),\n        Discrete(name=\"bar\", domain=[1, 2, 3, 4]),\n        Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]),\n    ])\n    ```\n    or from a list / tuple of dicts\n    ```\n    Parameters([\n        {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]},\n        {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]},\n        {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]},\n        {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"},\n    ])\n    ```\n    In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses.\n    Parameters(conf).to_config() == conf\n    \"\"\"\n    if not isinstance(parameters, (list, tuple)):\n        raise TypeError(\"Parameters expects a list or tuple of parameters.\")\n\n    self.parameters = {}\n    for d in parameters:\n        if not isinstance(d, Parameter):\n            d = make_parameter(**d)\n        if d.name in self.parameters:\n            raise ValueError(f\"Duplicate parameter name {d.name}\")\n        self.parameters[d.name] = d\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.contains","title":"<code>contains(self, points)</code>","text":"<p>Check if points are inside the domain of each parameter.</p> Source code in <code>opti/parameter.py</code> <pre><code>def contains(self, points: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if points are inside the domain of each parameter.\"\"\"\n    if isinstance(points, pd.DataFrame):\n        points = points[self.names]\n    b = np.stack([self[k].contains(v) for k, v in points.iteritems()], axis=1)\n    return b.all(axis=1)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.get","title":"<code>get(self, types)</code>","text":"<p>Get all parameters of the given type(s).</p> Source code in <code>opti/parameter.py</code> <pre><code>def get(self, types) -&gt; \"Parameters\":\n\"\"\"Get all parameters of the given type(s).\"\"\"\n    return Parameters([p for p in self if isinstance(p, types)])\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.round","title":"<code>round(self, points)</code>","text":"<p>Round points to the closest contained values.</p> Source code in <code>opti/parameter.py</code> <pre><code>def round(self, points: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Round points to the closest contained values.\"\"\"\n    return pd.concat([self[k].round(v) for k, v in points.iteritems()], axis=1)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.sample","title":"<code>sample(self, n=1)</code>","text":"<p>Draw uniformly distributed random samples.</p> Source code in <code>opti/parameter.py</code> <pre><code>def sample(self, n: int = 1) -&gt; pd.DataFrame:\n\"\"\"Draw uniformly distributed random samples.\"\"\"\n    return pd.concat([param.sample(n) for param in self], axis=1)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.to_config","title":"<code>to_config(self)</code>","text":"<p>Configuration of the parameter space.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_config(self) -&gt; List[dict]:\n\"\"\"Configuration of the parameter space.\"\"\"\n    return [param.to_config() for param in self.parameters.values()]\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.to_df","title":"<code>to_df(self, x, to_numeric=False)</code>","text":"<p>Create a dataframe for a given numpy array of parameter values.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_df(self, x: np.ndarray, to_numeric=False) -&gt; pd.DataFrame:\n\"\"\"Create a dataframe for a given numpy array of parameter values.\"\"\"\n    X = pd.DataFrame(np.atleast_2d(x), columns=self.names)\n    if to_numeric:\n        for n in self.get((Continuous, Discrete)).names:\n            X[n] = pd.to_numeric(X[n])\n    return X\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.transform","title":"<code>transform(self, points, continuous='none', discrete='none', categorical='onehot-encode')</code>","text":"<p>Transfrom the given dataframe according to a set of transformation rules.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>pd.DataFrame</code> <p>Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns.</p> required <code>continuous</code> <code>str</code> <p>Transform for continuous parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1]</p> <code>'none'</code> <code>discrete</code> <code>str</code> <p>Transform for discrete parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1]</p> <code>'none'</code> <code>categorical</code> <code>str</code> <p>Transform for categoricals. Options are - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1] - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1] - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2] - \"none\": keep values unchanged</p> <code>'onehot-encode'</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>Unknown transform.</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Transformed points. Columns that don't correspond to parameters are dropped.</p> Source code in <code>opti/parameter.py</code> <pre><code>def transform(\n    self,\n    points: pd.DataFrame,\n    continuous: str = \"none\",\n    discrete: str = \"none\",\n    categorical: str = \"onehot-encode\",\n) -&gt; pd.DataFrame:\n\"\"\"Transfrom the given dataframe according to a set of transformation rules.\n\n    Args:\n        points (pd.DataFrame): Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns.\n        continuous (str, optional): Transform for continuous parameters. Options are\n            - \"none\" (default): keep values unchanged.\n            - \"normalize\": transforms the domain bounds to [0, 1]\n        discrete (str, optional): Transform for discrete parameters. Options are\n            - \"none\" (default): keep values unchanged.\n            - \"normalize\": transforms the domain bounds to [0, 1]\n        categorical (str, optional): Transform for categoricals. Options are\n            - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1]\n            - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1]\n            - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2]\n            - \"none\": keep values unchanged\n\n    Raises:\n        ValueError: Unknown transform.\n\n    Returns:\n        pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped.\n    \"\"\"\n    transformed = []\n    for p in self:\n        s = points[p.name]\n        if isinstance(p, Continuous):\n            if continuous == \"none\":\n                transformed.append(s)\n            elif continuous == \"normalize\":\n                transformed.append(p.to_unit_range(s))\n            else:\n                raise ValueError(f\"Unknown continuous transform {continuous}.\")\n        if isinstance(p, Discrete):\n            if discrete == \"none\":\n                transformed.append(s)\n            elif discrete == \"normalize\":\n                transformed.append(p.to_unit_range(s))\n            else:\n                raise ValueError(f\"Unknown discrete transform {discrete}.\")\n        if isinstance(p, Categorical):\n            if categorical == \"none\":\n                transformed.append(s)\n            elif categorical == \"onehot-encode\":\n                transformed.append(p.to_onehot_encoding(s))\n            elif categorical == \"dummy-encode\":\n                transformed.append(p.to_dummy_encoding(s))\n            elif categorical == \"label-encode\":\n                transformed.append(p.to_label_encoding(s))\n            else:\n                raise ValueError(f\"Unknown categorical transform {categorical}.\")\n    return pd.concat(transformed, axis=1)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.make_parameter","title":"<code>make_parameter(name, type, domain=None, **kwargs)</code>","text":"<p>Make a parameter object from a configuration</p> <p>p = make_parameter(**config)</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>\"continuous\", \"discrete\" or \"categorical\"</p> required <code>name</code> <code>str</code> <p>Name of the parameter</p> required <code>domain</code> <code>list</code> <p>Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"]</p> <code>None</code> Source code in <code>opti/parameter.py</code> <pre><code>def make_parameter(\n    name: str,\n    type: str,\n    domain: Optional[Sequence] = None,\n    **kwargs,\n):\n\"\"\"Make a parameter object from a configuration\n\n    p = make_parameter(**config)\n\n    Args:\n        type (str): \"continuous\", \"discrete\" or \"categorical\"\n        name (str): Name of the parameter\n        domain (list): Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"]\n    \"\"\"\n    parameter = {\n        \"continuous\": Continuous,\n        \"discrete\": Discrete,\n        \"categorical\": Categorical,\n    }[type.lower()]\n    if domain is None and parameter is not Continuous:\n        raise ValueError(f\"Domain not given for parameter {name}.\")\n\n    return parameter(name=name, domain=domain, **kwargs)\n</code></pre>"},{"location":"ref-problem/","title":"Problem","text":""},{"location":"ref-problem/#opti.problem.Problem","title":"<code> Problem        </code>","text":"Source code in <code>opti/problem.py</code> <pre><code>class Problem:\n    def __init__(\n        self,\n        inputs: ParametersLike,\n        outputs: ParametersLike,\n        objectives: Optional[ObjectivesLike] = None,\n        constraints: Optional[ConstraintsLike] = None,\n        output_constraints: Optional[ObjectivesLike] = None,\n        f: Optional[Callable] = None,\n        models: Optional[ModelsLike] = None,\n        data: Optional[DataFrameLike] = None,\n        optima: Optional[DataFrameLike] = None,\n        name: Optional[str] = None,\n        **kwargs,\n    ):\n\"\"\"An optimization problem.\n\n        Args:\n            inputs: Input parameters.\n            outputs: Output parameters.\n            objectives: Optimization objectives. Defaults to minimization.\n            constraints: Constraints on the inputs.\n            output_constraints: Constraints on the outputs.\n            f: Function to evaluate the outputs for given inputs.\n                Must have the signature: f(x: pd.DataFrame) -&gt; pd.DataFrame\n            data: Experimental data.\n            optima: Pareto optima.\n            name: Name of the problem.\n        \"\"\"\n        self.name = name if name is not None else \"Problem\"\n        self.inputs = inputs if isinstance(inputs, Parameters) else Parameters(inputs)\n        self.outputs = (\n            outputs if isinstance(outputs, Parameters) else Parameters(outputs)\n        )\n\n        if objectives is None:\n            self.objectives = Objectives([Minimize(m) for m in self.outputs.names])\n        elif isinstance(objectives, Objectives):\n            self.objectives = objectives\n        else:\n            self.objectives = Objectives(objectives)\n\n        if isinstance(constraints, Constraints):\n            pass\n        elif not constraints:\n            constraints = None\n        else:\n            constraints = Constraints(constraints)\n            if len(constraints) == 0:  # no valid constraints\n                constraints = None\n        self.constraints = constraints\n\n        if isinstance(output_constraints, Objectives) or output_constraints is None:\n            self.output_constraints = output_constraints\n        else:\n            self.output_constraints = Objectives(output_constraints)\n\n        if isinstance(models, Models) or models is None:\n            self.models = models\n        else:\n            self.models = Models(models)\n\n        if f is not None:\n            self.f = f\n\n        if isinstance(data, dict):\n            data = pd.read_json(json.dumps(data), orient=\"split\")\n\n        if isinstance(optima, dict):\n            optima = pd.read_json(json.dumps(optima), orient=\"split\")\n\n        self.set_data(data)\n        self.set_optima(optima)\n        self.check_problem()\n        self.check_models()\n\n    @property\n    def n_inputs(self) -&gt; int:\n        return len(self.inputs)\n\n    @property\n    def n_outputs(self) -&gt; int:\n        return len(self.outputs)\n\n    @property\n    def n_objectives(self) -&gt; int:\n        return len(self.objectives)\n\n    @property\n    def n_constraints(self) -&gt; int:\n        return 0 if self.constraints is None else len(self.constraints)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        s = \"Problem(\\n\"\n        s += f\"name={self.name},\\n\"\n        s += f\"inputs={self.inputs},\\n\"\n        s += f\"outputs={self.outputs},\\n\"\n        s += f\"objectives={self.objectives},\\n\"\n        if self.output_constraints is not None:\n            s += f\"output_constraints={self.output_constraints},\\n\"\n        if self.constraints is not None:\n            s += f\"constraints={self.constraints},\\n\"\n        if self.models is not None:\n            s += f\"models={self.models},\\n\"\n        if self.data is not None:\n            s += f\"data=\\n{self.data.head()}\\n\"\n        if self.optima is not None:\n            s += f\"optima=\\n{self.optima.head()}\\n\"\n        return s + \")\"\n\n    @staticmethod\n    def from_config(config: dict) -&gt; \"Problem\":\n\"\"\"Create a Problem instance from a configuration dict.\"\"\"\n        return Problem(**config)\n\n    def to_config(self) -&gt; dict:\n\"\"\"Return json-serializable configuration dict.\"\"\"\n\n        config = {\n            \"name\": self.name,\n            \"inputs\": self.inputs.to_config(),\n            \"outputs\": self.outputs.to_config(),\n            \"objectives\": self.objectives.to_config(),\n        }\n        if self.output_constraints is not None:\n            config[\"output_constraints\"] = self.output_constraints.to_config()\n        if self.constraints is not None:\n            config[\"constraints\"] = self.constraints.to_config()\n        if self.models is not None:\n            config[\"models\"] = self.models.to_config()\n        if self.data is not None:\n            config[\"data\"] = self.data.replace({np.nan: None}).to_dict(\"split\")\n        if self.optima is not None:\n            config[\"optima\"] = self.optima.replace({np.nan: None}).to_dict(\"split\")\n        return config\n\n    @staticmethod\n    def from_json(fname: PathLike) -&gt; \"Problem\":\n\"\"\"Read a problem from a JSON file.\"\"\"\n        with open(fname, \"rb\") as infile:\n            config = json.loads(infile.read())\n        return Problem(**config)\n\n    def to_json(self, fname: PathLike) -&gt; None:\n\"\"\"Save a problem from a JSON file.\"\"\"\n        with open(fname, \"wb\") as outfile:\n            b = json.dumps(self.to_config(), ensure_ascii=False, separators=(\",\", \":\"))\n            outfile.write(b.encode(\"utf-8\"))\n\n    def check_problem(self) -&gt; None:\n\"\"\"Check if input and output parameters are consistent.\"\"\"\n        # check for duplicate names\n        duplicates = set(self.inputs.names).intersection(self.outputs.names)\n        if duplicates:\n            raise ValueError(f\"Parameter name in both inputs and outputs: {duplicates}\")\n\n        # check if all objectives refer to an output\n        for obj in self.objectives:\n            if obj.name not in self.outputs.names:\n                raise ValueError(f\"Objective refers to unknown parameter: {obj.name}\")\n\n    def check_data(self, data: pd.DataFrame) -&gt; None:\n\"\"\"Check if data is consistent with input and output parameters.\"\"\"\n        for p in self.inputs + self.outputs:\n            # data must contain all parameters\n            if p.name not in data.columns:\n                raise ValueError(\n                    f\"Parameter {p.name} is missing. Data must contain all parameters.\"\n                )\n\n            # data for continuous / discrete parameters must be numeric\n            if isinstance(p, (Continuous, Discrete)):\n                ok = is_numeric_dtype(data[p.name]) or data[p.name].isnull().all()\n                if not ok:\n                    raise ValueError(\n                        f\"Parameter {p.name} contains non-numeric values. Data for continuous / discrete parameters must be numeric.\"\n                    )\n\n            # categorical levels in data must be specified\n            elif isinstance(p, Categorical):\n                ok = p.contains(data[p.name]) | data[p.name].isna()\n                if not ok.all():\n                    unknowns = data[p.name][~ok].unique().tolist()\n                    raise ValueError(\n                        f\"Data for parameter {p.name} contains unknown values: {unknowns}. All categorical levels must be specified.\"\n                    )\n\n        # inputs must be complete\n        for p in self.inputs:\n            if data[p.name].isnull().any():\n                raise ValueError(\n                    f\"Input parameter {p.name} has missing data. Inputs must be complete.\"\n                )\n\n        # outputs must have at least one observation\n        for p in self.outputs:\n            if data[p.name].isnull().all():\n                raise ValueError(\n                    f\"Output parameter {p.name} has no data. Outputs must have at least one observation.\"\n                )\n\n    def check_models(self) -&gt; None:\n\"\"\"Check if the models are well defined\"\"\"\n        if self.models is None:\n            return\n\n        for model in self.models:\n            # models need to refer to output parameters\n            for n in model.names:\n                if n not in self.outputs.names:\n                    raise ValueError(f\"Model {model} refers to unknown outputs\")\n\n            if isinstance(model, LinearModel):\n                if len(model.coefficients) != self.n_inputs:\n                    raise ValueError(f\"Model {model} has wrong number of coefficients.\")\n\n    def set_data(self, data: Optional[pd.DataFrame]) -&gt; None:\n\"\"\"Set the data.\"\"\"\n        if data is not None:\n            for p in self.inputs:\n                # Categorical levels are required to be strings. Ensure that the corresponding data is as well.\n                if isinstance(p, Categorical):\n                    nulls = data[p.name].isna()\n                    data[p.name] = data[p.name].astype(str).mask(nulls, np.nan)\n\n            self.check_data(data)\n\n        self.data = data\n\n    def get_data(self) -&gt; pd.DataFrame:\n\"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\"\n        if self.data is None:\n            return pd.DataFrame(columns=self.inputs.names + self.outputs.names)\n        return self.data\n\n    def add_data(self, data: pd.DataFrame) -&gt; None:\n\"\"\"Add a number of data points.\"\"\"\n        self.check_data(data)\n        self.data = pd.concat([self.data, data], axis=0)\n\n    def set_optima(self, optima: Optional[pd.DataFrame]) -&gt; None:\n\"\"\"Set the optima / Pareto front.\"\"\"\n        if optima is not None:\n            self.check_data(optima)\n        self.optima = optima\n\n    def get_X(self, data: Optional[pd.DataFrame] = None) -&gt; np.ndarray:\n\"\"\"Return the input values in `data` or `self.data`.\"\"\"\n        if data is not None:\n            return data[self.inputs.names].values\n        return self.get_data()[self.inputs.names].values\n\n    def get_Y(self, data: Optional[pd.DataFrame] = None) -&gt; np.ndarray:\n\"\"\"Return the output values in `data` or `self.data`.\"\"\"\n        if data is not None:\n            return data[self.outputs.names].values\n        return self.get_data()[self.outputs.names].values\n\n    def get_XY(\n        self,\n        outputs: Optional[List[str]] = None,\n        data: Optional[pd.DataFrame] = None,\n        continuous: str = \"none\",\n        discrete: str = \"none\",\n        categorical: str = \"none\",\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Return the input and output values as numeric numpy arrays.\n\n        Rows with missing output values will be dropped.\n        Input values are assumed to be complete.\n        Categorical outputs are one-hot or label encoded.\n\n        Args:\n            outputs (optional): Subset of the outputs to consider.\n            data (optional): Dataframe to consider instead of problem.data\n        \"\"\"\n        if outputs is None:\n            outputs = self.outputs.names\n        if data is None:\n            data = self.get_data()\n        notna = data[outputs].notna().all(axis=1)\n        X = self.inputs.transform(\n            data, continuous=continuous, discrete=discrete, categorical=categorical\n        )[notna].values\n        Y = data[outputs][notna].values\n        return X, Y\n\n    def get_X_bounds(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Return the lower and upper data bounds.\"\"\"\n        X = self.get_X()\n        xlo = X.min(axis=0)\n        xhi = X.max(axis=0)\n        b = xlo == xhi\n        xhi[b] = xlo[b] + 1  # prevent division by zero when dividing by (xhi - xlo)\n        return xlo, xhi\n\n    def sample_inputs(self, n_samples=10) -&gt; pd.DataFrame:\n\"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\"\n        if self.constraints is None:\n            return sobol_sampling(n_samples, self.inputs)\n        return constrained_sampling(n_samples, self.inputs, self.constraints)\n\n    def create_initial_data(self, n_samples: int = 10) -&gt; None:\n\"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\"\n        if self.f is None:\n            raise NotImplementedError(\"problem.f is not implemented for the problem.\")\n        X = self.sample_inputs(n_samples)\n        Y = self.f(X)\n        self.data = pd.concat([X, Y], axis=1)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.__init__","title":"<code>__init__(self, inputs, outputs, objectives=None, constraints=None, output_constraints=None, f=None, models=None, data=None, optima=None, name=None, **kwargs)</code>  <code>special</code>","text":"<p>An optimization problem.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]]</code> <p>Input parameters.</p> required <code>outputs</code> <code>Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]]</code> <p>Output parameters.</p> required <code>objectives</code> <code>Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]]</code> <p>Optimization objectives. Defaults to minimization.</p> <code>None</code> <code>constraints</code> <code>Union[opti.constraint.Constraints, List[opti.constraint.Constraint], List[Dict]]</code> <p>Constraints on the inputs.</p> <code>None</code> <code>output_constraints</code> <code>Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]]</code> <p>Constraints on the outputs.</p> <code>None</code> <code>f</code> <code>Optional[Callable]</code> <p>Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -&gt; pd.DataFrame</p> <code>None</code> <code>data</code> <code>Union[pandas.core.frame.DataFrame, Dict]</code> <p>Experimental data.</p> <code>None</code> <code>optima</code> <code>Union[pandas.core.frame.DataFrame, Dict]</code> <p>Pareto optima.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of the problem.</p> <code>None</code> Source code in <code>opti/problem.py</code> <pre><code>def __init__(\n    self,\n    inputs: ParametersLike,\n    outputs: ParametersLike,\n    objectives: Optional[ObjectivesLike] = None,\n    constraints: Optional[ConstraintsLike] = None,\n    output_constraints: Optional[ObjectivesLike] = None,\n    f: Optional[Callable] = None,\n    models: Optional[ModelsLike] = None,\n    data: Optional[DataFrameLike] = None,\n    optima: Optional[DataFrameLike] = None,\n    name: Optional[str] = None,\n    **kwargs,\n):\n\"\"\"An optimization problem.\n\n    Args:\n        inputs: Input parameters.\n        outputs: Output parameters.\n        objectives: Optimization objectives. Defaults to minimization.\n        constraints: Constraints on the inputs.\n        output_constraints: Constraints on the outputs.\n        f: Function to evaluate the outputs for given inputs.\n            Must have the signature: f(x: pd.DataFrame) -&gt; pd.DataFrame\n        data: Experimental data.\n        optima: Pareto optima.\n        name: Name of the problem.\n    \"\"\"\n    self.name = name if name is not None else \"Problem\"\n    self.inputs = inputs if isinstance(inputs, Parameters) else Parameters(inputs)\n    self.outputs = (\n        outputs if isinstance(outputs, Parameters) else Parameters(outputs)\n    )\n\n    if objectives is None:\n        self.objectives = Objectives([Minimize(m) for m in self.outputs.names])\n    elif isinstance(objectives, Objectives):\n        self.objectives = objectives\n    else:\n        self.objectives = Objectives(objectives)\n\n    if isinstance(constraints, Constraints):\n        pass\n    elif not constraints:\n        constraints = None\n    else:\n        constraints = Constraints(constraints)\n        if len(constraints) == 0:  # no valid constraints\n            constraints = None\n    self.constraints = constraints\n\n    if isinstance(output_constraints, Objectives) or output_constraints is None:\n        self.output_constraints = output_constraints\n    else:\n        self.output_constraints = Objectives(output_constraints)\n\n    if isinstance(models, Models) or models is None:\n        self.models = models\n    else:\n        self.models = Models(models)\n\n    if f is not None:\n        self.f = f\n\n    if isinstance(data, dict):\n        data = pd.read_json(json.dumps(data), orient=\"split\")\n\n    if isinstance(optima, dict):\n        optima = pd.read_json(json.dumps(optima), orient=\"split\")\n\n    self.set_data(data)\n    self.set_optima(optima)\n    self.check_problem()\n    self.check_models()\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.add_data","title":"<code>add_data(self, data)</code>","text":"<p>Add a number of data points.</p> Source code in <code>opti/problem.py</code> <pre><code>def add_data(self, data: pd.DataFrame) -&gt; None:\n\"\"\"Add a number of data points.\"\"\"\n    self.check_data(data)\n    self.data = pd.concat([self.data, data], axis=0)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.check_data","title":"<code>check_data(self, data)</code>","text":"<p>Check if data is consistent with input and output parameters.</p> Source code in <code>opti/problem.py</code> <pre><code>def check_data(self, data: pd.DataFrame) -&gt; None:\n\"\"\"Check if data is consistent with input and output parameters.\"\"\"\n    for p in self.inputs + self.outputs:\n        # data must contain all parameters\n        if p.name not in data.columns:\n            raise ValueError(\n                f\"Parameter {p.name} is missing. Data must contain all parameters.\"\n            )\n\n        # data for continuous / discrete parameters must be numeric\n        if isinstance(p, (Continuous, Discrete)):\n            ok = is_numeric_dtype(data[p.name]) or data[p.name].isnull().all()\n            if not ok:\n                raise ValueError(\n                    f\"Parameter {p.name} contains non-numeric values. Data for continuous / discrete parameters must be numeric.\"\n                )\n\n        # categorical levels in data must be specified\n        elif isinstance(p, Categorical):\n            ok = p.contains(data[p.name]) | data[p.name].isna()\n            if not ok.all():\n                unknowns = data[p.name][~ok].unique().tolist()\n                raise ValueError(\n                    f\"Data for parameter {p.name} contains unknown values: {unknowns}. All categorical levels must be specified.\"\n                )\n\n    # inputs must be complete\n    for p in self.inputs:\n        if data[p.name].isnull().any():\n            raise ValueError(\n                f\"Input parameter {p.name} has missing data. Inputs must be complete.\"\n            )\n\n    # outputs must have at least one observation\n    for p in self.outputs:\n        if data[p.name].isnull().all():\n            raise ValueError(\n                f\"Output parameter {p.name} has no data. Outputs must have at least one observation.\"\n            )\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.check_models","title":"<code>check_models(self)</code>","text":"<p>Check if the models are well defined</p> Source code in <code>opti/problem.py</code> <pre><code>def check_models(self) -&gt; None:\n\"\"\"Check if the models are well defined\"\"\"\n    if self.models is None:\n        return\n\n    for model in self.models:\n        # models need to refer to output parameters\n        for n in model.names:\n            if n not in self.outputs.names:\n                raise ValueError(f\"Model {model} refers to unknown outputs\")\n\n        if isinstance(model, LinearModel):\n            if len(model.coefficients) != self.n_inputs:\n                raise ValueError(f\"Model {model} has wrong number of coefficients.\")\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.check_problem","title":"<code>check_problem(self)</code>","text":"<p>Check if input and output parameters are consistent.</p> Source code in <code>opti/problem.py</code> <pre><code>def check_problem(self) -&gt; None:\n\"\"\"Check if input and output parameters are consistent.\"\"\"\n    # check for duplicate names\n    duplicates = set(self.inputs.names).intersection(self.outputs.names)\n    if duplicates:\n        raise ValueError(f\"Parameter name in both inputs and outputs: {duplicates}\")\n\n    # check if all objectives refer to an output\n    for obj in self.objectives:\n        if obj.name not in self.outputs.names:\n            raise ValueError(f\"Objective refers to unknown parameter: {obj.name}\")\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.create_initial_data","title":"<code>create_initial_data(self, n_samples=10)</code>","text":"<p>Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.</p> Source code in <code>opti/problem.py</code> <pre><code>def create_initial_data(self, n_samples: int = 10) -&gt; None:\n\"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\"\n    if self.f is None:\n        raise NotImplementedError(\"problem.f is not implemented for the problem.\")\n    X = self.sample_inputs(n_samples)\n    Y = self.f(X)\n    self.data = pd.concat([X, Y], axis=1)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.from_config","title":"<code>from_config(config)</code>  <code>staticmethod</code>","text":"<p>Create a Problem instance from a configuration dict.</p> Source code in <code>opti/problem.py</code> <pre><code>@staticmethod\ndef from_config(config: dict) -&gt; \"Problem\":\n\"\"\"Create a Problem instance from a configuration dict.\"\"\"\n    return Problem(**config)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.from_json","title":"<code>from_json(fname)</code>  <code>staticmethod</code>","text":"<p>Read a problem from a JSON file.</p> Source code in <code>opti/problem.py</code> <pre><code>@staticmethod\ndef from_json(fname: PathLike) -&gt; \"Problem\":\n\"\"\"Read a problem from a JSON file.\"\"\"\n    with open(fname, \"rb\") as infile:\n        config = json.loads(infile.read())\n    return Problem(**config)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_X","title":"<code>get_X(self, data=None)</code>","text":"<p>Return the input values in <code>data</code> or <code>self.data</code>.</p> Source code in <code>opti/problem.py</code> <pre><code>def get_X(self, data: Optional[pd.DataFrame] = None) -&gt; np.ndarray:\n\"\"\"Return the input values in `data` or `self.data`.\"\"\"\n    if data is not None:\n        return data[self.inputs.names].values\n    return self.get_data()[self.inputs.names].values\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_XY","title":"<code>get_XY(self, outputs=None, data=None, continuous='none', discrete='none', categorical='none')</code>","text":"<p>Return the input and output values as numeric numpy arrays.</p> <p>Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>optional</code> <p>Subset of the outputs to consider.</p> <code>None</code> <code>data</code> <code>optional</code> <p>Dataframe to consider instead of problem.data</p> <code>None</code> Source code in <code>opti/problem.py</code> <pre><code>def get_XY(\n    self,\n    outputs: Optional[List[str]] = None,\n    data: Optional[pd.DataFrame] = None,\n    continuous: str = \"none\",\n    discrete: str = \"none\",\n    categorical: str = \"none\",\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Return the input and output values as numeric numpy arrays.\n\n    Rows with missing output values will be dropped.\n    Input values are assumed to be complete.\n    Categorical outputs are one-hot or label encoded.\n\n    Args:\n        outputs (optional): Subset of the outputs to consider.\n        data (optional): Dataframe to consider instead of problem.data\n    \"\"\"\n    if outputs is None:\n        outputs = self.outputs.names\n    if data is None:\n        data = self.get_data()\n    notna = data[outputs].notna().all(axis=1)\n    X = self.inputs.transform(\n        data, continuous=continuous, discrete=discrete, categorical=categorical\n    )[notna].values\n    Y = data[outputs][notna].values\n    return X, Y\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_X_bounds","title":"<code>get_X_bounds(self)</code>","text":"<p>Return the lower and upper data bounds.</p> Source code in <code>opti/problem.py</code> <pre><code>def get_X_bounds(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"Return the lower and upper data bounds.\"\"\"\n    X = self.get_X()\n    xlo = X.min(axis=0)\n    xhi = X.max(axis=0)\n    b = xlo == xhi\n    xhi[b] = xlo[b] + 1  # prevent division by zero when dividing by (xhi - xlo)\n    return xlo, xhi\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_Y","title":"<code>get_Y(self, data=None)</code>","text":"<p>Return the output values in <code>data</code> or <code>self.data</code>.</p> Source code in <code>opti/problem.py</code> <pre><code>def get_Y(self, data: Optional[pd.DataFrame] = None) -&gt; np.ndarray:\n\"\"\"Return the output values in `data` or `self.data`.\"\"\"\n    if data is not None:\n        return data[self.outputs.names].values\n    return self.get_data()[self.outputs.names].values\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_data","title":"<code>get_data(self)</code>","text":"<p>Return <code>self.data</code> if it exists or an empty dataframe.</p> Source code in <code>opti/problem.py</code> <pre><code>def get_data(self) -&gt; pd.DataFrame:\n\"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\"\n    if self.data is None:\n        return pd.DataFrame(columns=self.inputs.names + self.outputs.names)\n    return self.data\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.sample_inputs","title":"<code>sample_inputs(self, n_samples=10)</code>","text":"<p>Uniformly sample points from the input space subject to the constraints.</p> Source code in <code>opti/problem.py</code> <pre><code>def sample_inputs(self, n_samples=10) -&gt; pd.DataFrame:\n\"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\"\n    if self.constraints is None:\n        return sobol_sampling(n_samples, self.inputs)\n    return constrained_sampling(n_samples, self.inputs, self.constraints)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.set_data","title":"<code>set_data(self, data)</code>","text":"<p>Set the data.</p> Source code in <code>opti/problem.py</code> <pre><code>def set_data(self, data: Optional[pd.DataFrame]) -&gt; None:\n\"\"\"Set the data.\"\"\"\n    if data is not None:\n        for p in self.inputs:\n            # Categorical levels are required to be strings. Ensure that the corresponding data is as well.\n            if isinstance(p, Categorical):\n                nulls = data[p.name].isna()\n                data[p.name] = data[p.name].astype(str).mask(nulls, np.nan)\n\n        self.check_data(data)\n\n    self.data = data\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.set_optima","title":"<code>set_optima(self, optima)</code>","text":"<p>Set the optima / Pareto front.</p> Source code in <code>opti/problem.py</code> <pre><code>def set_optima(self, optima: Optional[pd.DataFrame]) -&gt; None:\n\"\"\"Set the optima / Pareto front.\"\"\"\n    if optima is not None:\n        self.check_data(optima)\n    self.optima = optima\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.to_config","title":"<code>to_config(self)</code>","text":"<p>Return json-serializable configuration dict.</p> Source code in <code>opti/problem.py</code> <pre><code>def to_config(self) -&gt; dict:\n\"\"\"Return json-serializable configuration dict.\"\"\"\n\n    config = {\n        \"name\": self.name,\n        \"inputs\": self.inputs.to_config(),\n        \"outputs\": self.outputs.to_config(),\n        \"objectives\": self.objectives.to_config(),\n    }\n    if self.output_constraints is not None:\n        config[\"output_constraints\"] = self.output_constraints.to_config()\n    if self.constraints is not None:\n        config[\"constraints\"] = self.constraints.to_config()\n    if self.models is not None:\n        config[\"models\"] = self.models.to_config()\n    if self.data is not None:\n        config[\"data\"] = self.data.replace({np.nan: None}).to_dict(\"split\")\n    if self.optima is not None:\n        config[\"optima\"] = self.optima.replace({np.nan: None}).to_dict(\"split\")\n    return config\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.to_json","title":"<code>to_json(self, fname)</code>","text":"<p>Save a problem from a JSON file.</p> Source code in <code>opti/problem.py</code> <pre><code>def to_json(self, fname: PathLike) -&gt; None:\n\"\"\"Save a problem from a JSON file.\"\"\"\n    with open(fname, \"wb\") as outfile:\n        b = json.dumps(self.to_config(), ensure_ascii=False, separators=(\",\", \":\"))\n        outfile.write(b.encode(\"utf-8\"))\n</code></pre>"},{"location":"ref-problem/#opti.problem.read_json","title":"<code>read_json(filepath)</code>","text":"<p>Read a problem specification from a JSON file.</p> Source code in <code>opti/problem.py</code> <pre><code>def read_json(filepath: PathLike) -&gt; Problem:\n\"\"\"Read a problem specification from a JSON file.\"\"\"\n    return Problem.from_json(filepath)\n</code></pre>"},{"location":"ref-problems/","title":"Test Problems","text":""},{"location":"ref-problems/#opti.problems.cbo_benchmarks","title":"<code>cbo_benchmarks</code>","text":"<p>Constrained, single-objective benchmark problems from the Bayesian optimization literature.</p>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.G10","title":"<code> G10            (Problem)         </code>","text":"<p>G10 test problem: 8 inputs, 6 constraints.</p> <p>From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class G10(Problem):\n\"\"\"G10 test problem: 8 inputs, 6 constraints.\n\n    From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"G10 (d=8, p=6)\",\n            inputs=[\n                Continuous(\"x1\", domain=[100.0, 10000.0]),\n                Continuous(\"x2\", domain=[1000.0, 10000.0]),\n                Continuous(\"x3\", domain=[1000.0, 10000.0]),\n                Continuous(\"x4\", domain=[10.0, 1000.0]),\n                Continuous(\"x5\", domain=[10.0, 1000.0]),\n                Continuous(\"x6\", domain=[10.0, 1000.0]),\n                Continuous(\"x7\", domain=[10.0, 1000.0]),\n                Continuous(\"x8\", domain=[10.0, 1000.0]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                LinearInequality([\"x4\", \"x6\"], lhs=[0.0025, 0.0025], rhs=1.0),\n                LinearInequality(\n                    [\"x4\", \"x5\", \"x7\"], lhs=[-0.0025, 0.0025, 0.0025], rhs=1.0\n                ),\n                LinearInequality([\"x5\", \"x8\"], lhs=[-0.01, 0.01], rhs=1.0),\n                NonlinearInequality(\n                    \"100.0 * x1 - x1 * x6 + 833.33252 * x4 - 83333.333\"\n                ),\n                NonlinearInequality(\"x2 * x4 - x2 * x7 - 1250.0 * x4 + 1250.0 * x5\"),\n                NonlinearInequality(\"x3 * x5 - x3 * x8 - 2500.0 * x5 + 1250000.0\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\"x1 + x2 + x3\"),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array(\n            [\n                [\n                    579.3167,\n                    1359.943,\n                    5110.071,\n                    182.0174,\n                    295.5985,\n                    217.9799,\n                    286.4162,\n                    395.5979,\n                ]\n            ]\n        )\n        y = np.array([[7049.3307]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.G4","title":"<code> G4            (Problem)         </code>","text":"<p>G4 test problme: 5 inputs, 6 constraints.</p> <p>From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class G4(Problem):\n\"\"\"G4 test problme: 5 inputs, 6 constraints.\n\n    From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"G4 (d=5, p=6)\",\n            inputs=[\n                Continuous(\"x1\", domain=[78.0, 102.0]),\n                Continuous(\"x2\", domain=[33.0, 45.0]),\n                Continuous(\"x3\", domain=[27.0, 45.0]),\n                Continuous(\"x4\", domain=[27.0, 45.0]),\n                Continuous(\"x5\", domain=[27.0, 45.0]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\n                    \"(85.334407 + 0.0056858 * x2 * x5 + 0.0006262 * x1 * x4 - 0.0022053 * x3 * x5) - 92.0\"\n                ),\n                NonlinearInequality(\n                    \"- (85.334407 + 0.0056858 * x2 * x5 + 0.0006262 * x1 * x4 - 0.0022053 * x3 * x5)\"\n                ),\n                NonlinearInequality(\n                    \"(80.51249 + 0.0071317 * x2 * x5 + 0.0029955 * x1 * x2 + 0.0021813 * x3**2) - 110.0\"\n                ),\n                NonlinearInequality(\n                    \"90.0 - (80.51249 + 0.0071317 * x2 * x5 + 0.0029955 * x1 * x2 + 0.0021813 * x3**2)\"\n                ),\n                NonlinearInequality(\n                    \"(9.300961 + 0.0047026 * x3 * x5 + 0.0012547 * x1 * x3 + 0.0019085 * x3 * x4) - 25.0\"\n                ),\n                NonlinearInequality(\n                    \"20.0 - (9.300961 + 0.0047026 * x3 * x5 + 0.0012547 * x1 * x3 + 0.0019085 * x3 * x4)\"\n                ),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\n                    \"5.3578547 * x3**2 + 0.8356891 * x1 * x5 + 37.293239 * x1 - 40792.141\"\n                ),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[78.0, 33.0, 29.995256025682, 45.0, 36.775812905788]])\n        y = np.array([[-30665.539]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.G6","title":"<code> G6            (Problem)         </code>","text":"<p>G6 test problem: 2 inputs, 2 constraints.</p> <p>From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class G6(Problem):\n\"\"\"G6 test problem: 2 inputs, 2 constraints.\n\n    From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"G6 (d=2, p=2)\",\n            inputs=[\n                Continuous(\"x1\", domain=[13.5, 14.5]),\n                Continuous(\"x2\", domain=[0.5, 1.5]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\"-(x1 - 5.0)**2 - (x2 - 5.0)**2 + 100.0\"),\n                NonlinearInequality(\"(x1 - 6.0)**2 + (x2 - 5.0)**2 - 82.81\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\"(x1 - 10.0)**3 + (x2 - 20.0)**3\"),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[14.095, 0.84296]])\n        y = np.array([[-6961.81388]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.G7","title":"<code> G7            (Problem)         </code>","text":"<p>G7 test problem: 10 inputs, 8 constraints.</p> <p>From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class G7(Problem):\n\"\"\"G7 test problem: 10 inputs, 8 constraints.\n\n    From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"G7 (d=10, p=8)\",\n            inputs=[Continuous(f\"x{i+1}\", domain=[-10.0, 10.0]) for i in range(10)],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                LinearInequality(\n                    [\"x1\", \"x2\", \"x7\", \"x8\"], lhs=[4.0, 5.0, -3.0, 9.0], rhs=105.0\n                ),\n                LinearInequality(\n                    [\"x1\", \"x2\", \"x7\", \"x8\"], lhs=[10.0, -8.0, -17.0, 2.0], rhs=0\n                ),\n                LinearInequality(\n                    [\"x1\", \"x2\", \"x9\", \"x10\"], lhs=[-8.0, 2.0, 5.0, -2.0], rhs=12.0\n                ),\n                NonlinearInequality(\n                    \"3.0 * (x1 - 2.0)**2 + 4.0 * (x2 - 3.0)**2 + 2.0 * x3**2 - 7.0 * x4 - 120.0\"\n                ),\n                NonlinearInequality(\n                    \"5.0 * x1**2 + 8.0 * x2 + (x3 - 6.0)**2 - 2.0 * x4 - 40.0\"\n                ),\n                NonlinearInequality(\n                    \"0.5 * (x1 - 8.0)**2 + 2.0 * (x2 - 4.0)**2 + 3.0 * x5**2 - x6 - 30.0\"\n                ),\n                NonlinearInequality(\n                    \"x1**2 + 2.0 * (x2 - 2.0)**2 - 2.0 * x1 * x2 + 14.0 * x5 - 6.0 * x6\"\n                ),\n                NonlinearInequality(\n                    \"- 3.0 * x1 + 6.0 * x2 + 12.0 * (x9 - 8.0)**2 - 7.0 * x10\"\n                ),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\n                    \"x1**2 + x2**2 + x1 * x2 - 14.0 * x1 - 16.0 * x2 + (x3 - 10.0)**2 + 4.0 * (x4 - 5.0)**2 + (x5 - 3.0)**2 + 2.0 * (x6 - 1.0)**2 + 5.0 * x7**2 + 7.0 * (x8 - 11.0)**2 + 2.0 * (x9 - 10.0)**2 + (x10 - 7.0)**2 + 45.0\"\n                ),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array(\n            [\n                [\n                    2.171996,\n                    2.363683,\n                    8.773926,\n                    5.095984,\n                    0.9906548,\n                    1.430574,\n                    1.321644,\n                    9.828726,\n                    8.280092,\n                    8.375927,\n                ]\n            ]\n        )\n        y = np.array([[24.3062091]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.G8","title":"<code> G8            (Problem)         </code>","text":"<p>G8 test problem: 2 inputs, 2 constraints</p> <p>From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class G8(Problem):\n\"\"\"G8 test problem: 2 inputs, 2 constraints\n\n    From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"G8 (d=2, p=2)\",\n            inputs=[\n                Continuous(\"x1\", domain=[0.5, 10.0]),\n                Continuous(\"x2\", domain=[0.5, 10.0]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\"x1**2 - x2 + 1.0\"),\n                NonlinearInequality(\"1.0 - x1 + (x2 - 4.0)**2\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\n                    \"- (sin(2.0 * arccos(-1.0) * x1)**3 * sin(2.0 * arccos(-1.0) * x2)) / (x1**3 * (x1 + x2))\"\n                ),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[1.2279713, 4.2453733]])\n        y = np.array([[-0.095825]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.G9","title":"<code> G9            (Problem)         </code>","text":"<p>G9 test problem: 7 inputs, 4 constraints.</p> <p>From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class G9(Problem):\n\"\"\"G9 test problem: 7 inputs, 4 constraints.\n\n    From Michalewicz and Schoenauer 1996. https://ieeexplore.ieee.org/document/6791784\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"G9 (d=7, p=4)\",\n            inputs=[Continuous(f\"x{i+1}\", domain=[-10.0, 10.0]) for i in range(7)],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\n                    \"2.0 * x1**2 + 3.0 * x2**4 + x3 + 4.0 * x4**2 + 5.0 * x5 - 127.0\"\n                ),\n                NonlinearInequality(\n                    \"7.0 * x1 + 3.0 * x2 + 10.0 * x3**2 + x4 - x5 - 282.0\"\n                ),\n                NonlinearInequality(\n                    \"23.0 * x1 + x2**2 + 6.0 * x6**2 - 8.0 * x7 - 196.0\"\n                ),\n                NonlinearInequality(\n                    \"4.0 * x1**2 + x2**2 - 3.0 * x1 * x2 + 2.0 * x3**2 + 5.0 * x6 - 11.0 * x7\"\n                ),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\n                    \"(x1 - 10.0)**2 + 5.0 * (x2 - 12.0)**2 + x3**4 + 3.0 * (x4 - 11.0)**2 + 10.0 * x5**6 + 7.0 * x6**2 + x7**4 - 4.0 * x6 * x7 - 10.0 * x6 - 8.0 * x7\"\n                ),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array(\n            [[2.330499, 1.951372, -0.4775414, 4.365726, -0.6244870, 1.038131, 1.594227]]\n        )\n        y = np.array([[680.6300573]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.Gardner","title":"<code> Gardner            (Problem)         </code>","text":"<p>Gardner test problem: 2 inputs, 1 constraint.</p> <p>From Gardner et al. 2014. http://proceedings.mlr.press/v32/gardner14.pdf</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class Gardner(Problem):\n\"\"\"Gardner test problem: 2 inputs, 1 constraint.\n\n    From Gardner et al. 2014. http://proceedings.mlr.press/v32/gardner14.pdf\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Gardner (d=2, p=1)\",\n            inputs=[\n                Continuous(\"x1\", domain=[0, 2.0 * np.pi]),\n                Continuous(\"x2\", domain=[0, 2.0 * np.pi]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[NonlinearInequality(\"sin(x1) * sin(x2) + 0.95\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\"sin(x1) + x2\"),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[1.5 * np.pi, np.arcsin(0.95)]])\n        y = np.array([[np.arcsin(0.95) - 1.0]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.Gramacy","title":"<code> Gramacy            (Problem)         </code>","text":"<p>Gramacy test problem: 2 inputs, 2 constraints</p> <p>From Gramacy et al. 2016. https://arxiv.org/pdf/1403.4890.pdf</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class Gramacy(Problem):\n\"\"\"Gramacy test problem: 2 inputs, 2 constraints\n\n    From Gramacy et al. 2016. https://arxiv.org/pdf/1403.4890.pdf\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Gramacy (d=2, p=2)\",\n            inputs=[\n                Continuous(\"x1\", domain=[0, 1.0]),\n                Continuous(\"x2\", domain=[0, 1.0]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\n                    \"1.5 - x1 - 2.0 * x2 - 0.5 * sin(2.0 * arccos(-1.0) * (x1**2 - 2.0 * x2))\"\n                ),\n                NonlinearInequality(\"x1**2 + x2**2 - 1.5\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\"x1 + x2\"),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[0.1954, 0.4044]])\n        y = np.array([[0.5998]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.PressureVessel","title":"<code> PressureVessel            (Problem)         </code>","text":"<p>Pressure vessel test problem: 4 inputs, 3 constraints.</p> <p>The 4D pressure vessel design problem  aims to minimize the cost of designing a cylindrical vessel subject to 4 constraints. The original problem did not give the bounds for design variables. Here we use the bounds in [1] that contains the best known solution found in [2]. Note that the fourth constraint always holds after bounding the input variables. Therefore, we remove it.</p> <p>[1] Eriksson and Poloczek 2021. http://proceedings.mlr.press/v130/eriksson21a/eriksson21a.pdf [2] Coello and Mezura-Montes 2002. https://link.springer.com/chapter/10.1007/978-0-85729-345-9_23</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class PressureVessel(Problem):\n\"\"\"Pressure vessel test problem: 4 inputs, 3 constraints.\n\n    The 4D pressure vessel design problem  aims to minimize the cost of designing a\n    cylindrical vessel subject to 4 constraints. The original problem did not give the\n    bounds for design variables. Here we use the bounds in [1] that contains the best\n    known solution found in [2]. Note that the fourth constraint always holds after\n    bounding the input variables. Therefore, we remove it.\n\n    [1] Eriksson and Poloczek 2021. http://proceedings.mlr.press/v130/eriksson21a/eriksson21a.pdf\n    [2] Coello and Mezura-Montes 2002. https://link.springer.com/chapter/10.1007/978-0-85729-345-9_23\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Pressure vessel (d=4, p=3)\",\n            inputs=[\n                Continuous(\"x1\", domain=[0, 10.0]),\n                Continuous(\"x2\", domain=[0, 10.0]),\n                Continuous(\"x3\", domain=[10.0, 50.0]),\n                Continuous(\"x4\", domain=[150.0, 200.0]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\"- x1 + 0.0193 * x3\"),\n                NonlinearInequality(\"- x2+ 0.00954 * x3\"),\n                NonlinearInequality(\n                    \"- arccos(-1.0) * x3**2 * x4 - 4.0 * arccos(-1.0) / 3.0 * x3**3 + 1296000.0\"\n                ),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\n                    \"0.6224 * x1 * x3 * x4 + 1.7781 * x2 * x3**2 + 3.1661 * x1**2 * x4 + 19.84 * x1**2 * x3\"\n                ),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[0.8125, 0.4375, 42.0984, 176.6368]])\n        y = np.array([[6059.715]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.Sasena","title":"<code> Sasena            (Problem)         </code>","text":"<p>Sasena test problem: 2 inputs, 3 constraints.</p> <p>From Sasena's PhD thesis 2002. https://www.mat.univie.ac.at/~neum/glopt/mss/Sas02.pdf</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class Sasena(Problem):\n\"\"\"Sasena test problem: 2 inputs, 3 constraints.\n\n    From Sasena's PhD thesis 2002. https://www.mat.univie.ac.at/~neum/glopt/mss/Sas02.pdf\n    \"\"\"\n\n    #\n    def __init__(self):\n        super().__init__(\n            name=\"Sasena (d=2, p=3)\",\n            inputs=[\n                Continuous(\"x1\", domain=[0, 1.0]),\n                Continuous(\"x2\", domain=[0, 1.0]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\n                    \"((x1 - 3.0)**2 + (x2 + 2.0)**2) * exp(- x2**7) - 12.0\"\n                ),\n                LinearInequality([\"x1\", \"x2\"], lhs=[10.0, 1.0], rhs=7.0),\n                NonlinearInequality(\"(x1 - 0.5)**2 + (x2 - 0.5)**2 - 0.2\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\"-(x1 - 1.0)**2 - (x2 - 0.5)**2\"),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[0.2017, 0.8332]])\n        y = np.array([[-0.7483]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.SpeedReducer","title":"<code> SpeedReducer            (Problem)         </code>","text":"<p>Speed reducer test problem: 7 inputs, 11 constraints.</p> <p>The goal of 7D speed reducer problem is to minimize the weight of a speed reducer under 11 mechanical constraints. The third variable is a category variable. However, regarding it as a continuous variable does not change the optimum.</p> <p>From Coello and Mezura-Montes 2002. https://link.springer.com/chapter/10.1007/978-0-85729-345-9_23</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class SpeedReducer(Problem):\n\"\"\"Speed reducer test problem: 7 inputs, 11 constraints.\n\n    The goal of 7D speed reducer problem is to minimize the weight of a speed reducer\n    under 11 mechanical constraints. The third variable is a category variable.\n    However, regarding it as a continuous variable does not change the optimum.\n\n    From Coello and Mezura-Montes 2002. https://link.springer.com/chapter/10.1007/978-0-85729-345-9_23\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Speed reducer (d=7, p=11)\",\n            inputs=[\n                Continuous(\"x1\", domain=[2.6, 3.6]),\n                Continuous(\"x2\", domain=[0.7, 0.8]),\n                Continuous(\"x3\", domain=[17.0, 28.0]),\n                Continuous(\"x4\", domain=[7.3, 8.3]),\n                Continuous(\"x5\", domain=[7.8, 8.3]),\n                Continuous(\"x6\", domain=[2.9, 3.9]),\n                Continuous(\"x7\", domain=[4.9, 5.9]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\"27.0 / (x1 * x2**2 * x3) - 1.0\"),\n                NonlinearInequality(\"397.5 / (x1 * x2**2 * x3**2) - 1.0\"),\n                NonlinearInequality(\"1.93 * x4**3 / (x2 * x3 * x6**4) - 1.0\"),\n                NonlinearInequality(\"1.93 * x5**3 / (x2 * x3 * x7**4) - 1.0\"),\n                NonlinearInequality(\n                    \"((745.0 * x4 / (x2 * x3))**2 + 16900000.0)**0.5 / x6**3 - 110.0\"\n                ),\n                NonlinearInequality(\n                    \"((745.0 * x5 / (x2 * x3))**2 + 157500000.0)**0.5 / x7**3 - 85.0\"\n                ),\n                NonlinearInequality(\"x2 * x3 - 40.0\"),\n                NonlinearInequality(\"- x1 / x2 + 5.0\"),\n                NonlinearInequality(\"x1 / x2 - 12.0\"),\n                NonlinearInequality(\"(1.5 * x6 + 1.9) / x4 - 1.0\"),\n                NonlinearInequality(\"(1.1 * x7 + 1.9) / x5 - 1.0\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\n                    \"0.7854 * x1 * x2**2 * (3.3333 * x3**2 + 14.9334 * x3 - 43.0934) - 1.508 * x1 * (x6**2 + x7**2) + 7.4777 * (x6**3 + x7**3) + 0.7854 * (x4 * x6**2 + x5 * x7**2)\"\n                ),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[3.5, 0.7, 17.0, 7.3, 7.8, 3.350215, 5.286683]])\n        y = np.array([[2996.3482]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.TensionCompression","title":"<code> TensionCompression            (Problem)         </code>","text":"<p>Tension compression test problem: 3 inputs, 4 constraints.</p> <p>The 3D tension-compression string design problem aims to minimize the weight of a tension/compression spring under 4 mechanical constraints.</p> <p>From Coello and Mezura-Montes 2002. https://link.springer.com/chapter/10.1007/978-0-85729-345-9_23</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class TensionCompression(Problem):\n\"\"\"Tension compression test problem: 3 inputs, 4 constraints.\n\n    The 3D tension-compression string design problem aims to minimize the weight of a\n    tension/compression spring under 4 mechanical constraints.\n\n    From Coello and Mezura-Montes 2002. https://link.springer.com/chapter/10.1007/978-0-85729-345-9_23\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Tension compression\",\n            inputs=[\n                Continuous(\"x1\", domain=[2.0, 15.0]),\n                Continuous(\"x2\", domain=[0.25, 1.3]),\n                Continuous(\"x3\", domain=[0.05, 2.0]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\"1.0 - x2**3 * x1 / (71785.0 * x3**4)\"),\n                NonlinearInequality(\n                    \"(4.0 * x2**2 - x2 * x3) / (12566.0 * x3**3 * (x2 - x3)) + 1.0 / (5108.0 * x3**2) - 1.0\"\n                ),\n                NonlinearInequality(\"1.0 - 140.45 * x3 / (x1 * x2**2)\"),\n                NonlinearInequality(\"(x2 + x3) / 1.5 - 1.0\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\"x3**2 * x2 * (x1 + 2.0)\"),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[11.21390736278739, 0.35800478345599, 0.05174250340926]])\n        y = np.array([[0.012666]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.cbo_benchmarks.WeldedBeam1","title":"<code> WeldedBeam1            (Problem)         </code>","text":"<p>Welded beam test problem: 1 output, 4 inputs, 5 constraints.</p> <p>In this problem the cost of a welded beam is minimized subject to 5 constraints. The original version of this problem in [1] had 7 constraints, 2 of which could be shrunk into the bounds. Here we use the formulations in [2].</p> <p>[1] Coello and Mezura-Montes 2002. https://link.springer.com/chapter/10.1007/978-0-85729-345-9_23 [2] Hedar and Fukushima 2006. https://link.springer.com/article/10.1007/s10898-005-3693-z</p> Source code in <code>opti/problems/cbo_benchmarks.py</code> <pre><code>class WeldedBeam1(Problem):\n\"\"\"Welded beam test problem: 1 output, 4 inputs, 5 constraints.\n\n    In this problem the cost of a welded beam is minimized subject to 5 constraints.\n    The original version of this problem in [1] had 7 constraints, 2 of which could be\n    shrunk into the bounds. Here we use the formulations in [2].\n\n    [1] Coello and Mezura-Montes 2002. https://link.springer.com/chapter/10.1007/978-0-85729-345-9_23\n    [2] Hedar and Fukushima 2006. https://link.springer.com/article/10.1007/s10898-005-3693-z\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Single-objective welded beam problem (d=4, p=5)\",\n            inputs=[\n                Continuous(\"x1\", domain=[0.125, 10.0]),\n                Continuous(\"x2\", domain=[0.1, 10.0]),\n                Continuous(\"x3\", domain=[0.1, 10.0]),\n                Continuous(\"x4\", domain=[0.1, 10.0]),\n            ],\n            outputs=[Continuous(\"y0\")],\n            objectives=[Minimize(\"y0\")],\n            constraints=[\n                NonlinearInequality(\n                    \"((6000.0 / (2.0**0.5 * x1 * x2))**2 + ((6000. * (14.0 + 0.5 * x2) * (0.25 * (x2**2 + (x1 + x3)**2))**0.5) / (2.0 * (0.707 * x1 * x2 * (x2**2 / 12.0 + 0.25 * (x1 + x3)**2))))**2 + x2 * (6000.0 / (2.0**0.5 * x1 * x2)) * ((6000. * (14.0 + 0.5 * x2) * (0.25 * (x2**2 + (x1 + x3)**2))**0.5) / (2.0 * (0.707 * x1 * x2 * (x2**2 / 12.0 + 0.25 * (x1 + x3)**2)))) / (0.25 * (x2**2 + (x1 + x3)**2))**0.5) ** 0.5 - 13000.0\"\n                ),\n                NonlinearInequality(\"504000.0 / (x3**2 * x4) - 30000.0\"),\n                LinearInequality([\"x1\", \"x4\"], lhs=[1.0, -1.0], rhs=0),\n                NonlinearInequality(\n                    \"6000.0 - 64746.022 * (1.0 - 0.0282346 * x3) * x3 * x4**3\"\n                ),\n                NonlinearInequality(\"2.1952 / (x3**3 * x4) - 0.25\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y0\": X.eval(\"1.10471 * x1**2 * x2 + 0.04811 * x3 * x4 * (14.0 + x2)\"),\n            },\n            index=X.index,\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array([[0.24435257, 6.2157922, 8.2939046, 0.24435257]])\n        y = np.array([[2.381065]])\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets","title":"<code>datasets</code>","text":"<p>Chemical datasets. These problems contain observed data but don't come with a ground truth.</p>"},{"location":"ref-problems/#opti.problems.datasets.Alkox","title":"<code> Alkox            (Problem)         </code>","text":"<p>Alkoxylation dataset</p> <p>This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx). The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective.</p> <p>Reference</p> <p>F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI. Obtained from Olympus.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class Alkox(Problem):\n\"\"\"Alkoxylation dataset\n\n    This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx).\n    The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed.\n    The dataset includes 104 samples with four parameters and one objective.\n\n    Reference:\n        F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153.\n        [DOI](https://doi.org/10.1088/2632-2153/abedc8).\n        Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Alkox\",\n            inputs=[\n                Continuous(\"residence_time\", domain=[0.05, 1]),\n                Continuous(\"ratio\", domain=[0.5, 10]),\n                Continuous(\"concentration\", domain=[2, 8]),\n                Continuous(\"temperature\", domain=[6, 8]),\n            ],\n            outputs=[Continuous(\"conversion\")],\n            objectives=[Maximize(\"conversion\")],\n            data=get_data(\"alkox.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.BaumgartnerAniline","title":"<code> BaumgartnerAniline            (Problem)         </code>","text":"<p>Aniline C-N cross-coupling dataset.</p> <p>Reference</p> <p>Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI. Data obtained from Summit.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class BaumgartnerAniline(Problem):\n\"\"\"Aniline C-N cross-coupling dataset.\n\n    Reference:\n        Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases\n        [DOI](https://doi.org/10.1021/acs.oprd.9b00236).\n        Data obtained from [Summit](https://github.com/sustainable-processes/summit).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Aniline cross-coupling, Baumgartner 2019\",\n            inputs=[\n                Categorical(\"catalyst\", domain=[\"tBuXPhos\", \"tBuBrettPhos\", \"AlPhos\"]),\n                Categorical(\"base\", domain=[\"TEA\", \"TMG\", \"BTMG\", \"DBU\"]),\n                Continuous(\"base_equivalents\", domain=[1.0, 2.5]),\n                Continuous(\"temperature\", domain=[30, 100]),\n                Continuous(\"residence_time\", domain=[60, 1800]),\n            ],\n            outputs=[Continuous(\"yield\", domain=[0, 1])],\n            objectives=[Maximize(\"yield\")],\n            data=get_data(\"baumgartner_aniline.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.BaumgartnerBenzamide","title":"<code> BaumgartnerBenzamide            (Problem)         </code>","text":"<p>Benzamide C-N cross-coupling dataset.</p> <p>Reference</p> <p>Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI. Data obtained from Summit.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class BaumgartnerBenzamide(Problem):\n\"\"\"Benzamide C-N cross-coupling dataset.\n\n    Reference:\n        Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases\n        [DOI](https://doi.org/10.1021/acs.oprd.9b00236).\n        Data obtained from [Summit](https://github.com/sustainable-processes/summit).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Benzamide cross-coupling, Baumgartner 2019\",\n            inputs=[\n                Categorical(\"catalyst\", domain=[\"tBuXPhos\", \"tBuBrettPhos\"]),\n                Categorical(\"base\", domain=[\"TMG\", \"BTMG\", \"DBU\", \"MTBD\"]),\n                Continuous(\"base_equivalents\", domain=[1.0, 2.1]),\n                Continuous(\"temperature\", domain=[30, 100]),\n                Continuous(\"residence_time\", domain=[60, 1850]),\n            ],\n            outputs=[Continuous(\"yield\", domain=[0, 1])],\n            objectives=[Maximize(\"yield\")],\n            data=get_data(\"baumgartner_benzamide.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.Benzylation","title":"<code> Benzylation            (Problem)         </code>","text":"<p>Benzylation dataset.</p> <p>This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction. Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective.</p> <p>Reference</p> <p>A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI. Obtained from Olympus.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class Benzylation(Problem):\n\"\"\"Benzylation dataset.\n\n    This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction.\n    Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity.\n    The dataset includes 73 samples with four parameters and one objective.\n\n    Reference:\n        A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282.\n        [DOI](https://doi.org/10.1016/j.cej.2018.07.031).\n        Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Benzylation\",\n            inputs=[\n                Continuous(\"flow_rate\", domain=[0.2, 0.4]),\n                Continuous(\"ratio\", domain=[1.0, 5.0]),\n                Continuous(\"solvent\", domain=[0.5, 1.0]),\n                Continuous(\"temperature\", domain=[110.0, 150.0]),\n            ],\n            outputs=[Continuous(\"impurity\")],\n            objectives=[Minimize(\"impurity\")],\n            data=get_data(\"benzylation.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.Cake","title":"<code> Cake            (Problem)         </code>","text":"<p>Cake recipe optimization with mixed objectives.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class Cake(Problem):\n\"\"\"Cake recipe optimization with mixed objectives.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Cake\",\n            inputs=[\n                Continuous(\"wheat_flour\", domain=[0, 1]),\n                Continuous(\"spelt_flour\", domain=[0, 1]),\n                Continuous(\"sugar\", domain=[0, 1]),\n                Continuous(\"chocolate\", domain=[0, 1]),\n                Continuous(\"nuts\", domain=[0, 1]),\n                Continuous(\"carrot\", domain=[0, 1]),\n            ],\n            outputs=[\n                Continuous(\"calories\", domain=[300, 600]),\n                Continuous(\"taste\", domain=[0, 5]),\n                Continuous(\"browning\", domain=[0, 2]),\n            ],\n            objectives=[\n                Minimize(\"calories\"),\n                Maximize(\"taste\"),\n                CloseToTarget(\"browning\", target=1.4),\n            ],\n            constraints=[\n                LinearEquality(\n                    [\n                        \"wheat_flour\",\n                        \"spelt_flour\",\n                        \"sugar\",\n                        \"chocolate\",\n                        \"nuts\",\n                        \"carrot\",\n                    ],\n                    rhs=1,\n                )\n            ],\n            data=get_data(\"cake.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.Fullerenes","title":"<code> Fullerenes            (Problem)         </code>","text":"<p>Buckminsterfullerene dataset.</p> <p>This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes. Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product. Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective.</p> <p>Reference</p> <p>B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798. DOI. Obtained from Olympus.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class Fullerenes(Problem):\n\"\"\"Buckminsterfullerene dataset.\n\n    This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes.\n    Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product.\n    Experiments are executed on a three factor fully factorial grid with six levels per factor.\n    The dataset includes 246 samples with three parameters and one objective.\n\n    Reference:\n        B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798.\n        [DOI](https://doi.org/10.1039/C7RE00123A).\n        Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Fullerenes\",\n            inputs=[\n                Continuous(\"reaction_time\", domain=[3.0, 31.0]),\n                Continuous(\"sultine\", domain=[1.5, 6.0]),\n                Continuous(\"temperature\", domain=[100.0, 150.0]),\n            ],\n            outputs=[Continuous(\"product\")],\n            objectives=[Maximize(\"product\")],\n            data=get_data(\"fullerenes.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.HPLC","title":"<code> HPLC            (Problem)         </code>","text":"<p>High-performance liquid chromatography dataset.</p> <p>This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective.</p> <p>Reference</p> <p>L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) DOI. Obtained from Olympus.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class HPLC(Problem):\n\"\"\"High-performance liquid chromatography dataset.\n\n    This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters.\n    The dataset includes 1,386 samples with six parameters and one objective.\n\n    Reference:\n        L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018)\n        [DOI](https://doi.org/10.26434/chemrxiv.5953606.v1).\n        Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"HPLC\",\n            inputs=[\n                Continuous(\"sample_loop\", domain=[0.0, 0.08]),\n                Continuous(\"additional_volume\", domain=[0.0, 0.06]),\n                Continuous(\"tubing_volume\", domain=[0.1, 0.9]),\n                Continuous(\"sample_flow\", domain=[0.5, 2.5]),\n                Continuous(\"push_speed\", domain=[80.0, 150]),\n                Continuous(\"wait_time\", domain=[0.5, 10.0]),\n            ],\n            outputs=[Continuous(\"peak_area\")],\n            objectives=[Maximize(\"peak_area\")],\n            data=get_data(\"hplc.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.Photodegradation","title":"<code> Photodegradation            (Problem)         </code>","text":"<p>Photodegration dataset.</p> <p>This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective.</p> <p>Reference</p> <p>S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. DOI. Obtained from Olympus.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class Photodegradation(Problem):\n\"\"\"Photodegration dataset.\n\n    This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light.\n    Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend.\n    The dataset includes 2,080 samples with five parameters and one objective.\n\n    Reference:\n        S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801.\n        [DOI](https://doi.org/10.1002/adma.201907801).\n        Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Photodegradation\",\n            inputs=[\n                Continuous(\"PCE10\", domain=[0, 1]),\n                Continuous(\"WF3\", domain=[0, 1]),\n                Continuous(\"P3HT\", domain=[0, 1]),\n                Continuous(\"PCBM\", domain=[0, 1]),\n                Continuous(\"oIDTBR\", domain=[0, 1]),\n            ],\n            outputs=[Continuous(\"degradation\")],\n            objectives=[Minimize(\"degradation\")],\n            constraints=[\n                LinearEquality(\n                    [\"PCE10\", \"WF3\", \"P3HT\", \"PCBM\", \"oIDTBR\"], rhs=1, lhs=1\n                ),\n                NChooseK([\"PCE10\", \"WF3\"], max_active=1),\n            ],\n            data=get_data(\"photodegradation.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.ReizmanSuzuki","title":"<code> ReizmanSuzuki            (Problem)         </code>","text":"<p>Suzuki-Miyaura cross-coupling optimization.</p> <p>Each case was has a different set of substrates but the same possible catalysts.</p> <p>Reference</p> <p>Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry &amp; engineering, 1(6), 658-666 DOI. Data obtained from Summit.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class ReizmanSuzuki(Problem):\n\"\"\"Suzuki-Miyaura cross-coupling optimization.\n\n    Each case was has a different set of substrates but the same possible catalysts.\n\n    Reference:\n        Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry &amp; engineering, 1(6), 658-666\n        [DOI](https://doi.org/10.1039/C6RE00153J).\n        Data obtained from [Summit](https://github.com/sustainable-processes/summit).\n    \"\"\"\n\n    def __init__(self, case=1):\n        assert case in [1, 2, 3, 4]\n        super().__init__(\n            name=f\"Reizman 2016 - Suzuki Case {case}\",\n            inputs=[\n                Categorical(\n                    \"catalyst\",\n                    domain=[\n                        \"P1-L1\",\n                        \"P2-L1\",\n                        \"P1-L2\",\n                        \"P1-L3\",\n                        \"P1-L4\",\n                        \"P1-L5\",\n                        \"P1-L6\",\n                        \"P1-L7\",\n                    ],\n                ),\n                Continuous(\"t_res\", domain=[60, 600]),\n                Continuous(\"temperature\", domain=[30, 110]),\n                Continuous(\"catalyst_loading\", domain=[0.496, 2.515]),\n            ],\n            outputs=[\n                Continuous(\"ton\", domain=[0, 100]),\n                Continuous(\"yield\", domain=[0, 100]),\n            ],\n            objectives=[Maximize(\"ton\"), Maximize(\"yield\")],\n            data=get_data(f\"reizman_suzuki{case}.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.SnAr","title":"<code> SnAr            (Problem)         </code>","text":"<p>SnAr reaction optimization.</p> <p>This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism. Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective.</p> <p>Reference</p> <p>A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI. Obtained from Olympus.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class SnAr(Problem):\n\"\"\"SnAr reaction optimization.\n\n    This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism.\n    Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product).\n    The dataset includes 67 samples with four parameters and one objective.\n\n    Reference:\n        A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282.\n        [DOI](https://doi.org/10.1016/j.cej.2018.07.031).\n        Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"SnAr\",\n            inputs=[\n                Continuous(\"residence_time\", domain=[0.5, 2.0]),\n                Continuous(\"ratio\", domain=[1.0, 5.0]),\n                Continuous(\"concentration\", domain=[0.1, 0.5]),\n                Continuous(\"temperature\", domain=[60.0, 140.0]),\n            ],\n            outputs=[Continuous(\"impurity\")],\n            objectives=[Minimize(\"impurity\")],\n            data=get_data(\"snar.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.datasets.Suzuki","title":"<code> Suzuki            (Problem)         </code>","text":"<p>Suzuki reaction dataset.</p> <p>This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate. Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective.</p> <p>Reference</p> <p>F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI. Obtained from Olympus.</p> Source code in <code>opti/problems/datasets.py</code> <pre><code>class Suzuki(Problem):\n\"\"\"Suzuki reaction dataset.\n\n    This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate.\n    Four reaction conditions can be controlled to maximise the reaction yield.\n    The dataset includes 247 samples with four parameters and one objective.\n\n    Reference:\n        F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153.\n        [DOI](https://doi.org/10.1088/2632-2153/abedc8).\n        Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus).\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Suzuki\",\n            inputs=[\n                Continuous(\"temperature\", domain=[75.0, 90.0]),\n                Continuous(\"pd_mol\", domain=[0.5, 5.0]),\n                Continuous(\"arbpin\", domain=[1.0, 1.8]),\n                Continuous(\"k3po4\", domain=[1.5, 3.0]),\n            ],\n            outputs=[Continuous(\"yield\")],\n            objectives=[Maximize(\"yield\")],\n            data=get_data(\"suzuki.csv\"),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.detergent","title":"<code>detergent</code>","text":""},{"location":"ref-problems/#opti.problems.detergent.Detergent","title":"<code> Detergent            (Problem)         </code>","text":"<p>Detergent formulation problem.</p> <p>There are 5 outputs representing the washing performance on different stain types. Each output is modeled as a second degree polynomial. The formulation consists of 5 components. The sixth input is a filler (water) and is factored out and it's parameter bounds 0.6 &lt; water &lt; 0.8 result in 2 linear inequality constraints for the other parameters.</p> Source code in <code>opti/problems/detergent.py</code> <pre><code>class Detergent(Problem):\n\"\"\"Detergent formulation problem.\n\n    There are 5 outputs representing the washing performance on different stain types.\n    Each output is modeled as a second degree polynomial.\n    The formulation consists of 5 components.\n    The sixth input is a filler (water) and is factored out and it's parameter bounds\n    0.6 &lt; water &lt; 0.8 result in 2 linear inequality constraints for the other parameters.\n    \"\"\"\n\n    def __init__(self):\n        # coefficients for the 2-order polynomial; generated with\n        # base = 3 * np.ones((1, 5))\n        # scale = PolynomialFeatures(degree=2).fit_transform(base).T\n        # coef = np.random.RandomState(42).normal(scale=scale, size=(len(scale), 5))\n        # coef = np.clip(coef, 0, None)\n        self.coef = np.array(\n            [\n                [0.4967, 0.0, 0.6477, 1.523, 0.0],\n                [0.0, 4.7376, 2.3023, 0.0, 1.6277],\n                [0.0, 0.0, 0.7259, 0.0, 0.0],\n                [0.0, 0.0, 0.9427, 0.0, 0.0],\n                [4.3969, 0.0, 0.2026, 0.0, 0.0],\n                [0.3328, 0.0, 1.1271, 0.0, 0.0],\n                [0.0, 16.6705, 0.0, 0.0, 7.4029],\n                [0.0, 1.8798, 0.0, 0.0, 1.7718],\n                [6.6462, 1.5423, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 9.5141, 3.0926, 0.0],\n                [2.9168, 0.0, 0.0, 5.5051, 9.279],\n                [8.3815, 0.0, 0.0, 2.9814, 8.7799],\n                [0.0, 0.0, 0.0, 0.0, 7.3127],\n                [12.2062, 0.0, 9.0318, 3.2547, 0.0],\n                [3.2526, 13.8423, 0.0, 14.0818, 0.0],\n                [7.3971, 0.7834, 0.0, 0.8258, 0.0],\n                [0.0, 3.214, 13.301, 0.0, 0.0],\n                [0.0, 8.2386, 2.9588, 0.0, 4.6194],\n                [0.8737, 8.7178, 0.0, 0.0, 0.0],\n                [0.0, 2.6651, 2.3495, 0.046, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0],\n            ]\n        )\n\n        super().__init__(\n            name=\"Detergent optimization\",\n            inputs=[\n                Continuous(\"x1\", domain=[0.0, 0.2]),\n                Continuous(\"x2\", domain=[0.0, 0.3]),\n                Continuous(\"x3\", domain=[0.02, 0.2]),\n                Continuous(\"x4\", domain=[0.0, 0.06]),\n                Continuous(\"x5\", domain=[0.0, 0.04]),\n            ],\n            outputs=[Continuous(f\"y{i+1}\", domain=[0, 3]) for i in range(5)],\n            objectives=[Maximize(f\"y{i+1}\") for i in range(5)],\n            constraints=[\n                LinearInequality([\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"], lhs=-1, rhs=-0.2),\n                LinearInequality([\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"], lhs=1, rhs=0.4),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = np.atleast_2d(X[self.inputs.names])\n        xp = np.stack([_poly2(xi) for xi in x], axis=0)\n        return pd.DataFrame(xp @ self.coef, columns=self.outputs.names, index=X.index)\n</code></pre>"},{"location":"ref-problems/#opti.problems.detergent.Detergent_NChooseKConstraint","title":"<code> Detergent_NChooseKConstraint            (Problem)         </code>","text":"<p>Variant of the Detergent problem where only 3 of the 5 formulation components are allowed to be active (n-choose-k constraint).</p> Source code in <code>opti/problems/detergent.py</code> <pre><code>class Detergent_NChooseKConstraint(Problem):\n\"\"\"Variant of the Detergent problem where only 3 of the 5 formulation components are allowed to be active (n-choose-k constraint).\"\"\"\n\n    def __init__(self):\n        base = Detergent()\n\n        super().__init__(\n            name=\"Detergent optimization with n-choose-k constraint\",\n            inputs=base.inputs,\n            outputs=base.outputs,\n            objectives=base.objectives,\n            constraints=list(base.constraints)\n            + [NChooseK(names=base.inputs.names, max_active=3)],\n            f=base.f,\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.detergent.Detergent_OutputConstraint","title":"<code> Detergent_OutputConstraint            (Problem)         </code>","text":"<p>Variant of the Detergent problem with an additional output/black-box constraint.</p> <p>In addition to the 5 washing performances there is a sixth output reflecting the stability of the formulation. If <code>discrete=True</code> the stability can only be measured qualitatively (0: not stable, 1: stable). If <code>discrete=True</code> the stability can be measured quantively with smaller values indicating less stable formulations.</p> Source code in <code>opti/problems/detergent.py</code> <pre><code>class Detergent_OutputConstraint(Problem):\n\"\"\"Variant of the Detergent problem with an additional output/black-box constraint.\n\n    In addition to the 5 washing performances there is a sixth output reflecting the stability of the formulation.\n    If `discrete=True` the stability can only be measured qualitatively (0: not stable, 1: stable).\n    If `discrete=True` the stability can be measured quantively with smaller values indicating less stable formulations.\n    \"\"\"\n\n    def __init__(self, discrete=False):\n        base = Detergent()\n\n        def f(X):\n            Y = base.f(X)\n            if discrete:\n                Y[\"stable\"] = (X.sum(axis=1) &lt; 0.3).astype(int)\n            else:\n                Y[\"stable\"] = (0.4 - X.sum(axis=1)) / 0.2\n            return Y\n\n        outputs = list(base.outputs)\n        if discrete:\n            outputs += [Discrete(\"stable\", domain=[0, 1])]\n        else:\n            outputs += [Continuous(\"stable\", domain=[0, 1])]\n\n        super().__init__(\n            name=\"Detergent optimization with stability constraint\",\n            inputs=base.inputs,\n            outputs=outputs,\n            objectives=base.objectives,\n            output_constraints=[Maximize(\"stable\", target=0.5)],\n            constraints=base.constraints,\n            f=f,\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.detergent.Detergent_TwoOutputConstraints","title":"<code> Detergent_TwoOutputConstraints            (Problem)         </code>","text":"<p>Variant of the Detergent problem with two outputs constraint.</p> <p>In addition to the 5 washing performances there are two more outputs measuring the formulation stability. The first, stability 1, measures the immediate stability. If not stable, the other properties cannot be measured, except for stability 2. The second, stability 2, measures the long-term stability.</p> Source code in <code>opti/problems/detergent.py</code> <pre><code>class Detergent_TwoOutputConstraints(Problem):\n\"\"\"Variant of the Detergent problem with two outputs constraint.\n\n    In addition to the 5 washing performances there are two more outputs measuring the formulation stability.\n    The first, stability 1, measures the immediate stability. If not stable, the other properties cannot be measured, except for stability 2.\n    The second, stability 2, measures the long-term stability.\n    \"\"\"\n\n    def __init__(self):\n        base = Detergent()\n\n        def f(X: pd.DataFrame) -&gt; pd.DataFrame:\n            Y = base.f(X)\n            x = self.get_X(X)\n            stable1 = (x.sum(axis=1) &lt; 0.3).astype(int)\n            stable2 = (x[:, :-1].sum(axis=1) &lt; 0.25).astype(int)\n            Y[stable1 == 0] = np.nan\n            Y[\"stability 1\"] = stable1\n            Y[\"stability 2\"] = stable2\n            return Y\n\n        outputs = list(base.outputs) + [\n            Discrete(\"stability 1\", domain=[0, 1]),\n            Discrete(\"stability 2\", domain=[0, 1]),\n        ]\n\n        super().__init__(\n            name=\"Detergent optimization with two output constraint\",\n            inputs=base.inputs,\n            outputs=outputs,\n            objectives=base.objectives,\n            output_constraints=[\n                Maximize(\"stability 1\", target=0.5),\n                Maximize(\"stability 2\", target=0.5),\n            ],\n            constraints=base.constraints,\n            f=f,\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.mixed","title":"<code>mixed</code>","text":"<p>Mixed variables single and multi-objective test problems.</p>"},{"location":"ref-problems/#opti.problems.mixed.DiscreteFuelInjector","title":"<code> DiscreteFuelInjector            (Problem)         </code>","text":"<p>Fuel injector test problem, modified to contain an integer variable.</p> <ul> <li>4 objectives,</li> <li>mixed variables, unconstrained</li> </ul> <p>See</p> <p>Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9</p> Source code in <code>opti/problems/mixed.py</code> <pre><code>class DiscreteFuelInjector(Problem):\n\"\"\"Fuel injector test problem, modified to contain an integer variable.\n\n    * 4 objectives,\n    * mixed variables, unconstrained\n\n    See:\n        Manson2021, MVMOO: Mixed variable multi-objective optimisation\n        https://doi.org/10.1007/s10898-021-01052-9\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Discrete fuel injector test problem\",\n            inputs=[\n                Discrete(\"x1\", [0, 1, 2, 3]),\n                Continuous(\"x2\", [-2, 2]),\n                Continuous(\"x3\", [-2, 2]),\n                Continuous(\"x4\", [-2, 2]),\n            ],\n            outputs=[Continuous(f\"y{i+1}\") for i in range(4)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x1 = X[\"x1\"].to_numpy().astype(float)\n        x2 = X[\"x2\"].to_numpy().astype(float)\n        x3 = X[\"x3\"].to_numpy().astype(float)\n        x4 = X[\"x4\"].to_numpy().astype(float)\n        x1 *= 0.2\n        y1 = (\n            0.692\n            + 0.4771 * x1\n            - 0.687 * x4\n            - 0.08 * x3\n            - 0.065 * x2\n            - 0.167 * x1**2\n            - 0.0129 * x1 * x4\n            + 0.0796 * x4**2\n            - 0.0634 * x1 * x3\n            - 0.0257 * x3 * x4\n            + 0.0877 * x3**2\n            - 0.0521 * x1 * x2\n            + 0.00156 * x2 * x4\n            + 0.00198 * x2 * x3\n            + 0.0184 * x2**2\n        )\n        y2 = (\n            0.37\n            - 0.205 * x1\n            + 0.0307 * x4\n            + 0.108 * x3\n            + 1.019 * x2\n            - 0.135 * x1**2\n            + 0.0141 * x1 * x4\n            + 0.0998 * x4**2\n            + 0.208 * x1 * x3\n            - 0.0301 * x3 * x4\n            - 0.226 * x3**2\n            + 0.353 * x1 * x2\n            - 0.0497 * x2 * x3\n            - 0.423 * x2**2\n            + 0.202 * x1**2 * x4\n            - 0.281 * x1**2 * x3\n            - 0.342 * x1 * x4**2\n            - 0.245 * x3 * x4**2\n            + 0.281 * x3**2 * x4\n            - 0.184 * x1 * x2**2\n            + 0.281 * x1 * x3 * x4\n        )\n        y3 = (\n            0.153\n            - 0.322 * x1\n            + 0.396 * x4\n            + 0.424 * x3\n            + 0.0226 * x2\n            + 0.175 * x1**2\n            + 0.0185 * x1 * x4\n            - 0.0701 * x4**2\n            - 0.251 * x1 * x3\n            + 0.179 * x3 * x4\n            + 0.015 * x3**2\n            + 0.0134 * x1 * x2\n            + 0.0296 * x2 * x4\n            + 0.0752 * x2 * x3\n            + 0.0192 * x2**2\n        )\n        y4 = (\n            0.758\n            + 0.358 * x1\n            - 0.807 * x4\n            + 0.0925 * x3\n            - 0.0468 * x2\n            - 0.172 * x1**2\n            + 0.0106 * x1 * x4\n            + 0.0697 * x4**2\n            - 0.146 * x1 * x3\n            - 0.0416 * x3 * x4\n            + 0.102 * x3**2\n            - 0.0694 * x1 * x2\n            - 0.00503 * x2 * x4\n            + 0.0151 * x2 * x3\n            + 0.0173 * x2**2\n        )\n        return pd.DataFrame({\"y1\": y1, \"y2\": y2, \"y3\": y3, \"y4\": y4})\n</code></pre>"},{"location":"ref-problems/#opti.problems.mixed.DiscreteVLMOP2","title":"<code> DiscreteVLMOP2            (Problem)         </code>","text":"<p>VLMOP2 problem (also known as Fonzeca &amp; Fleming), modified to contain a discrete variable.</p> <ul> <li>2 minimization objectives</li> <li>1 categorical and n continuous inputs, unconstrained</li> </ul> <p>See</p> <p>Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9</p> Source code in <code>opti/problems/mixed.py</code> <pre><code>class DiscreteVLMOP2(Problem):\n\"\"\"VLMOP2 problem (also known as Fonzeca &amp; Fleming), modified to contain a discrete variable.\n\n    * 2 minimization objectives\n    * 1 categorical and n continuous inputs, unconstrained\n\n    See:\n        Manson2021, MVMOO: Mixed variable multi-objective optimisation\n        https://doi.org/10.1007/s10898-021-01052-9\n\n    \"\"\"\n\n    def __init__(self, n_inputs: int = 3):\n        assert n_inputs &gt;= 2\n        super().__init__(\n            name=\"Discrete VLMOP2 test problem\",\n            inputs=[Categorical(\"x1\", [\"a\", \"b\"])]\n            + [Continuous(f\"x{i+1}\", [-2, 2]) for i in range(1, n_inputs)],\n            outputs=[Continuous(\"y1\"), Continuous(\"y2\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        d = X[self.inputs.names[0]].values\n        x = X[self.inputs.names[1:]].values\n        n = self.n_inputs\n        y1 = np.exp(-np.sum((x - n**-0.5) ** 2, axis=1))\n        y2 = np.exp(-np.sum((x + n**-0.5) ** 2, axis=1))\n        y1 = np.where(d == \"a\", 1 - y1, 1.25 - y1)\n        y2 = np.where(d == \"a\", 1 - y2, 0.75 - y2)\n        return pd.DataFrame({\"y1\": y1, \"y2\": y2}, index=X.index)\n</code></pre>"},{"location":"ref-problems/#opti.problems.multi","title":"<code>multi</code>","text":""},{"location":"ref-problems/#opti.problems.multi.Daechert1","title":"<code> Daechert1            (Problem)         </code>","text":"<p>Problem with a non-convex Pareto front.</p> <p>From D\u00e4chert &amp; Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249</p> <p>The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44].</p> Source code in <code>opti/problems/multi.py</code> <pre><code>class Daechert1(Problem):\n\"\"\"Problem with a non-convex Pareto front.\n\n    From D\u00e4chert &amp; Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249\n\n    The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44].\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Daechert-1\",\n            inputs=[\n                Continuous(\"x1\", domain=[0, np.pi]),\n                Continuous(\"x2\", domain=[0, 10]),\n                Continuous(\"x3\", domain=[1.2, 10]),\n            ],\n            outputs=[Continuous(f\"y{i+1}\") for i in range(3)],\n            constraints=[NonlinearInequality(\"- cos(x1) - exp(-x2) + x3\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame({\"y1\": -X[\"x1\"], \"y2\": -X[\"x2\"], \"y3\": -X[\"x3\"] ** 2})\n</code></pre>"},{"location":"ref-problems/#opti.problems.multi.Daechert2","title":"<code> Daechert2            (Problem)         </code>","text":"<p>Unconstrained problem with a Pareto front resembling a comet.</p> <p>From D\u00e4chert &amp; Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249</p> <p>minimize     f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2)     f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2)     f3(x) = 3 (1 + x3) x1^2 s.t.      1 &lt;= x1 &lt;= 3.5     -2 &lt;= x2 &lt;= 2      0 &lt;= x3 &lt;= 1</p> <p>The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5].</p> Source code in <code>opti/problems/multi.py</code> <pre><code>class Daechert2(Problem):\n\"\"\"Unconstrained problem with a Pareto front resembling a comet.\n\n    From D\u00e4chert &amp; Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249\n\n    minimize\n        f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2)\n        f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2)\n        f3(x) = 3 (1 + x3) x1^2\n    s.t.\n         1 &lt;= x1 &lt;= 3.5\n        -2 &lt;= x2 &lt;= 2\n         0 &lt;= x3 &lt;= 1\n\n    The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5].\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Daechert-2\",\n            inputs=[\n                Continuous(\"x1\", domain=[1, 3.5]),\n                Continuous(\"x2\", domain=[-2, 2]),\n                Continuous(\"x3\", domain=[0, 1]),\n            ],\n            outputs=[Continuous(f\"y{i + 1}\") for i in range(3)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y1\": X.eval(\"(1 + x3) * (x1**3 * x2**2 - 10 * x1 - 4 * x2)\"),\n                \"y2\": X.eval(\"(1 + x3) * (x1**3 * x2**2 - 10 * x1 + 4 * x2)\"),\n                \"y3\": X.eval(\"3 * (1 + x3) * x1**2\"),\n            }\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.multi.Daechert3","title":"<code> Daechert3            (Problem)         </code>","text":"<p>Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts.</p> <p>From D\u00e4chert &amp; Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249</p> <p>The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6].</p> Source code in <code>opti/problems/multi.py</code> <pre><code>class Daechert3(Problem):\n\"\"\"Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts.\n\n    From D\u00e4chert &amp; Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249\n\n    The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6].\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Daechert-3\",\n            inputs=[Continuous(f\"x{i+1}\", domain=[0, 1]) for i in range(2)],\n            outputs=[Continuous(f\"y{i+1}\") for i in range(3)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = X[self.inputs.names]\n        return pd.DataFrame(\n            {\n                \"y1\": X[\"x1\"],\n                \"y2\": X[\"x2\"],\n                \"y3\": 6 - np.sum(x * (1 + np.sin(3 * np.pi * x)), axis=1),\n            }\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.multi.Hyperellipsoid","title":"<code> Hyperellipsoid            (Problem)         </code>","text":"<p>Hyperellipsoid in n dimensions</p> <p>minimize     f_m(x) = x_m    m = 1, ... n for     x in R^n s.t.     sum((x / a)^2) - 1 &lt;= 0</p> <p>The ideal point is -a and the is nadir 0^n.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Dimension of the hyperellipsoid. Defaults to 5.</p> <code>5</code> <code>a</code> <code>list-like</code> <p>Half length of principal axes. a = None or a = [1, ...] results in a hypersphere.</p> <code>None</code> Source code in <code>opti/problems/multi.py</code> <pre><code>class Hyperellipsoid(Problem):\n\"\"\"Hyperellipsoid in n dimensions\n\n    minimize\n        f_m(x) = x_m    m = 1, ... n\n    for\n        x in R^n\n    s.t.\n        sum((x / a)^2) - 1 &lt;= 0\n\n    The ideal point is -a and the is nadir 0^n.\n\n    Args:\n        n (int, optional): Dimension of the hyperellipsoid. Defaults to 5.\n        a (list-like, optional): Half length of principal axes. a = None or a = [1, ...] results in a hypersphere.\n    \"\"\"\n\n    def __init__(self, n: int = 5, a: Optional[Union[list, np.ndarray]] = None):\n        if a is None:\n            a = np.ones(n)\n            constr = \" + \".join([f\"x{i+1}**2\" for i in range(n)]) + \" - 1\"\n        else:\n            a = np.array(a).squeeze()\n            if len(a) != n:\n                raise ValueError(\"Dimension of half axes doesn't match input dimension\")\n            constr = \" + \".join([f\"(x{i+1}/{a[i]})**2\" for i in range(n)]) + \" - 1\"\n        self.a = a\n\n        super().__init__(\n            name=\"Hyperellipsoid\",\n            inputs=[Continuous(f\"x{i+1}\", [-a[i], a[i]]) for i in range(n)],\n            outputs=[Continuous(f\"y{i+1}\", [-a[i], a[i]]) for i in range(n)],\n            constraints=[NonlinearInequality(constr)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        y = X[self.inputs.names]\n        y.columns = self.outputs.names\n        return y\n\n    def get_optima(self, n=10) -&gt; pd.DataFrame:\n        X = opti.sampling.sphere.sample(self.n_inputs, n, positive=True)\n        X = np.concatenate([-np.eye(self.n_inputs), -X], axis=0)[:n]\n        Y = self.a * X\n        return pd.DataFrame(\n            data=np.column_stack([X, Y]),\n            columns=self.inputs.names + self.outputs.names,\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.multi.OmniTest","title":"<code> OmniTest            (Problem)         </code>","text":"<p>Bi-objective benchmark problem with D inputs and a multi-modal Pareto set.</p> <p>It has 3^D Pareto subsets in the decision space corresponding to the same Pareto front.</p> <p>Reference</p> <p>Deb &amp; Tiwari \"Omni-optimizer: A generic evolutionary algorithm for single and multi-objective optimization\"</p> Source code in <code>opti/problems/multi.py</code> <pre><code>class OmniTest(Problem):\n\"\"\"Bi-objective benchmark problem with D inputs and a multi-modal Pareto set.\n\n    It has 3^D Pareto subsets in the decision space corresponding to the same Pareto front.\n\n    Reference:\n        Deb &amp; Tiwari \"Omni-optimizer: A generic evolutionary algorithm for single and multi-objective optimization\"\n    \"\"\"\n\n    def __init__(self, n_inputs: int = 2):\n        super().__init__(\n            name=\"Omni\",\n            inputs=[Continuous(f\"x{i+1}\", domain=[0, 6]) for i in range(2)],\n            outputs=[Continuous(\"y1\"), Continuous(\"y2\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        X = X[self.inputs.names]\n        return pd.DataFrame(\n            {\n                \"y1\": np.sum(np.sin(np.pi * X), axis=1),\n                \"y2\": np.sum(np.cos(np.pi * X), axis=1),\n            }\n        )\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        n = 11  # points per set (3^D sets)\n        s = [np.linspace(1, 1.5, n) + 2 * i for i in range(3)]\n        C = list(\n            product(\n                *[\n                    s,\n                ]\n                * self.n_inputs\n            )\n        )\n        C = np.moveaxis(C, 1, 2).reshape(-1, 2)\n        X = pd.DataFrame(C, columns=self.inputs.names)\n        XY = pd.concat([X, self.f(X)], axis=1)\n        XY[\"_patch\"] = np.repeat(np.arange(3**self.n_inputs), n)\n        return XY\n</code></pre>"},{"location":"ref-problems/#opti.problems.multi.Poloni","title":"<code> Poloni            (Problem)         </code>","text":"<p>Poloni benchmark problem.</p> Source code in <code>opti/problems/multi.py</code> <pre><code>class Poloni(Problem):\n\"\"\"Poloni benchmark problem.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Poloni function\",\n            inputs=[Continuous(f\"x{i+1}\", [-np.pi, np.pi]) for i in range(2)],\n            outputs=[Continuous(\"y1\"), Continuous(\"y2\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x1, x2 = self.get_X(X).T\n        A1 = 0.5 * np.sin(1) - 2 * np.cos(1) + np.sin(2) - 1.5 * np.cos(2)\n        A2 = 1.5 * np.sin(1) - np.cos(1) + 2 * np.sin(2) - 0.5 * np.cos(2)\n        B1 = 0.5 * np.sin(x1) - 2 * np.cos(x1) + np.sin(x2) - 1.5 * np.cos(x2)\n        B2 = 1.5 * np.sin(x1) - np.cos(x1) + 2 * np.sin(x2) - 0.5 * np.cos(x2)\n        return pd.DataFrame(\n            {\n                \"y1\": 1 + (A1 - B1) ** 2 + (A2 - B2) ** 2,\n                \"y2\": (x1 + 3) ** 2 + (x2 + 1) ** 2,\n            },\n            index=X.index,\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.multi.Qapi1","title":"<code> Qapi1            (Problem)         </code>","text":"<p>Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not.</p> <p>minimize     f1(x) = (x1 - 2)^2 + (x2 - 1)^2     f2(x) = x1^2 + (x2 - 3)^2 for     x1 in [0, inf)     x2 in (-inf, inf) s.t.     0 &lt;= x1     c1(x) = - x1^2 + x2 &lt;= 0     c2(x) = - x1 - x2 + 2 &lt;= 0</p> <p>The ideal point is [0, 0] and the nadir is [8, 8].</p> Source code in <code>opti/problems/multi.py</code> <pre><code>class Qapi1(Problem):\n\"\"\"Constrained problem from the Qriteria API tests.\n    Note that while the function is convex, the constraints are not.\n\n    minimize\n        f1(x) = (x1 - 2)^2 + (x2 - 1)^2\n        f2(x) = x1^2 + (x2 - 3)^2\n    for\n        x1 in [0, inf)\n        x2 in (-inf, inf)\n    s.t.\n        0 &lt;= x1\n        c1(x) = - x1^2 + x2 &lt;= 0\n        c2(x) = - x1 - x2 + 2 &lt;= 0\n\n    The ideal point is [0, 0] and the nadir is [8, 8].\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Constrained bi-objective problem\",\n            inputs=[Continuous(\"x1\", [0, 10]), Continuous(\"x2\", [-10, 10])],\n            outputs=[Continuous(\"y1\"), Continuous(\"y2\")],\n            constraints=[\n                NonlinearInequality(\"x2 - x1**2\"),\n                NonlinearInequality(\"2 - x1 - x2\"),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            {\n                \"y1\": X.eval(\"(x1 - 2)**2 + (x2 - 1)**2\"),\n                \"y2\": X.eval(\"x1**2 + (x2 - 3)**2\"),\n            }\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.multi.WeldedBeam","title":"<code> WeldedBeam            (Problem)         </code>","text":"<p>Design optimization of a welded beam.</p> <p>This is a bi-objective problem with 4 inputs and 3 (non-)linear inequality constraints. The two objectives are the fabrication cost of the beam and the deflection of the end of the beam under the applied load P. The load P is fixed at 6000 lbs, and the distance L is fixed at 14 inch.</p> <p>Note that for simplicity the constraint shear stress &lt; 13600 psi is not included.</p> <p>See https://www.mathworks.com/help/gads/multiobjective-optimization-welded-beam.html</p> Source code in <code>opti/problems/multi.py</code> <pre><code>class WeldedBeam(Problem):\n\"\"\"Design optimization of a welded beam.\n\n    This is a bi-objective problem with 4 inputs and 3 (non-)linear inequality constraints.\n    The two objectives are the fabrication cost of the beam and the deflection of the end of the beam under the applied load P.\n    The load P is fixed at 6000 lbs, and the distance L is fixed at 14 inch.\n\n    Note that for simplicity the constraint shear stress &lt; 13600 psi is not included.\n\n    See https://www.mathworks.com/help/gads/multiobjective-optimization-welded-beam.html\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Welded beam problem\",\n            inputs=[\n                Continuous(\"h\", [0.125, 5]),  # thickness of welds\n                Continuous(\"l\", [0.1, 10]),  # length of welds\n                Continuous(\"t\", [0.1, 10]),  # height of beam\n                Continuous(\"b\", [0.125, 5]),  # width of beam\n            ],\n            outputs=[Continuous(\"cost\"), Continuous(\"deflection\")],\n            constraints=[\n                # h &lt;= b, weld thickness cannot exceed beam width\n                LinearInequality([\"h\", \"b\"], lhs=[1, -1], rhs=0),\n                # normal stress on the welds cannot exceed 30000 psi\n                NonlinearInequality(\"6000 * 6 * 14 / b / t**3 - 30000\"),\n                # buckling load capacity must exceed 6000 lbs\n                NonlinearInequality(\n                    \"6000 - 60746.022 * (1 - 0.0282346 * t) * t * b**4\"\n                ),\n            ],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x1, x2, x3, x4 = self.get_X(X).T\n        return pd.DataFrame(\n            {\n                \"cost\": 1.10471 * x1**2 * x2 + 0.04811 * x3 * x4 * (14 + x2),\n                \"deflection\": 2.1952 / (x4 * x3**3),\n            },\n            index=X.index,\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.single","title":"<code>single</code>","text":"<p>Single objective benchmark problems.</p>"},{"location":"ref-problems/#opti.problems.single.Ackley","title":"<code> Ackley            (Problem)         </code>","text":"<p>Ackley benchmark problem.</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Ackley(Problem):\n\"\"\"Ackley benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=2):\n        super().__init__(\n            name=\"Ackley problem\",\n            inputs=[Continuous(f\"x{i+1}\", [-32.768, +32.768]) for i in range(n_inputs)],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        a = 20\n        b = 1 / 5\n        c = 2 * np.pi\n        n = self.n_inputs\n        x = self.get_X(X)\n        part1 = -a * np.exp(-b * np.sqrt((1 / n) * np.sum(x**2, axis=-1)))\n        part2 = -np.exp((1 / n) * np.sum(np.cos(c * x), axis=-1))\n        y = part1 + part2 + a + np.exp(1)\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.zeros((1, self.n_inputs))\n        y = 0\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Branin","title":"<code> Branin            (Problem)         </code>","text":"<p>The Branin (Branin-Hoo) benchmark problem.</p> <p>f(x) = a(x2 - b x1^2 + cx1 - r)^2 + s(1 - t) cos(x1) + s a = 1, b = 5.1 / (4 pi^2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8pi)</p> <p>It has 3 global optima.</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Branin(Problem):\n\"\"\"The Branin (Branin-Hoo) benchmark problem.\n\n    f(x) = a(x2 - b x1^2 + cx1 - r)^2 + s(1 - t) cos(x1) + s\n    a = 1, b = 5.1 / (4 pi^2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8pi)\n\n    It has 3 global optima.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Branin function\",\n            inputs=[Continuous(\"x1\", [-5, 10]), Continuous(\"x2\", [0, 15])],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x1, x2 = self.get_X(X).T\n        y = (\n            (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5 / np.pi * x1 - 6) ** 2\n            + 10 * (1 - 1 / (8 * np.pi)) * np.cos(x1)\n            + 10\n        )\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            [\n                [-np.pi, 12.275, 0.397887],\n                [np.pi, 2.275, 0.397887],\n                [9.42478, 2.475, 0.397887],\n            ],\n            columns=self.inputs.names + self.outputs.names,\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Himmelblau","title":"<code> Himmelblau            (Problem)         </code>","text":"<p>Himmelblau benchmark problem</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Himmelblau(Problem):\n\"\"\"Himmelblau benchmark problem\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Himmelblau function\",\n            inputs=[Continuous(f\"x{i+1}\", [-6, 6]) for i in range(2)],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x0, x1 = self.get_X(X).T\n        y = (x0**2 + x1 - 11) ** 2 + (x0 + x1**2 - 7) ** 2\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.array(\n            [\n                [3.0, 2.0],\n                [-2.805118, 3.131312],\n                [-3.779310, -3.283186],\n                [3.584428, -1.848126],\n            ]\n        )\n        y = np.zeros(4)\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Michalewicz","title":"<code> Michalewicz            (Problem)         </code>","text":"<p>Michalewicz benchmark problem.</p> <p>The Michalewicz function has d! local minima, and it is multimodal. The parameter m (m=10 is used here) defines the steepness of they valleys and a larger m leads to a more difficult search.</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Michalewicz(Problem):\n\"\"\"Michalewicz benchmark problem.\n\n    The Michalewicz function has d! local minima, and it is multimodal.\n    The parameter m (m=10 is used here) defines the steepness of they valleys and a larger m leads to a more difficult search.\n    \"\"\"\n\n    def __init__(self, n_inputs: int = 2):\n        super().__init__(\n            name=\"Michalewicz function\",\n            inputs=[Continuous(f\"x{i+1}\", [0, np.pi]) for i in range(n_inputs)],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = self.get_X(X)\n        m = 10\n        i = np.arange(1, self.n_inputs + 1)\n        y = -np.sum(np.sin(x) * np.sin(i * x**2 / np.pi) ** (2 * m), axis=1)\n        return pd.DataFrame({\"y\": y}, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = pd.DataFrame([[2.2, 1.57]], columns=self.inputs.names)\n        return pd.concat([x, self.f(x)], axis=1)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Rastrigin","title":"<code> Rastrigin            (Problem)         </code>","text":"<p>Rastrigin benchmark problem.</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Rastrigin(Problem):\n\"\"\"Rastrigin benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=2):\n        super().__init__(\n            name=\"Rastrigin function\",\n            inputs=[Continuous(f\"x{i+1}\", [-5, 5]) for i in range(n_inputs)],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = self.get_X(X)\n        a = 10\n        y = a * self.n_inputs + np.sum(x**2 - a * np.cos(2 * np.pi * x), axis=1)\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.zeros((1, self.n_inputs))\n        y = 0\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Rosenbrock","title":"<code> Rosenbrock            (Problem)         </code>","text":"<p>Rosenbrock benchmark problem.</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Rosenbrock(Problem):\n\"\"\"Rosenbrock benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=2):\n        super().__init__(\n            name=\"Rosenbrock function\",\n            inputs=[Continuous(f\"x{i+1}\", [-2.048, 2.048]) for i in range(n_inputs)],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = self.get_X(X).T\n        y = np.sum(100 * (x[1:] - x[:-1] ** 2) ** 2 + (1 - x[:-1]) ** 2, axis=0)\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.ones((1, self.n_inputs))\n        y = 0\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Schwefel","title":"<code> Schwefel            (Problem)         </code>","text":"<p>Schwefel benchmark problem</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Schwefel(Problem):\n\"\"\"Schwefel benchmark problem\"\"\"\n\n    def __init__(self, n_inputs=2):\n        super().__init__(\n            name=\"Schwefel function\",\n            inputs=[Continuous(f\"x{i+1}\", [-500, 500]) for i in range(n_inputs)],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = self.get_X(X)\n        y = 418.9829 * self.n_inputs - np.sum(x * np.sin(np.abs(x) ** 0.5), axis=1)\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.full((1, self.n_inputs), 420.9687)\n        y = 0\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Sphere","title":"<code> Sphere            (Problem)         </code>","text":"<p>Sphere benchmark problem.</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Sphere(Problem):\n\"\"\"Sphere benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=10):\n        super().__init__(\n            name=\"Sphere function\",\n            inputs=[Continuous(f\"x{i+1}\", [0, 1]) for i in range(n_inputs)],\n            outputs=[Continuous(\"y\", [0, 2])],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = self.get_X(X)\n        y = np.sum((x - 0.5) ** 2, axis=1)\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.full((1, self.n_inputs), 0.5)\n        y = 0\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.ThreeHumpCamel","title":"<code> ThreeHumpCamel            (Problem)         </code>","text":"<p>Three-hump camel benchmark problem.</p> Source code in <code>opti/problems/single.py</code> <pre><code>class ThreeHumpCamel(Problem):\n\"\"\"Three-hump camel benchmark problem.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"Three-hump camel function\",\n            inputs=[Continuous(f\"x{i+1}\", [-5, 5]) for i in range(2)],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x1, x2 = self.get_X(X).T\n        y = 2 * x1**2 - 1.05 * x1**4 + x1**6 / 6 + x1 * x2 + x2**2\n        return pd.DataFrame(y, columns=[\"y\"], index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            np.zeros((1, 3)), columns=self.inputs.names + self.outputs.names\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Zakharov","title":"<code> Zakharov            (Problem)         </code>","text":"<p>Zakharov benchmark problem.</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Zakharov(Problem):\n\"\"\"Zakharov benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=2):\n        super().__init__(\n            name=\"Zakharov function\",\n            inputs=[Continuous(f\"x{i+1}\", [-10, 10]) for i in range(n_inputs)],\n            outputs=[Continuous(\"y\")],\n        )\n\n    def f(self, X: pd.DataFrame):\n        x = self.get_X(X)\n        a = 0.5 * np.sum(np.arange(1, self.n_inputs + 1) * x, axis=1)\n        y = np.sum(x**2, axis=1) + a**2 + a**4\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = np.zeros((1, self.n_inputs))\n        y = 0\n        return pd.DataFrame(np.c_[x, y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Zakharov_Categorical","title":"<code> Zakharov_Categorical            (Problem)         </code>","text":"<p>Zakharov problem with one categorical input</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Zakharov_Categorical(Problem):\n\"\"\"Zakharov problem with one categorical input\"\"\"\n\n    def __init__(self, n_inputs=3):\n        base = Zakharov(n_inputs)\n        super().__init__(\n            name=\"Zakharov function with one categorical input\",\n            inputs=[Continuous(f\"x{i}\", [-10, 10]) for i in range(n_inputs - 1)]\n            + [Categorical(\"expon_switch\", [\"one\", \"two\"])],\n            outputs=base.outputs,\n        )\n\n    def f(self, X: pd.DataFrame):\n        x_conti = X[self.inputs.names[:-1]].values  # just the continuous inputs\n        a = 0.5 * np.sum(np.arange(1, self.n_inputs) * x_conti, axis=1)\n        powers = np.repeat(np.expand_dims([2.0, 2.0, 4.0], 0), repeats=len(X), axis=0)\n        modify_powers = X[self.inputs.names[-1]] == \"two\"\n        powers[modify_powers, :] += powers[modify_powers, :]\n        res = (\n            np.sum(x_conti ** np.expand_dims(powers[:, 0], 1), axis=1)\n            + a ** np.expand_dims(powers[:, 1], 0)\n            + a ** np.expand_dims(powers[:, 2], 0)\n        )\n        res_float_array = np.array(res, dtype=np.float64).ravel()\n        y = res_float_array\n        return pd.DataFrame(y, columns=self.outputs.names, index=X.index)\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        x = list(np.zeros(self.n_inputs - 1)) + [\"one\"]\n        y = [0]\n        return pd.DataFrame([x + y], columns=self.inputs.names + self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Zakharov_Constrained","title":"<code> Zakharov_Constrained            (Problem)         </code>","text":"<p>Zakharov problem with one linear constraint</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Zakharov_Constrained(Problem):\n\"\"\"Zakharov problem with one linear constraint\"\"\"\n\n    def __init__(self, n_inputs=5):\n        base = Zakharov(n_inputs)\n        self.base = base\n        super().__init__(\n            name=\"Zakharov with one linear constraint\",\n            inputs=base.inputs,\n            outputs=base.outputs,\n            constraints=[LinearInequality(base.inputs.names, lhs=1, rhs=10)],\n            f=base.f,\n        )\n\n    def get_optima(self):\n        return self.base.get_optima()\n</code></pre>"},{"location":"ref-problems/#opti.problems.single.Zakharov_NChooseKConstraint","title":"<code> Zakharov_NChooseKConstraint            (Problem)         </code>","text":"<p>Zakharov problem with an n-choose-k constraint</p> Source code in <code>opti/problems/single.py</code> <pre><code>class Zakharov_NChooseKConstraint(Problem):\n\"\"\"Zakharov problem with an n-choose-k constraint\"\"\"\n\n    def __init__(self, n_inputs=5, n_max_active=3):\n        base = Zakharov(n_inputs)\n        self.base = base\n        super().__init__(\n            name=\"Zakharov with n-choose-k constraint\",\n            inputs=base.inputs,\n            outputs=base.outputs,\n            constraints=[NChooseK(names=base.inputs.names, max_active=n_max_active)],\n            f=base.f,\n        )\n\n    def get_optima(self):\n        return self.base.get_optima()\n</code></pre>"},{"location":"ref-problems/#opti.problems.univariate","title":"<code>univariate</code>","text":"<p>Simple 1D problems for assessing probabilistic surrogate models. Note: these problems should be output-noisified, e.g. <pre><code>import opti\n\nproblem = opti.problems.noisify_problem_with_gaussian(\n    opti.problems.Line1D(),\n    sigma=0.1\n)\n</code></pre></p>"},{"location":"ref-problems/#opti.problems.univariate.Line1D","title":"<code> Line1D            (Problem)         </code>","text":"<p>A line.</p> Source code in <code>opti/problems/univariate.py</code> <pre><code>class Line1D(Problem):\n\"\"\"A line.\"\"\"\n\n    def __init__(self):\n        def f(X: pd.DataFrame) -&gt; pd.DataFrame:\n            return pd.DataFrame({\"y\": X.eval(\"0.1 * x + 1\")}, index=X.index)\n\n        super().__init__(\n            inputs=[Continuous(\"x\", [0, 10])],\n            outputs=[Continuous(\"y\", [0, 3])],\n            f=f,\n            data=pd.concat([_X, f(_X)], axis=1),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.univariate.Parabola1D","title":"<code> Parabola1D            (Problem)         </code>","text":"<p>A parabola.</p> Source code in <code>opti/problems/univariate.py</code> <pre><code>class Parabola1D(Problem):\n\"\"\"A parabola.\"\"\"\n\n    def __init__(self):\n        def f(X: pd.DataFrame) -&gt; pd.DataFrame:\n            return pd.DataFrame(\n                {\"y\": X.eval(\"0.025 * (x - 5) ** 2 + 1\")}, index=X.index\n            )\n\n        super().__init__(\n            inputs=[Continuous(\"x\", [0, 10])],\n            outputs=[Continuous(\"y\", [0, 3])],\n            f=f,\n            data=pd.concat([_X, f(_X)], axis=1),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.univariate.Sigmoid1D","title":"<code> Sigmoid1D            (Problem)         </code>","text":"<p>A smooth step at x=5.</p> Source code in <code>opti/problems/univariate.py</code> <pre><code>class Sigmoid1D(Problem):\n\"\"\"A smooth step at x=5.\"\"\"\n\n    def __init__(self):\n        def f(X: pd.DataFrame) -&gt; pd.DataFrame:\n            return pd.DataFrame(\n                {\"y\": X.eval(\"1 / (1 + exp(-2 * (x - 5))) + 1\")}, index=X.index\n            )\n\n        super().__init__(\n            inputs=[Continuous(\"x\", [0, 10])],\n            outputs=[Continuous(\"y\", [0, 3])],\n            f=f,\n            data=pd.concat([_X, f(_X)], axis=1),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.univariate.Sinus1D","title":"<code> Sinus1D            (Problem)         </code>","text":"<p>A sinus-function with one full period over the domain.</p> Source code in <code>opti/problems/univariate.py</code> <pre><code>class Sinus1D(Problem):\n\"\"\"A sinus-function with one full period over the domain.\"\"\"\n\n    def __init__(self):\n        def f(X: pd.DataFrame) -&gt; pd.DataFrame:\n            return pd.DataFrame(\n                {\"y\": X.eval(\"sin(x * 2 * 3.14159 / 10) / 2 + 2\")}, index=X.index\n            )\n\n        super().__init__(\n            inputs=[Continuous(\"x\", [0, 10])],\n            outputs=[Continuous(\"y\", [0, 3])],\n            f=f,\n            data=pd.concat([_X, f(_X)], axis=1),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.univariate.Step1D","title":"<code> Step1D            (Problem)         </code>","text":"<p>A discrete step at x=1.1.</p> Source code in <code>opti/problems/univariate.py</code> <pre><code>class Step1D(Problem):\n\"\"\"A discrete step at x=1.1.\"\"\"\n\n    def __init__(self):\n        def f(X: pd.DataFrame) -&gt; pd.DataFrame:\n            return pd.DataFrame({\"y\": X.eval(\"x &gt; 1.1\").astype(float)}, index=X.index)\n\n        super().__init__(\n            inputs=[Continuous(\"x\", [0, 10])],\n            outputs=[Discrete(\"y\", [0, 1])],\n            f=f,\n            data=pd.concat([_X, f(_X)], axis=1),\n        )\n</code></pre>"},{"location":"ref-problems/#opti.problems.zdt","title":"<code>zdt</code>","text":"<p>ZDT benchmark problem suite. All problems are bi-objective, have D continuous inputs and are unconstrained.</p> <p>Zitzler, Deb, Thiele 2000 - Comparison of Multiobjective Evolutionary Algorithms: Empirical Results http://dx.doi.org/10.1162/106365600568202</p>"},{"location":"ref-problems/#opti.problems.zdt.ZDT1","title":"<code> ZDT1            (Problem)         </code>","text":"<p>ZDT-1 benchmark problem.</p> Source code in <code>opti/problems/zdt.py</code> <pre><code>class ZDT1(Problem):\n\"\"\"ZDT-1 benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=30):\n        super().__init__(\n            name=\"ZDT-1 problem\",\n            inputs=[Continuous(f\"x{i+1}\", [0, 1]) for i in range(n_inputs)],\n            outputs=[Continuous(f\"y{i+1}\", [0, np.inf]) for i in range(2)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = X[self.inputs.names[1:]].to_numpy()\n        g = 1 + 9 / (self.n_inputs - 1) * np.sum(x, axis=1)\n        y1 = X[\"x1\"].to_numpy()\n        y2 = g * (1 - (y1 / g) ** 0.5)\n        return pd.DataFrame({\"y1\": y1, \"y2\": y2}, index=X.index)\n\n    def get_optima(self, points=100):\n        x = np.linspace(0, 1, points)\n        y = np.stack([x, 1 - np.sqrt(x)], axis=1)\n        return pd.DataFrame(y, columns=self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.zdt.ZDT2","title":"<code> ZDT2            (Problem)         </code>","text":"<p>ZDT-2 benchmark problem.</p> Source code in <code>opti/problems/zdt.py</code> <pre><code>class ZDT2(Problem):\n\"\"\"ZDT-2 benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=30):\n        super().__init__(\n            name=\"ZDT-2 problem\",\n            inputs=[Continuous(f\"x{i+1}\", [0, 1]) for i in range(n_inputs)],\n            outputs=[Continuous(f\"y{i+1}\", [0, np.inf]) for i in range(2)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = X[self.inputs.names[1:]].to_numpy()\n        g = 1 + 9 / (self.n_inputs - 1) * np.sum(x, axis=1)\n        y1 = X[\"x1\"].to_numpy()\n        y2 = g * (1 - (y1 / g) ** 2)\n        return pd.DataFrame({\"y1\": y1, \"y2\": y2}, index=X.index)\n\n    def get_optima(self, points=100):\n        x = np.linspace(0, 1, points)\n        y = np.stack([x, 1 - np.power(x, 2)], axis=1)\n        return pd.DataFrame(y, columns=self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.zdt.ZDT3","title":"<code> ZDT3            (Problem)         </code>","text":"<p>ZDT-3 benchmark problem.</p> Source code in <code>opti/problems/zdt.py</code> <pre><code>class ZDT3(Problem):\n\"\"\"ZDT-3 benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=30):\n        super().__init__(\n            name=\"ZDT-3 problem\",\n            inputs=[Continuous(f\"x{i+1}\", [0, 1]) for i in range(n_inputs)],\n            outputs=[Continuous(f\"y{i+1}\", [-np.inf, np.inf]) for i in range(2)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = X[self.inputs.names[1:]].to_numpy()\n        g = 1 + 9 / (self.n_inputs - 1) * np.sum(x, axis=1)\n        y1 = X[\"x1\"].to_numpy()\n        y2 = g * (1 - (y1 / g) ** 0.5 - (y1 / g) * np.sin(10 * np.pi * y1))\n        return pd.DataFrame({\"y1\": y1, \"y2\": y2}, index=X.index)\n\n    def get_optima(self, points=100):\n        regions = [\n            [0, 0.0830015349],\n            [0.182228780, 0.2577623634],\n            [0.4093136748, 0.4538821041],\n            [0.6183967944, 0.6525117038],\n            [0.8233317983, 0.8518328654],\n        ]\n\n        pf = []\n        for r in regions:\n            x1 = np.linspace(r[0], r[1], int(points / len(regions)))\n            x2 = 1 - np.sqrt(x1) - x1 * np.sin(10 * np.pi * x1)\n            pf.append(np.stack([x1, x2], axis=1))\n\n        y = np.concatenate(pf, axis=0)\n        return pd.DataFrame(y, columns=self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.zdt.ZDT4","title":"<code> ZDT4            (Problem)         </code>","text":"<p>ZDT-4 benchmark problem.</p> Source code in <code>opti/problems/zdt.py</code> <pre><code>class ZDT4(Problem):\n\"\"\"ZDT-4 benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=10):\n        super().__init__(\n            name=\"ZDT-4 problem\",\n            inputs=[Continuous(\"x1\", [0, 1])]\n            + [Continuous(f\"x{i+1}\", [-5, 5]) for i in range(1, n_inputs)],\n            outputs=[Continuous(f\"y{i+1}\", [0, np.inf]) for i in range(2)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = X[self.inputs.names].to_numpy()\n        g = 1 + 10 * (self.n_inputs - 1)\n        for i in range(1, self.n_inputs):\n            g += x[:, i] ** 2 - 10 * np.cos(4.0 * np.pi * x[:, i])\n        y1 = X[\"x1\"].to_numpy()\n        y2 = g * (1 - np.sqrt(y1 / g))\n        return pd.DataFrame({\"y1\": y1, \"y2\": y2}, index=X.index)\n\n    def get_optima(self, points=100):\n        x = np.linspace(0, 1, points)\n        y = np.stack([x, 1 - np.sqrt(x)], axis=1)\n        return pd.DataFrame(y, columns=self.outputs.names)\n</code></pre>"},{"location":"ref-problems/#opti.problems.zdt.ZDT6","title":"<code> ZDT6            (Problem)         </code>","text":"<p>ZDT-6 benchmark problem.</p> Source code in <code>opti/problems/zdt.py</code> <pre><code>class ZDT6(Problem):\n\"\"\"ZDT-6 benchmark problem.\"\"\"\n\n    def __init__(self, n_inputs=30):\n        super().__init__(\n            name=\"ZDT-6 problem\",\n            inputs=[Continuous(f\"x{i+1}\", [0, 1]) for i in range(n_inputs)],\n            outputs=[Continuous(f\"y{i+1}\", [-np.inf, np.inf]) for i in range(2)],\n        )\n\n    def f(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        x = X[self.inputs.names].to_numpy()\n        n = self.n_inputs\n        g = 1 + 9 * (np.sum(x[:, 1:], axis=1) / (n - 1)) ** 0.25\n        y1 = 1 - np.exp(-4 * x[:, 0]) * (np.sin(6 * np.pi * x[:, 0])) ** 6\n        y2 = g * (1 - (y1 / g) ** 2)\n        return pd.DataFrame({\"y1\": y1, \"y2\": y2}, index=X.index)\n\n    def get_optima(self, points=100):\n        x = np.linspace(0.2807753191, 1, points)\n        y = np.stack([x, 1 - x**2], axis=1)\n        return pd.DataFrame(y, columns=self.outputs.names)\n</code></pre>"},{"location":"ref-sampling/","title":"Sampling","text":""},{"location":"ref-sampling/#opti.sampling.base","title":"<code>base</code>","text":""},{"location":"ref-sampling/#opti.sampling.base.apply_nchoosek","title":"<code>apply_nchoosek(samples, constraint)</code>","text":"<p>Apply an n-choose-k constraint in-place</p> Source code in <code>opti/sampling/base.py</code> <pre><code>def apply_nchoosek(samples: pd.DataFrame, constraint: NChooseK):\n\"\"\"Apply an n-choose-k constraint in-place\"\"\"\n    n_zeros = len(constraint.names) - constraint.max_active\n    for i in samples.index:\n        s = np.random.choice(constraint.names, size=n_zeros, replace=False)\n        samples.loc[i, s] = 0\n</code></pre>"},{"location":"ref-sampling/#opti.sampling.base.constrained_sampling","title":"<code>constrained_sampling(n_samples, parameters, constraints)</code>","text":"<p>Uniform sampling from a constrained space.</p> Source code in <code>opti/sampling/base.py</code> <pre><code>def constrained_sampling(\n    n_samples: int, parameters: Parameters, constraints: Constraints\n) -&gt; pd.DataFrame:\n\"\"\"Uniform sampling from a constrained space.\"\"\"\n    nchoosek_constraints, other_constraints = split_nchoosek(constraints)\n\n    try:\n        samples = rejection_sampling(n_samples, parameters, other_constraints)\n    except Exception:\n        samples = polytope_sampling(n_samples, parameters, other_constraints)\n\n    if len(nchoosek_constraints) == 0:\n        return samples\n\n    for c in nchoosek_constraints:\n        apply_nchoosek(samples, c)\n    # check if other constraints are still satisfied\n    if not constraints.satisfied(samples).all():\n        raise Exception(\n            \"Applying the n-choose-k constraint(s) violated another constraint.\"\n        )\n    return samples\n</code></pre>"},{"location":"ref-sampling/#opti.sampling.base.rejection_sampling","title":"<code>rejection_sampling(n_samples, parameters, constraints, max_iters=1000)</code>","text":"<p>Uniformly distributed samples from a constrained space via rejection sampling.</p> Source code in <code>opti/sampling/base.py</code> <pre><code>def rejection_sampling(\n    n_samples: int,\n    parameters: Parameters,\n    constraints: Constraints,\n    max_iters: int = 1000,\n) -&gt; pd.DataFrame:\n\"\"\"Uniformly distributed samples from a constrained space via rejection sampling.\"\"\"\n    if constraints is None:\n        return parameters.sample(n_samples)\n\n    # check for equality constraints in combination with continuous parameters\n    for c in constraints:\n        if isinstance(c, (LinearEquality, NonlinearEquality)):\n            for p in parameters:\n                if isinstance(p, Continuous):\n                    raise Exception(\n                        \"Rejection sampling doesn't work for equality constraints over continuous variables.\"\n                    )\n\n    n_iters = 0\n    n_found = 0\n    points_found = []\n    while n_found &lt; n_samples:\n        n_iters += 1\n        if n_iters &gt; max_iters:\n            raise Exception(\"Maximum iterations exceeded in rejection sampling\")\n        points = parameters.sample(10000)\n        valid = constraints.satisfied(points)\n        n_found += np.sum(valid)\n        points_found.append(points[valid])\n\n    return pd.concat(points_found, ignore_index=True).iloc[:n_samples]\n</code></pre>"},{"location":"ref-sampling/#opti.sampling.base.sobol_sampling","title":"<code>sobol_sampling(n_samples, parameters)</code>","text":"<p>Super-uniform sampling from an unconstrained space using a Sobol sequence.</p> Source code in <code>opti/sampling/base.py</code> <pre><code>def sobol_sampling(n_samples: int, parameters: Parameters) -&gt; pd.DataFrame:\n\"\"\"Super-uniform sampling from an unconstrained space using a Sobol sequence.\"\"\"\n    d = len(parameters)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        X = Sobol(d).random(n_samples)\n    res = []\n    for i, p in enumerate(parameters):\n        if isinstance(p, Continuous):\n            x = p.from_unit_range(X[:, i])\n        else:\n            bins = np.linspace(0, 1, len(p.domain) + 1)\n            idx = np.digitize(X[:, i], bins) - 1\n            x = np.array(p.domain)[idx]\n        res.append(pd.Series(x, name=p.name))\n    return pd.concat(res, axis=1)\n</code></pre>"},{"location":"ref-sampling/#opti.sampling.base.split_nchoosek","title":"<code>split_nchoosek(constraints)</code>","text":"<p>Split constraints in n-choose-k constraint and all other constraints.</p> Source code in <code>opti/sampling/base.py</code> <pre><code>def split_nchoosek(\n    constraints: Optional[Constraints],\n) -&gt; Tuple[Constraints, Constraints]:\n\"\"\"Split constraints in n-choose-k constraint and all other constraints.\"\"\"\n    if constraints is None:\n        return Constraints([]), Constraints([])\n    nchoosek_constraints = Constraints(\n        [c for c in constraints if isinstance(c, NChooseK)]\n    )\n    other_constraints = Constraints(\n        [c for c in constraints if not isinstance(c, NChooseK)]\n    )\n    return nchoosek_constraints, other_constraints\n</code></pre>"},{"location":"ref-sampling/#opti.sampling.polytope","title":"<code>polytope</code>","text":"<p>This module provides functions to uniformly sample points subject to a system of linear inequality constraints, :math:<code>Ax &lt;= b</code> (convex polytope), and linear equality constraints, :math:<code>Ax = b</code> (affine projection).</p> <p>A comparison of MCMC algorithms to generate uniform samples over a convex polytope is given in [Chen2018]. Here, we use the Hit &amp; Run algorithm described in [Smith1984]. The R-package <code>hitandrun</code>_ provides similar functionality to this module.</p>"},{"location":"ref-sampling/#opti.sampling.polytope--references","title":"References","text":"<p>.. [Chen2018] Chen Y., Dwivedi, R., Wainwright, M., Yu B. (2018) Fast MCMC Sampling     Algorithms on Polytopes. JMLR, 19(55):1\u221286     https://arxiv.org/abs/1710.08165 .. [Smith1984] Smith, R. (1984). Efficient Monte Carlo Procedures for Generating     Points Uniformly Distributed Over Bounded Regions. Operations Research,     32(6), 1296-1308.     www.jstor.org/stable/170949 .. _<code>hitandrun</code>: https://cran.r-project.org/web/packages/hitandrun/index.html</p>"},{"location":"ref-sampling/#opti.sampling.polytope.polytope_sampling","title":"<code>polytope_sampling(n_samples, parameters, constraints, thin=100)</code>","text":"<p>Hit-and-run method to sample uniformly under linear constraints.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples.</p> required <code>parameters</code> <code>opti.Parameters</code> <p>Parameter space.</p> required <code>constraints</code> <code>opti.Constraints</code> <p>Constraints on the parameters.</p> required <code>thin</code> <code>int</code> <p>Thinning factor of the generated samples.</p> <code>100</code> <p>Returns:</p> Type Description <code>array, shape=(n_samples, dimension)</code> <p>Randomly sampled points.</p> Source code in <code>opti/sampling/polytope.py</code> <pre><code>def polytope_sampling(\n    n_samples: int, parameters: Parameters, constraints: Constraints, thin: int = 100\n) -&gt; pd.DataFrame:\n\"\"\"Hit-and-run method to sample uniformly under linear constraints.\n\n    Args:\n        n_samples (int): Number of samples.\n        parameters (opti.Parameters): Parameter space.\n        constraints (opti.Constraints): Constraints on the parameters.\n        thin (int, optional): Thinning factor of the generated samples.\n\n    Returns:\n        array, shape=(n_samples, dimension): Randomly sampled points.\n    \"\"\"\n    for c in constraints:\n        if not isinstance(c, (LinearEquality, LinearInequality)):\n            raise Exception(\"Polytope sampling only works for linear constraints.\")\n\n    At, bt, N, xp = _get_AbNx(parameters, constraints)\n\n    # hit &amp; run sampling\n    x0 = _chebyshev_center(At, bt)\n    sampler = _hitandrun(At, bt, x0)\n    X = np.empty((n_samples, At.shape[1]))\n    for i in range(n_samples):\n        for _ in range(thin - 1):\n            next(sampler)\n        X[i] = next(sampler)\n\n    # project back\n    X = X @ N.T + xp\n    return pd.DataFrame(columns=parameters.names, data=X)\n</code></pre>"},{"location":"ref-sampling/#opti.sampling.simplex","title":"<code>simplex</code>","text":""},{"location":"ref-sampling/#opti.sampling.simplex.grid","title":"<code>grid(dimension, levels)</code>","text":"<p>Construct a regular grid on the unit simplex.</p> <p>The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels.</p> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>Number of variables.</p> required <code>levels</code> <code>int</code> <p>Number of levels for each variable.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Regularily spaced points on the unit simplex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simplex_grid(3, 3)\narray([\n    [0. , 0. , 1. ],\n    [0. , 0.5, 0.5],\n    [0. , 1. , 0. ],\n    [0.5, 0. , 0.5],\n    [0.5, 0.5, 0. ],\n    [1. , 0. , 0. ]\n])\n</code></pre> <p>References</p> <p>Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978.</p> Source code in <code>opti/sampling/simplex.py</code> <pre><code>def grid(dimension: int, levels: int) -&gt; np.ndarray:\n\"\"\"Construct a regular grid on the unit simplex.\n\n    The number of grid points is L = (n+m-1)!/(n!*(m-1)!)\n    where m is the dimension and n+1 is the number of levels.\n\n    Args:\n        dimension (int): Number of variables.\n        levels (int): Number of levels for each variable.\n\n    Returns:\n       array: Regularily spaced points on the unit simplex.\n\n    Examples:\n        &gt;&gt;&gt; simplex_grid(3, 3)\n        array([\n            [0. , 0. , 1. ],\n            [0. , 0.5, 0.5],\n            [0. , 1. , 0. ],\n            [0.5, 0. , 0.5],\n            [0.5, 0.5, 0. ],\n            [1. , 0. , 0. ]\n        ])\n\n    References:\n        Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978.\n    \"\"\"\n    m = dimension\n    n = levels - 1\n    L = int(comb(dimension - 1 + levels - 1, dimension - 1, exact=True))\n\n    x = np.zeros(m, dtype=int)\n    x[-1] = n\n\n    out = np.empty((L, m), dtype=int)\n    out[0] = x\n\n    h = m\n    for i in range(1, L):\n        h -= 1\n\n        val = x[h]\n        x[h] = 0\n        x[h - 1] += 1\n        x[-1] = val - 1\n\n        if val != 1:\n            h = m\n\n        out[i] = x\n\n    return out / n\n</code></pre>"},{"location":"ref-sampling/#opti.sampling.simplex.sample","title":"<code>sample(dimension, n_samples=1)</code>","text":"<p>Sample uniformly from the unit simplex.</p> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>Number of dimensions.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to draw.</p> <code>1</code> <p>Returns:</p> Type Description <code>array, shape=(n_samples, dimesnion)</code> <p>Random samples from the unit simplex.</p> Source code in <code>opti/sampling/simplex.py</code> <pre><code>def sample(dimension: int, n_samples: int = 1) -&gt; np.ndarray:\n\"\"\"Sample uniformly from the unit simplex.\n\n    Args:\n        dimension (int): Number of dimensions.\n        n_samples (int): Number of samples to draw.\n\n    Returns:\n        array, shape=(n_samples, dimesnion): Random samples from the unit simplex.\n    \"\"\"\n    s = np.random.standard_exponential((n_samples, dimension))\n    return (s.T / s.sum(axis=1)).T\n</code></pre>"},{"location":"ref-sampling/#opti.sampling.sphere","title":"<code>sphere</code>","text":""},{"location":"ref-sampling/#opti.sampling.sphere.sample","title":"<code>sample(dimension, n_samples=1, positive=False)</code>","text":"<p>Sample uniformly from the unit hypersphere.</p> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>Number of dimensions.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to draw.</p> <code>1</code> <code>positive</code> <code>bool</code> <p>Sample from the non-negative unit-sphere.</p> <code>False</code> <p>Returns:</p> Type Description <code>array, shape=(n_samples, dimesnion)</code> <p>Random samples from the unit simplex.</p> Source code in <code>opti/sampling/sphere.py</code> <pre><code>def sample(dimension: int, n_samples: int = 1, positive: bool = False) -&gt; np.ndarray:\n\"\"\"Sample uniformly from the unit hypersphere.\n\n    Args:\n        dimension (int): Number of dimensions.\n        n_samples (int): Number of samples to draw.\n        positive (bool): Sample from the non-negative unit-sphere.\n\n    Returns:\n        array, shape=(n_samples, dimesnion): Random samples from the unit simplex.\n    \"\"\"\n    x = np.random.normal(0, 1, size=(n_samples, dimension))\n    x = x / np.sum(x**2, axis=1, keepdims=True) ** 0.5\n    if positive:\n        x *= -2 * (x &lt; 0) + 1\n    return x\n</code></pre>"},{"location":"ref-tools/","title":"Tools","text":""},{"location":"ref-tools/#opti.tools.modde","title":"<code>modde</code>","text":""},{"location":"ref-tools/#opti.tools.modde.MipFile","title":"<code> MipFile        </code>","text":"<p>File reader for MODDE investigation files (.mip)</p> Source code in <code>opti/tools/modde.py</code> <pre><code>class MipFile:\n\"\"\"File reader for MODDE investigation files (.mip)\"\"\"\n\n    def __init__(self, filename):\n\"\"\"Read in a MODDE file.\n\n        Args:\n            filename (str or path): path to MODDE file\n        \"\"\"\n        s = b\"\".join(open(filename, \"rb\").readlines())\n\n        # Split into blocks of 1024 bytes\n        num_blocks = len(s) // 1024\n        assert len(s) % 1024 == 0\n        blocks = [s[i * 1024 : (i + 1) * 1024] for i in range(num_blocks)]\n\n        # Split blocks into header (first 21 bytes) and body\n        headers = [b[0:21] for b in blocks]\n        data = [b[21:] for b in blocks]\n\n        # Parse the block headers to create a mapping {block_index: next_block_index}\n        block_order = {}\n        for i, header in enumerate(headers):\n            i_next = np.where([h.startswith(header[15:19]) for h in headers])[0]\n            block_order[i] = i_next[0] if len(i_next) == 1 else None\n\n        # Join all blocks that belong together and decode to UTF-8\n        self.parts = []\n        while len(block_order) &gt; 0:\n            i = next(iter(block_order))  # get first key in ordered dict\n            s = b\"\"\n            while i is not None:\n                s += data[i]\n                i = block_order.pop(i)\n            s = re.sub(b\"\\r|\\x00\", b\"\", s).decode(\"utf-8\", errors=\"ignore\")\n            self.parts.append(s)\n\n        self.settings = self._get_design_settings()\n        self.factors = {k: self.settings[k] for k in self.settings[\"Factors\"]}\n        self.responses = {k: self.settings[k] for k in self.settings[\"Responses\"]}\n        self.data = self._get_experimental_data()\n\n    def _get_experimental_data(self):\n\"\"\"Parse the experimental data.\n\n        Returns:\n            pd.DataFrame: dataframe of experimental data\n        \"\"\"\n        part = [p for p in self.parts if p.startswith(\"ExpNo\")][0]\n        return pd.read_csv(io.StringIO(part), delimiter=\"\\t\", index_col=\"ExpNo\")\n\n    def _get_design_settings(self):\n\"\"\"Parse the design settings.\n\n        Returns:\n            dict of dicts: dictionary with 'Factors', 'Responses', 'Options' as well as the individual variables.\n        \"\"\"\n        part = [p for p in self.parts if p.startswith(\"[Status]\")][0]\n        settings = {}\n        for line in part.split(\"\\n\"):\n            if len(line) == 0:\n                continue\n            if line.startswith(\"[\"):\n                thing = line.strip(\"[]\")\n                settings[thing] = {}\n            else:\n                key, value = line.split(\"=\")\n                settings[thing][key] = value\n        return settings\n</code></pre>"},{"location":"ref-tools/#opti.tools.modde.MipFile.__init__","title":"<code>__init__(self, filename)</code>  <code>special</code>","text":"<p>Read in a MODDE file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str or path</code> <p>path to MODDE file</p> required Source code in <code>opti/tools/modde.py</code> <pre><code>def __init__(self, filename):\n\"\"\"Read in a MODDE file.\n\n    Args:\n        filename (str or path): path to MODDE file\n    \"\"\"\n    s = b\"\".join(open(filename, \"rb\").readlines())\n\n    # Split into blocks of 1024 bytes\n    num_blocks = len(s) // 1024\n    assert len(s) % 1024 == 0\n    blocks = [s[i * 1024 : (i + 1) * 1024] for i in range(num_blocks)]\n\n    # Split blocks into header (first 21 bytes) and body\n    headers = [b[0:21] for b in blocks]\n    data = [b[21:] for b in blocks]\n\n    # Parse the block headers to create a mapping {block_index: next_block_index}\n    block_order = {}\n    for i, header in enumerate(headers):\n        i_next = np.where([h.startswith(header[15:19]) for h in headers])[0]\n        block_order[i] = i_next[0] if len(i_next) == 1 else None\n\n    # Join all blocks that belong together and decode to UTF-8\n    self.parts = []\n    while len(block_order) &gt; 0:\n        i = next(iter(block_order))  # get first key in ordered dict\n        s = b\"\"\n        while i is not None:\n            s += data[i]\n            i = block_order.pop(i)\n        s = re.sub(b\"\\r|\\x00\", b\"\", s).decode(\"utf-8\", errors=\"ignore\")\n        self.parts.append(s)\n\n    self.settings = self._get_design_settings()\n    self.factors = {k: self.settings[k] for k in self.settings[\"Factors\"]}\n    self.responses = {k: self.settings[k] for k in self.settings[\"Responses\"]}\n    self.data = self._get_experimental_data()\n</code></pre>"},{"location":"ref-tools/#opti.tools.modde.read_modde","title":"<code>read_modde(filepath)</code>","text":"<p>Read a problem specification from a MODDE file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>path-like</code> <p>path to MODDE .mip file</p> required <p>Returns:</p> Type Description <code>opti.Problem</code> <p>problem specification</p> Source code in <code>opti/tools/modde.py</code> <pre><code>def read_modde(filepath):\n\"\"\"Read a problem specification from a MODDE file.\n\n    Args:\n        filepath (path-like): path to MODDE .mip file\n\n    Returns:\n        opti.Problem: problem specification\n    \"\"\"\n    mip = MipFile(filepath)\n\n    inputs = []\n    outputs = []\n    constraints = []\n    formulation_parameters = []\n\n    # build input space\n    for name, props in mip.factors.items():\n        if props[\"Use\"] == \"Uncontrolled\":\n            print(f\"Uncontrolled factors not supported. Skipping {name}\")\n            continue\n        if props[\"Type\"] == \"Formulation\":\n            domain = [float(s) for s in props[\"Settings\"].split(\",\")]\n            inputs.append(opti.Continuous(name=name, domain=domain))\n            formulation_parameters.append(name)\n        elif props[\"Type\"] == \"Quantitative\":\n            domain = [float(s) for s in props[\"Settings\"].split(\",\")]\n            inputs.append(opti.Continuous(name=name, domain=domain))\n        elif props[\"Type\"] == \"Multilevel\":\n            domain = [float(s) for s in props[\"Settings\"].split(\",\")]\n            inputs.append(opti.Discrete(name=name, domain=domain))\n        elif props[\"Type\"] == \"Qualitative\":\n            domain = props[\"Settings\"].split(\",\")\n            inputs.append(opti.Categorical(name=name, domain=domain))\n    inputs = opti.Parameters(inputs)\n\n    # build formulation constraint\n    constraints.append(\n        opti.constraint.LinearEquality(\n            names=formulation_parameters,\n            lhs=np.ones(len(formulation_parameters)),\n            rhs=1,\n        )\n    )\n\n    # build output space\n    for name, props in mip.responses.items():\n        # check if data available that allows to infer the domain\n        vmin = mip.data[name].min()\n        vmax = mip.data[name].max()\n        if np.isfinite(vmin) or np.isfinite(vmax) and vmin &lt; vmax:\n            domain = [vmin, vmax]\n        else:\n            domain = [0, 1]\n        dim = opti.Continuous(name=name, domain=domain)\n        outputs.append(dim)\n    outputs = opti.Parameters(outputs)\n\n    # data\n    data = mip.data.drop(columns=[\"ExpName\", \"InOut\"])\n\n    return opti.Problem(\n        inputs=inputs, outputs=outputs, constraints=constraints, data=data\n    )\n</code></pre>"},{"location":"ref-tools/#opti.tools.noisify","title":"<code>noisify</code>","text":""},{"location":"ref-tools/#opti.tools.noisify.noisify_problem","title":"<code>noisify_problem(problem, noisifiers)</code>","text":"<p>Creates a new problem that is based on the given one plus noise on the outputs.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem</code> <p>given problem where we will add noise</p> required <code>noisifiers</code> <code>List[Callable]</code> <p>list of functions that add noise to the outputs</p> required <p>Returns: new problem with noise on the output</p> Source code in <code>opti/tools/noisify.py</code> <pre><code>def noisify_problem(\n    problem: Problem,\n    noisifiers: List[Callable],\n) -&gt; Problem:\n\"\"\"Creates a new problem that is based on the given one plus noise on the outputs.\n\n    Args:\n        problem: given problem where we will add noise\n        noisifiers: list of functions that add noise to the outputs\n\n    Returns: new problem with noise on the output\n    \"\"\"\n\n    def noisy_f(X):\n        return _add_noise_to_data(problem.f(X), noisifiers, problem.outputs)\n\n    if problem.data is not None:\n        data = problem.get_data()\n        X = data[problem.inputs.names]\n        Yn = _add_noise_to_data(\n            data[problem.outputs.names], noisifiers, problem.outputs\n        )\n        data = pd.concat([X, Yn], axis=1)\n    else:\n        data = None\n\n    return Problem(\n        inputs=problem.inputs,\n        outputs=problem.outputs,\n        objectives=problem.objectives,\n        constraints=problem.constraints,\n        output_constraints=problem.output_constraints,\n        f=noisy_f,\n        data=data,\n        name=problem.name,\n    )\n</code></pre>"},{"location":"ref-tools/#opti.tools.noisify.noisify_problem_with_gaussian","title":"<code>noisify_problem_with_gaussian(problem, mu=0, sigma=0.05)</code>","text":"<p>Given an instance of a problem, this returns the problem with additive Gaussian noise</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem</code> <p>problem instance where we add the noise</p> required <code>mu</code> <code>float</code> <p>mean of the Gaussian noise to be added</p> <code>0</code> <code>sigma</code> <code>float</code> <p>standard deviation of the Gaussian noise to be added</p> <code>0.05</code> <p>Returns: input problem with additive Gaussian noise</p> Source code in <code>opti/tools/noisify.py</code> <pre><code>def noisify_problem_with_gaussian(problem: Problem, mu: float = 0, sigma: float = 0.05):\n\"\"\"\n    Given an instance of a problem, this returns the problem with additive Gaussian noise\n    Args:\n        problem: problem instance where we add the noise\n        mu: mean of the Gaussian noise to be added\n        sigma: standard deviation of the Gaussian noise to be added\n\n    Returns: input problem with additive Gaussian noise\n    \"\"\"\n\n    def noisify(y):\n        rv = norm(loc=mu, scale=sigma)\n        return y + rv.rvs(len(y))\n\n    return noisify_problem(problem, noisifiers=[noisify] * len(problem.outputs))\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce","title":"<code>reduce</code>","text":""},{"location":"ref-tools/#opti.tools.reduce.AffineTransform","title":"<code> AffineTransform        </code>","text":"Source code in <code>opti/tools/reduce.py</code> <pre><code>class AffineTransform:\n    def __init__(self, equalities):\n        self.equalities = equalities\n\n    def augment_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Restore eliminated parameters in a dataframe.\"\"\"\n        data = data.copy()\n        for name_lhs, names_rhs, coeffs in self.equalities:\n            data[name_lhs] = coeffs[-1]\n            for i, name in enumerate(names_rhs):\n                data[name_lhs] += coeffs[i] * data[name]\n        return data\n\n    def drop_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Drop eliminated parameters from a DataFrame.\"\"\"\n        drop = []\n        for name_lhs, _, _ in self.equalities:\n            if name_lhs in data.columns:\n                drop.append(name_lhs)\n        return data.drop(columns=drop)\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.AffineTransform.augment_data","title":"<code>augment_data(self, data)</code>","text":"<p>Restore eliminated parameters in a dataframe.</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def augment_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Restore eliminated parameters in a dataframe.\"\"\"\n    data = data.copy()\n    for name_lhs, names_rhs, coeffs in self.equalities:\n        data[name_lhs] = coeffs[-1]\n        for i, name in enumerate(names_rhs):\n            data[name_lhs] += coeffs[i] * data[name]\n    return data\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.AffineTransform.drop_data","title":"<code>drop_data(self, data)</code>","text":"<p>Drop eliminated parameters from a DataFrame.</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def drop_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Drop eliminated parameters from a DataFrame.\"\"\"\n    drop = []\n    for name_lhs, _, _ in self.equalities:\n        if name_lhs in data.columns:\n            drop.append(name_lhs)\n    return data.drop(columns=drop)\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.check_existence_of_solution","title":"<code>check_existence_of_solution(A_aug)</code>","text":"<p>Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def check_existence_of_solution(A_aug):\n\"\"\"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.\"\"\"\n    A = A_aug[:, :-1]\n    b = A_aug[:, -1]\n    len_inputs = np.shape(A)[1]\n\n    # catch special cases\n    rk_A_aug = np.linalg.matrix_rank(A_aug)\n    rk_A = np.linalg.matrix_rank(A)\n\n    if rk_A == rk_A_aug:\n        if rk_A &lt; len_inputs:\n            return  # all good\n        else:\n            x = np.linalg.solve(A, b)\n            raise Exception(\n                f\"There is a unique solution x for the linear equality constraints: x={x}\"\n            )\n    elif rk_A &lt; rk_A_aug:\n        raise Exception(\n            \"There is no solution fulfilling the linear equality constraints.\"\n        )\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.check_problem_for_reduction","title":"<code>check_problem_for_reduction(problem)</code>","text":"<p>Check if the reduction can be applied or if a trivial case is present.</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def check_problem_for_reduction(problem: Problem) -&gt; bool:\n\"\"\"Check if the reduction can be applied or if a trivial case is present.\"\"\"\n    # are there any constraints?\n    if problem.constraints is None:\n        return False\n\n    # are there any linear equality constraints?\n    linear_equalities, _ = find_linear_equalities(problem.constraints)\n    if len(linear_equalities) == 0:\n        return False\n\n    # are there continuous inputs\n    continuous_inputs, _ = find_continuous_inputs(problem.inputs)\n    if len(continuous_inputs) == 0:\n        return False\n\n    # check that equality constraints only contain continuous inputs\n    for c in linear_equalities:\n        for name in c.names:\n            if name not in continuous_inputs.names:\n                raise Exception(\n                    f\"Linear equality constraint {c} contains a non-continuous parameter. Problem reduction is not supported.\"\n                )\n    return True\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.find_continuous_inputs","title":"<code>find_continuous_inputs(inputs)</code>","text":"<p>Separate parameters into continuous and all other parameters.</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def find_continuous_inputs(inputs: Parameters) -&gt; Tuple[Parameters, Parameters]:\n\"\"\"Separate parameters into continuous and all other parameters.\"\"\"\n    continous_inputs = [p for p in inputs if isinstance(p, Continuous)]\n    other_inputs = [p for p in inputs if not isinstance(p, Continuous)]\n    return Parameters(continous_inputs), Parameters(other_inputs)\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.find_linear_equalities","title":"<code>find_linear_equalities(constraints)</code>","text":"<p>Separate constraints into linear equalities and all other constraints.</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def find_linear_equalities(constraints: Constraints) -&gt; Tuple[Constraints, Constraints]:\n\"\"\"Separate constraints into linear equalities and all other constraints.\"\"\"\n    linear_equalities = [c for c in constraints if isinstance(c, LinearEquality)]\n    other_constraints = [c for c in constraints if not isinstance(c, LinearEquality)]\n    return Constraints(linear_equalities), Constraints(other_constraints)\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.reduce_problem","title":"<code>reduce_problem(problem)</code>","text":"<p>Reduce a problem with linear constraints to a subproblem where linear equality constraints are eliminated.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem</code> <p>problem to be reduced</p> required <p>Returns:</p> Type Description <code>Tuple[opti.problem.Problem, opti.tools.reduce.AffineTransform]</code> <p>(problem, trafo). Problem is the reduced problem where linear equality constraints have been eliminated. trafo is the according transformation.</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def reduce_problem(problem: Problem) -&gt; Tuple[Problem, AffineTransform]:\n\"\"\"Reduce a problem with linear constraints to a subproblem where linear equality constraints are eliminated.\n\n    Args:\n        problem (Problem): problem to be reduced\n\n    Returns:\n        (problem, trafo). Problem is the reduced problem where linear equality constraints\n        have been eliminated. trafo is the according transformation.\n    \"\"\"\n    # check if the problem can be reduced\n    if not check_problem_for_reduction(problem):\n        return problem, AffineTransform([])\n\n    # find linear equality constraints\n    linear_equalities, other_constraints = find_linear_equalities(problem.constraints)\n\n    # only consider continuous inputs\n    continuous_inputs, other_inputs = find_continuous_inputs(problem.inputs)\n\n    # assemble Matrix A from equality constraints\n    N = len(linear_equalities)\n    M = len(continuous_inputs) + 1\n    names = np.concatenate((continuous_inputs.names, [\"rhs\"]))\n\n    A_aug = pd.DataFrame(data=np.zeros(shape=(N, M)), columns=names)\n\n    for i in range(len(linear_equalities)):\n        c = linear_equalities[i]\n\n        A_aug.loc[i, c.names] = c.lhs\n        A_aug.loc[i, \"rhs\"] = c.rhs\n    A_aug = A_aug.values\n\n    # catch special cases\n    check_existence_of_solution(A_aug)\n\n    # bring A_aug to reduced row-echelon form\n    A_aug_rref, pivots = rref(A_aug)\n    pivots = np.array(pivots)\n    A_aug_rref = np.array(A_aug_rref).astype(np.float64)\n\n    # formulate box bounds as linear inequality constraints in matrix form\n    B = np.zeros(shape=(2 * (M - 1), M))\n    B[: M - 1, : M - 1] = np.eye(M - 1)\n    B[M - 1 :, : M - 1] = -np.eye(M - 1)\n\n    B[: M - 1, -1] = continuous_inputs.bounds.loc[\"max\"].copy()\n    B[M - 1 :, -1] = -continuous_inputs.bounds.loc[\"min\"].copy()\n\n    # eliminate columns with pivot element\n    for i in range(len(pivots)):\n        p = pivots[i]\n        B[p, :] -= A_aug_rref[i, :]\n        B[p + M - 1, :] += A_aug_rref[i, :]\n\n    # build up reduced problem\n    _inputs = list(other_inputs.parameters.values())\n    for i in range(len(continuous_inputs)):\n        # add all inputs that were not eliminated\n        if i not in pivots:\n            _inputs.append(continuous_inputs[names[i]])\n    _inputs = Parameters(_inputs)\n\n    _constraints = other_constraints.constraints\n    for i in pivots:\n        # reduce equation system of upper bounds\n        ind = np.where(B[i, :-1] != 0)[0]\n        if len(ind) &gt; 0 and B[i, -1] &lt; np.inf:\n            c = LinearInequality(names=list(names[ind]), lhs=B[i, ind], rhs=B[i, -1])\n            _constraints.append(c)\n        else:\n            if B[i, -1] &lt; -1e-16:\n                raise Exception(\"There is no solution that fulfills the constraints.\")\n\n        # reduce equation system of lower bounds\n        ind = np.where(B[i + M - 1, :-1] != 0)[0]\n        if len(ind) &gt; 0 and B[i + M - 1, -1] &lt; np.inf:\n            c = LinearInequality(\n                names=list(names[ind]), lhs=B[i + M - 1, ind], rhs=B[i + M - 1, -1]\n            )\n            _constraints.append(c)\n        else:\n            if B[i + M - 1, -1] &lt; -1e-16:\n                raise Exception(\"There is no solution that fulfills the constraints.\")\n\n    _constraints = Constraints(_constraints)\n\n    # assemble equalities\n    _equalities = []\n    for i in range(len(pivots)):\n        name_lhs = names[pivots[i]]\n        names_rhs = []\n        coeffs = []\n\n        for j in range(len(names) - 1):\n            if A_aug_rref[i, j] != 0 and j != pivots[i]:\n                coeffs.append(-A_aug_rref[i, j])\n                names_rhs.append(names[j])\n\n        coeffs.append(A_aug_rref[i, -1])\n\n        _equalities.append([name_lhs, names_rhs, coeffs])\n\n    _data = problem.data\n    trafo = AffineTransform(_equalities)\n\n    _models = problem.models\n    if _models is not None:\n        warnings.warn(\"Models are currently not adapted in reduce_problem.\")\n\n    if hasattr(problem, \"f\") and problem.f is not None:\n\n        def _f(X: pd.DataFrame) -&gt; pd.DataFrame:\n            return problem.f(trafo.augment_data(X))\n\n    else:\n        _f = None\n\n    _problem = Problem(\n        inputs=_inputs,\n        outputs=deepcopy(problem.outputs),\n        objectives=deepcopy(problem.objectives),\n        constraints=deepcopy(_constraints),\n        f=_f,\n        models=_models,\n        data=_data,\n        optima=deepcopy(problem.optima),\n        name=deepcopy(problem.name),\n    )\n\n    # remove remaining dependencies of eliminated inputs from the problem\n    _problem = remove_eliminated_inputs(_problem, trafo)\n    return _problem, trafo\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.remove_eliminated_inputs","title":"<code>remove_eliminated_inputs(problem, transform)</code>","text":"<p>Eliminates remaining occurences of eliminated inputs in linear constraints.</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def remove_eliminated_inputs(problem: Problem, transform: AffineTransform) -&gt; Problem:\n\"\"\"Eliminates remaining occurences of eliminated inputs in linear constraints.\"\"\"\n    inputs_names = problem.inputs.names\n    M = len(inputs_names)\n\n    # write the equalities for the backtransformation into one matrix\n    inputs_dict = {inputs_names[i]: i for i in range(M)}\n\n    # build up dict from problem.equalities e.g. {\"xi1\": [coeff(xj1), ..., coeff(xjn)], ... \"xik\":...}\n    coeffs_dict = {}\n    for i, e in enumerate(transform.equalities):\n        coeffs = np.zeros(M + 1)\n        for j, name in enumerate(e[1]):\n            coeffs[inputs_dict[name]] = e[2][j]\n        coeffs[-1] = e[2][-1]\n        coeffs_dict[e[0]] = coeffs\n\n    constraints = []\n    for c in problem.constraints:\n        # Nonlinear constraints supported\n        if not isinstance(c, (LinearEquality, LinearInequality)):\n            raise Exception(\n                \"Elimination of variables is only supported for LinearEquality and LinearInequality constraints.\"\n            )\n\n        # no changes, if the constraint does not contain eliminated inputs\n        elif all(name in inputs_names for name in c.names):\n            constraints.append(c)\n\n        # remove inputs from the constraint that were eliminated from the inputs before\n        else:\n            _names = np.array(inputs_names)\n            _rhs = c.rhs\n\n            # create new lhs and rhs from the old one and knowledge from problem._equalities\n            _lhs = np.zeros(M)\n            for j, name in enumerate(c.names):\n                if name in inputs_names:\n                    _lhs[inputs_dict[name]] += c.lhs[j]\n                else:\n                    _lhs += c.lhs[j] * coeffs_dict[name][:-1]\n                    _rhs -= c.lhs[j] * coeffs_dict[name][-1]\n\n            _names = _names[np.abs(_lhs) &gt; 1e-16]\n            _lhs = _lhs[np.abs(_lhs) &gt; 1e-16]\n\n            # create new Constraints\n            if isinstance(c, LinearEquality):\n                _c = LinearEquality(_names, _lhs, _rhs)\n            else:\n                _c = LinearInequality(_names, _lhs, _rhs)\n\n            # check if constraint is always fulfilled/not fulfilled\n            if len(_c.names) == 0 and _c.rhs &gt;= 0:\n                pass\n            elif len(_c.names) == 0 and _c.rhs &lt; 0:\n                raise Exception(\"Linear constraints cannot be fulfilled.\")\n            elif np.isinf(_c.rhs):\n                pass\n            else:\n                constraints.append(_c)\n    problem.constraints = Constraints(constraints)\n\n    return problem\n</code></pre>"},{"location":"ref-tools/#opti.tools.reduce.rref","title":"<code>rref(A, tol=1e-08)</code>","text":"<p>Computes the reduced row echelon form of a Matrix</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>2d array representing a matrix.</p> required <code>tol</code> <code>float</code> <p>tolerance for rounding to 0</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tuple[numpy.ndarray, List[int]]</code> <p>(A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref</p> Source code in <code>opti/tools/reduce.py</code> <pre><code>def rref(A: np.ndarray, tol=1e-8) -&gt; Tuple[np.ndarray, List[int]]:\n\"\"\"Computes the reduced row echelon form of a Matrix\n\n    Args:\n        A (ndarray): 2d array representing a matrix.\n        tol (float): tolerance for rounding to 0\n\n    Returns:\n        (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots\n        is a numpy array containing the pivot columns of A_rref\n    \"\"\"\n    A = np.array(A, dtype=np.float64)\n    n, m = np.shape(A)\n\n    col = 0\n    row = 0\n    pivots = []\n\n    for col in range(m):\n        # does a pivot element exist?\n        if all(np.abs(A[row:, col]) &lt; tol):\n            pass\n        # if yes: start elimination\n        else:\n            pivots.append(col)\n            max_row = np.argmax(np.abs(A[row:, col])) + row\n            # switch to most stable row\n            A[[row, max_row], :] = A[[max_row, row], :]\n            # normalize row\n            A[row, :] /= A[row, col]\n            # eliminate other elements from column\n            for r in range(n):\n                if r != row:\n                    A[r, :] -= A[r, col] / A[row, col] * A[row, :]\n            row += 1\n\n    prec = int(-np.log10(tol))\n    return np.round(A, prec), pivots\n</code></pre>"},{"location":"ref-tools/#opti.tools.sanitize","title":"<code>sanitize</code>","text":""},{"location":"ref-tools/#opti.tools.sanitize.sanitize_problem","title":"<code>sanitize_problem(problem)</code>","text":"<p>This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives.</p> <p>More precisely, the resulting problem has the following properties: - Inputs are named <code>input_0</code>, <code>input_1</code>, .... Outputs are named analogously. - The data is scaled per feature to <code>[0, 1]</code>. - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of <code>f</code> are dropped if there are any.</p> <p>Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem</code> <p>to be sanitized</p> required <p>Exceptions:</p> Type Description <code>TypeError</code> <p>in case there are unsupported constraints, data is None, or there are output constraints</p> <p>Returns:</p> Type Description <code>Problem</code> <p>Problem instance with sanitized labels and normalized data</p> Source code in <code>opti/tools/sanitize.py</code> <pre><code>def sanitize_problem(problem: Problem) -&gt; Problem:\n\"\"\"\n    This creates a transformation of the problem with sanitized data. Thereby, we try\n    to preserve relationships between inputs, outputs, and objectives.\n\n    More precisely, the resulting problem has the following properties:\n    - Inputs are named `input_0`, `input_1`, .... Outputs are named analogously.\n    - The data is scaled per feature to `[0, 1]`.\n    - Coefficients of linear constraints are adapted to the data scaling.\n    - Models and evaluatons in terms of `f` are dropped if there are any.\n\n    Currently unsuported are problems with\n    - discrete or categorical variables,\n    - nonlinear (in)equality input constraints, or\n    - output constraints.\n\n    Args:\n        problem: to be sanitized\n\n    Raises:\n        TypeError: in case there are unsupported constraints, data is None, or there are output constraints\n\n    Returns:\n        Problem instance with sanitized labels and normalized data\n    \"\"\"\n    if problem.data is None:\n        raise TypeError(\"we cannot sanitize a problem without data\")\n    if problem.output_constraints is not None:\n        raise TypeError(\"output constraints are currently not supported\")\n    if getattr(problem, \"f\", None) is not None:\n        warnings.warn(\"f is not sanitized but dropped\")\n    if problem.models is not None:\n        warnings.warn(\"models are not sanitized but dropped\")\n\n    inputs = _sanitize_params(problem.inputs, \"input\")\n    input_name_map = {pi.name: i.name for pi, i in zip(problem.inputs, inputs)}\n    normalized_in_data, xmin, \u0394x = _normalize_parameters_data(\n        problem.data, problem.inputs\n    )\n    outputs = _sanitize_params(problem.outputs, \"output\")\n    output_name_map = {pi.name: i.name for pi, i in zip(problem.outputs, outputs)}\n    normalized_out_data, ymin, \u0394y = _normalize_parameters_data(\n        problem.data, problem.outputs\n    )\n\n    normalized_in_data.columns = inputs.names\n    normalized_out_data.columns = outputs.names\n    normalized_data = pd.concat([normalized_in_data, normalized_out_data], axis=1)\n    normalized_data.reset_index(inplace=True, drop=True)\n\n    objectives = deepcopy(problem.objectives)\n    for obj in objectives:\n        sanitized_name = output_name_map[obj.name]\n        i = outputs.names.index(sanitized_name)\n        obj.name = sanitized_name\n        obj.parameter = sanitized_name\n        obj.target = (obj.target - ymin[i]) / \u0394y[i]\n        if hasattr(obj, \"tolerance\"):\n            obj.tolerance /= \u0394y[i]\n\n    constraints = deepcopy(problem.constraints)\n    if constraints is not None:\n        for c in constraints:\n            c.names = [input_name_map[n] for n in c.names]\n            if isinstance(c, (LinearEquality, LinearInequality)):\n                c.lhs = (c.lhs + xmin) * \u0394x\n                if c.rhs &gt; 1e-5:\n                    c.lhs = c.lhs / c.rhs\n                    c.rhs = 1.0\n            elif isinstance(c, NChooseK):\n                pass\n            else:\n                raise TypeError(\n                    \"sanitizer only supports linear and n-choose-k constraints\"\n                )\n\n    normalized_problem = Problem(\n        inputs=inputs,\n        outputs=outputs,\n        objectives=objectives,\n        constraints=constraints,\n        data=normalized_data,\n    )\n    return normalized_problem\n</code></pre>"},{"location":"datasets/alkox/","title":"Alkox","text":"<p>This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx).  The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective.</p> <pre><code>problem = opti.problems.Alkox()\n</code></pre> <p>Reference</p> <p>F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI. Obtained from Olympus.</p>"},{"location":"datasets/baumgartner-aniline/","title":"Aniline cross-coupling (Baumgartner 2019)","text":"<p>Dataset on optimizing Pd-catalyzed C\u2013N coupling reactions promoted by organic bases using aniline as starting material.</p> <pre><code>problem = opti.problems.BaumgartnerAniline()\n</code></pre> <p>Reference</p> <p>Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI. Data obtained from Summit.</p>"},{"location":"datasets/baumgartner-benzamide/","title":"Benzamide cross-coupling (Baumgartner 2019)","text":"<p>Dataset on optimizing Pd-catalyzed C\u2013N coupling reactions promoted by organic bases using benzamide as starting material.</p> <pre><code>problem = opti.problems.BaumgartnerBenzamide()\n</code></pre> <p>Reference</p> <p>Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI. Data obtained from Summit.</p>"},{"location":"datasets/benzylation/","title":"Benzylation","text":"<p>This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction.  Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective.</p> <pre><code>problem = opti.problems.Benzylation()\n</code></pre> <p>Reference</p> <p>A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI. Obtained from Olympus.</p>"},{"location":"datasets/cake/","title":"Cake","text":"<p>Fictional dataset for cake recipe optimization with mixed objectives.</p> <pre><code>problem = opti.problems.Cake()\n</code></pre>"},{"location":"datasets/fullerenes/","title":"Fullerenes","text":"<p>This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes.  Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product.  Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective.</p> <pre><code>problem = opti.problems.Fullerenes()\n</code></pre> <p>Reference</p> <p>B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798.  DOI. Obtained from Olympus.</p>"},{"location":"datasets/hplc/","title":"HPLC","text":"<p>This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective.</p> <pre><code>problem = opti.problems.HPLC()\n</code></pre> <p>Reference</p> <p>L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) DOI. Obtained from Olympus.</p>"},{"location":"datasets/photodegradation/","title":"Photodegradation","text":"<p>This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light.  Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective.</p> <pre><code>problem = opti.problems.Photodegradation()\n</code></pre> <p>Reference</p> <p>S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. DOI. Obtained from Olympus.</p>"},{"location":"datasets/reizmann-suzuki/","title":"Suzuki (Reizmann 2016)","text":"<p>Each case was has a different set of substrates but the same possible catalysts. </p> <pre><code>problem = opti.problems.ReizmannSuzuki()\n</code></pre> <p>Reference</p> <p>Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry &amp; engineering, 1(6), 658-666 DOI. Data obtained from Summit.</p>"},{"location":"datasets/snar/","title":"SnAr","text":"<p>This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism.  Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective.</p> <pre><code>problem = opti.problems.SnAr()\n</code></pre> <p>Reference</p> <p>A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI. Obtained from Olympus.</p>"},{"location":"datasets/suzuki/","title":"Suzuki","text":"<p>This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate.  Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective.</p> <pre><code>problem = opti.problems.Suzuki()\n</code></pre> <p>Reference</p> <p>F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI. Obtained from Olympus.</p>"},{"location":"examples/problem_reduction/","title":"Problem reduction","text":"<p>When describing physical experiments there are often linear equality constraints to be considered. For example in a formulation all ingredients of a mixture add up to 1. </p> <pre><code>problem = opti.Problem(\n    inputs=[\n        opti.Continuous(\"x1\", [0.1, 0.7]),\n        opti.Continuous(\"x2\", [0, 0.8]),\n        opti.Continuous(\"x3\", [0.3, 0.9]),\n    ],\n    outputs=[opti.Continuous(\"y\")],\n    constraints=[opti.LinearEquality([\"x1\", \"x2\", \"x3\"], rhs=1)]\n)\n</code></pre> <p>In statistical modeling linear equalities lead to multicollinearities, which makes the coefficients of linear models sensitive to noise. For modeling tasks this collinearity can be addressed by e.g. dropping one input parameter for each corresponding equality constraint.</p> <p>For sampling and optimization tasks this becomes a bit trickier as the parameter bounds and inequality constraints need to be adapted as well. Consider in the initial example we drop \\(x_1\\) together with the linear equality.  To ensure that solutions (\\(x_2\\), \\(x_3\\)) still satisfy the box bounds and constraints, we need to add the following two inequality constraints: $$ \\begin{align} x_1 \\geq 0.1 \\Longleftrightarrow x_2 + x_3 \\leq 0.9 \\newline x_1 \\leq 0.7 \\Longleftrightarrow x_2 + x_3 \\geq 0.3 \\end{align} $$</p> <p>The function <code>reduce_problem</code> automates this tedious task. Given a problem containing any number of linear inequalities and at least one equality constraint, it returns an equivalent problem where the linear equalities are removed by eliminating a corresponding number of inputs.</p> <pre><code>reduced_problem, transform = opti.tools.reduce_problem(problem)\nprint(reduced_problem)\n&gt;&gt;&gt; \nProblem(\n    inputs=Parameters([\n        Continuous('x2', domain=[0.0, 0.8]), \n        Continuous('x3', domain=[0.3, 0.9])\n    ]),\n    outputs=Parameters([Continuous('y')]),\n    objectives=Objectives([Minimize('y')]),\n    constraints=Constraints([\n        LinearInequality(names=['x2', 'x3'], lhs=[-1.0, -1.0], rhs=-0.3),\n        LinearInequality(names=['x2', 'x3'], lhs=[1.0, 1.0], rhs=0.9)\n    ])\n)\n</code></pre> <p>The transformer object allows to transfrom data to and from the reduced space.</p> <pre><code>X1 = problem.sample_inputs(10)\nXr = transform.drop_data(X1)\nX2 = transform.augment_data(Xr)\nassert np.allclose(X1, X2[X1.columns])\n</code></pre> <p>Equality constraints are not well supported in sampling (any form of acceptance-rejection sampling will not work) and optimization methods.  For example population-based optimization approaches such as evolutionary algorithms only approximately support linear equalities via penalties or a conversion to two-sided inequalites. By reducing the problem, such optimization tasks become significantly easier to solve.</p> <p>Finally, let's consider a more involved example involving two mixtures, A and B, as well as an additional discrete and categorical variable, and an extra inequality constraint for some of the components of mixture A. We also set up a function <code>y = f(X)</code> to evaluate the system.</p> <pre><code>def f(X):\n    y = X[[\"A1\", \"A2\", \"A3\", \"A4\"]] @ [1, -2, 3, 2]\n    y += X[[\"B1\", \"B2\", \"B3\"]] @ [0.1, 0.4, 0.3]\n    y += X[\"Temperature\"] / 30\n    y += X[\"Process\"] == \"process 2\"\n    return pd.DataFrame({\"y\": y})\n\nproblem = opti.Problem(\n    inputs=[\n        opti.Continuous(\"A1\", [0, 0.9]),\n        opti.Continuous(\"A2\", [0, 0.8]),\n        opti.Continuous(\"A3\", [0, 0.9]),\n        opti.Continuous(\"A4\", [0, 0.9]),\n        opti.Continuous(\"B1\", [0.3, 0.9]),\n        opti.Continuous(\"B2\", [0, 0.8]),\n        opti.Continuous(\"B3\", [0.1, 1]),\n        opti.Discrete(\"Temperature\", [20, 25, 30]),\n        opti.Categorical(\"Process\", [\"process 1\", \"process 2\", \"process 3\"])\n    ],\n    outputs=[opti.Continuous(\"y\")],\n    constraints=[\n        opti.LinearEquality([\"A1\", \"A2\", \"A3\", \"A4\"], rhs=1),\n        opti.LinearEquality([\"B1\", \"B2\", \"B3\"], rhs=1),\n        opti.LinearInequality([\"A1\", \"A2\"], lhs=[1, 2], rhs=0.8),\n    ],\n    f=f\n)\n</code></pre> <p>Reducing the problem works despite the discrete and categorical inputs as these don't appear in the linear equalities. We end up 7 out of 9 initial inputs and 5 inequality constraints, which are only referring to the remaining inputs. <pre><code>reduced_problem, transform = opti.tools.reduce_problem(problem)\nprint(reduced_problem)\n&gt;&gt;&gt; \nProblem(\n    inputs=Parameters([\n        Discrete('Temperature', domain=[20.0, 25.0, 30.0]),\n        Categorical('Process', domain=['process 1', 'process 2', 'process 3']),\n        Continuous('A2', domain=[0.0, 0.8]),\n        Continuous('A3', domain=[0.0, 0.9]),\n        Continuous('A4', domain=[0.0, 0.9]),\n        Continuous('B2', domain=[0.0, 0.8]),\n        Continuous('B3', domain=[0.1, 1.0])\n    ]),\n    outputs=Parameters([Continuous('y')]),\n    objectives=Objectives([Minimize('y')]),\n    constraints=Constraints([\n        LinearInequality(names=['A2' 'A3' 'A4'], lhs=[1.0, -1.0, -1.0], rhs=-0.2),\n        LinearInequality(names=['A2', 'A3', 'A4'], lhs=[-1.0, -1.0, -1.0], rhs=-0.1),\n        LinearInequality(names=['A2', 'A3', 'A4'], lhs=[1.0, 1.0, 1.0], rhs=1.0),\n        LinearInequality(names=['B2', 'B3'], lhs=[-1.0, -1.0], rhs=-0.1),\n        LinearInequality(names=['B2', 'B3'], lhs=[1.0, 1.0], rhs=0.7)\n    ])\n)\n</code></pre></p> <p>The function <code>f(X)</code> was automaticaly wrapped so in the reduced problem it can be evaluated for points in the reduced space, with the same result.</p> <pre><code>Xr = reduced_problem.sample_inputs(10)\nX = transform.augment_data(Xr)\ny1 = problem.f(X)\ny2 = reduced_problem.f(Xr)\nassert np.allclose(y1, y2)\n</code></pre>"}]}