{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The primary purpose of opti is to define tasks or problems in a number of closely related fields, including experimental design, multiobjective optimization and decision making and Bayesian optimization. Opti specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved. Experimental design In the context of experimental design opti allows to define a design space \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] where the design parameters may take values depending on their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) and a set of equations define additional experimental constraints, e.g. linear equality: \\(\\sum x_i = 1\\) linear inequality: \\(2 x_1 \\leq x_2\\) non-linear inequality: \\(\\sum x_i^2 \\leq 1\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values. Multiobjective optimization In the context of multiobjective optimization opti allows to define a vector-valued optimization problem \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] where \\(x \\in \\mathbb{X}\\) is again the experimental design space \\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and \\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized. Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision. Bayesian optimization In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\) . An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.","title":"Introduction"},{"location":"#introduction","text":"The primary purpose of opti is to define tasks or problems in a number of closely related fields, including experimental design, multiobjective optimization and decision making and Bayesian optimization. Opti specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.","title":"Introduction"},{"location":"#experimental-design","text":"In the context of experimental design opti allows to define a design space \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] where the design parameters may take values depending on their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) and a set of equations define additional experimental constraints, e.g. linear equality: \\(\\sum x_i = 1\\) linear inequality: \\(2 x_1 \\leq x_2\\) non-linear inequality: \\(\\sum x_i^2 \\leq 1\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.","title":"Experimental design"},{"location":"#multiobjective-optimization","text":"In the context of multiobjective optimization opti allows to define a vector-valued optimization problem \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] where \\(x \\in \\mathbb{X}\\) is again the experimental design space \\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and \\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized. Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.","title":"Multiobjective optimization"},{"location":"#bayesian-optimization","text":"In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\) . An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.","title":"Bayesian optimization"},{"location":"install/","text":"Install Note: the package name is mopti while the import name is opti . pip install mopti or install the latest version with pip install git+https://github.com/basf/mopti.git","title":"Install"},{"location":"install/#install","text":"Note: the package name is mopti while the import name is opti . pip install mopti or install the latest version with pip install git+https://github.com/basf/mopti.git","title":"Install"},{"location":"problem/","text":"Getting started Opti problems consist of a definition of the input parameters \\(x \\in \\mathbb{X}\\) , the output parameters \\(y \\in \\mathbb{Y}\\) , the objectives \\(s(y)\\) (optional), the input constraints \\(g(x) \\leq 0\\) (optional), the output constraints \\(h(y)\\) (optional), a data set of previous function evaluations \\(\\{x, y\\}\\) (optional) and the function \\(f(x)\\) to be optimized (optional). Parameters Input and output spaces are defined using Parameters objects. For example a mixed input space of three continuous, one discrete and one categorical parameter(s), along with an output space of continuous parameters can be defined as: import opti inputs = opti . Parameters ([ opti . Continuous ( \"x1\" , domain = [ 0 , 1 ]), opti . Continuous ( \"x2\" , domain = [ 0 , 1 ]), opti . Continuous ( \"x3\" , domain = [ 0 , 1 ]), opti . Discrete ( \"x4\" , domain = [ 1 , 2 , 5 , 7.5 ]), opti . Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) ]) outputs = opti . Parameters ([ opti . Continuous ( \"y1\" , domain = [ 0 , None ]), opti . Continuous ( \"y2\" , domain = [ None , None ]), opti . Continuous ( \"y3\" , domain = [ 0 , 100 ]) ]) Note that for some of the outputs we didn't specify bounds as we may not know them. Individual parameters can be indexed by name. inputs [ \"x5\" ] >>> Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) and all parameter names can retrieved with inputs.names >>> [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"] We can sample from individual parameters, parameter spaces or parameter spaces including constraints (more on that later) x5 = inputs [ \"x1\" ] . sample ( 3 ) print ( x5 . values ) >>> array ([ 0.72405216 , 0.14914942 , 0.46051132 ]) X = inputs . sample ( 5 ) print ( X ) >>> x1 x2 x3 x4 x5 0 0.760116 0.063584 0.518885 7.5 A 1 0.807928 0.496213 0.885545 1.0 C 2 0.351253 0.993993 0.340414 5.0 B 3 0.385825 0.857306 0.355267 1.0 C 4 0.191907 0.993494 0.384322 2.0 A We can also check for each point in a dataframe, whether it is contained in the space. inputs . contains ( X ) >>> array ([ True , True , True , True , True ]) In general all opti functions operate on dataframes and thus use the parameter name to identify corresponding column. Hence, a dataframe may contain additional columns and columns may be in arbitrary order. The index of a dataframe is preserved, meaning that the returned dataframe will have the same indices as the original dataframe. Constraints Input constraints are defined separately from the input space. The following constraints are supported. Linear constraints ( LinearEquality and LinearInequality ) are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input parameters they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\) . # A mixture: x1 + x2 + x3 = 1 constr1 = opti . LinearEquality ([ \"x1\" , \"x2\" , \"x3\" ], lhs = 1 , rhs = 1 ) # x1 + 2 * x3 < 0.8 constr2 = opti . LinearInequality ([ \"x1\" , \"x3\" ], lhs = [ 1 , 2 ], rhs = 0.8 ) Because of the product \\(a_i x_i\\) , linear constraints cannot operate on categorical parameters. Nonlinear constraints ( NonlinearEquality and NonlinearInequality ) take any expression that can be evaluated by pandas.eval , including mathematical operators such as sin , exp , log10 or exponentiation. # The unit circle: x1**2 + x2**2 = 1 constr3 = opti . NonlinearEquality ( \"x1**2 + x2**2 - 1\" ) Nonlinear constraints can also operate on categorical parameters and support conditional statements. # Require x1 < 0.5 if x5 == \"A\" constr4 = opti . NonlinearInequality ( \"(x1 - 0.5) * (x5 =='A')\" ) Finally, there is a combinatorical constraint ( NChooseK ) to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe. # Only 2 out of 3 parameters can be greater than zero constr5 = opti . NChooseK ([ \"x1\" , \"x2\" , \"x3\" ], max_active = 2 ) Constraints can be grouped in a container which acts as the union constraints. constraints = opti . Constraints ([ constr1 , constr2 , constr3 , constr4 , constr5 ]) We can check whether a point satisfies individual constraints or the list of constraints. constr2 . satisfied ( X ) . values >>> array ([ False , False , True , True , True ]) The distance to the constraint boundary can also be evaluated for use in numerical optimization methods, where values \\(\\leq 0\\) correspond to a satisified constraint. constr2 ( X ) . values >>> array ([ 0.479001 , 0.89347371 , - 0.10833372 , - 0.05890873 , - 0.22377122 ]) Opti contains a number of methods to draw random samples from constrained spaces, see the sampling reference. Objectives In an optimization problem we need to define the target direction or target value individually for each output. This is done using objectives \\(s_m(y_m)\\) so that a mixed objective optimization becomes a minimization problem. objectives = opti . Objectives ([ opti . Minimize ( \"y1\" ), opti . Maximize ( \"y2\" ), opti . CloseToTarget ( \"y3\" , target = 7 ) ]) We can compute objective values from output values to see the objective transformation applied. Y = pd . DataFrame ({ \"y1\" : [ 1 , 2 , 3 ], \"y2\" : [ 7 , 4 , 5 ], \"y3\" : [ 5 , 6.9 , 12 ] }) objectives ( Y ) >>> minimize_y1 maximize_y2 closetotarget_y3 0 1 - 7 4.00 1 2 - 4 0.01 2 3 - 5 25.00 Objectives can also be used as output constraints. This is different from an objective in that we want the constraint to be satisfied and not explore possible tradeoffs. Problem Finally, a problem is the combination of inputs, outputs, objectives, constraints, output_constraints, (true) function and data. problem = opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , objectives = objectives ) Problems can be serialized to and from a dictionary config = problem . to_config () problem = opti . Problem ( ** config ) or to a json file problem . to_json ( \"problem.json\" ) problem = opti . read_json ( \"problem.json\" )","title":"Getting started"},{"location":"problem/#getting-started","text":"Opti problems consist of a definition of the input parameters \\(x \\in \\mathbb{X}\\) , the output parameters \\(y \\in \\mathbb{Y}\\) , the objectives \\(s(y)\\) (optional), the input constraints \\(g(x) \\leq 0\\) (optional), the output constraints \\(h(y)\\) (optional), a data set of previous function evaluations \\(\\{x, y\\}\\) (optional) and the function \\(f(x)\\) to be optimized (optional).","title":"Getting started"},{"location":"problem/#parameters","text":"Input and output spaces are defined using Parameters objects. For example a mixed input space of three continuous, one discrete and one categorical parameter(s), along with an output space of continuous parameters can be defined as: import opti inputs = opti . Parameters ([ opti . Continuous ( \"x1\" , domain = [ 0 , 1 ]), opti . Continuous ( \"x2\" , domain = [ 0 , 1 ]), opti . Continuous ( \"x3\" , domain = [ 0 , 1 ]), opti . Discrete ( \"x4\" , domain = [ 1 , 2 , 5 , 7.5 ]), opti . Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) ]) outputs = opti . Parameters ([ opti . Continuous ( \"y1\" , domain = [ 0 , None ]), opti . Continuous ( \"y2\" , domain = [ None , None ]), opti . Continuous ( \"y3\" , domain = [ 0 , 100 ]) ]) Note that for some of the outputs we didn't specify bounds as we may not know them. Individual parameters can be indexed by name. inputs [ \"x5\" ] >>> Categorical ( \"x5\" , domain = [ \"A\" , \"B\" , \"C\" ]) and all parameter names can retrieved with inputs.names >>> [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"] We can sample from individual parameters, parameter spaces or parameter spaces including constraints (more on that later) x5 = inputs [ \"x1\" ] . sample ( 3 ) print ( x5 . values ) >>> array ([ 0.72405216 , 0.14914942 , 0.46051132 ]) X = inputs . sample ( 5 ) print ( X ) >>> x1 x2 x3 x4 x5 0 0.760116 0.063584 0.518885 7.5 A 1 0.807928 0.496213 0.885545 1.0 C 2 0.351253 0.993993 0.340414 5.0 B 3 0.385825 0.857306 0.355267 1.0 C 4 0.191907 0.993494 0.384322 2.0 A We can also check for each point in a dataframe, whether it is contained in the space. inputs . contains ( X ) >>> array ([ True , True , True , True , True ]) In general all opti functions operate on dataframes and thus use the parameter name to identify corresponding column. Hence, a dataframe may contain additional columns and columns may be in arbitrary order. The index of a dataframe is preserved, meaning that the returned dataframe will have the same indices as the original dataframe.","title":"Parameters"},{"location":"problem/#constraints","text":"Input constraints are defined separately from the input space. The following constraints are supported. Linear constraints ( LinearEquality and LinearInequality ) are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input parameters they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\) . # A mixture: x1 + x2 + x3 = 1 constr1 = opti . LinearEquality ([ \"x1\" , \"x2\" , \"x3\" ], lhs = 1 , rhs = 1 ) # x1 + 2 * x3 < 0.8 constr2 = opti . LinearInequality ([ \"x1\" , \"x3\" ], lhs = [ 1 , 2 ], rhs = 0.8 ) Because of the product \\(a_i x_i\\) , linear constraints cannot operate on categorical parameters. Nonlinear constraints ( NonlinearEquality and NonlinearInequality ) take any expression that can be evaluated by pandas.eval , including mathematical operators such as sin , exp , log10 or exponentiation. # The unit circle: x1**2 + x2**2 = 1 constr3 = opti . NonlinearEquality ( \"x1**2 + x2**2 - 1\" ) Nonlinear constraints can also operate on categorical parameters and support conditional statements. # Require x1 < 0.5 if x5 == \"A\" constr4 = opti . NonlinearInequality ( \"(x1 - 0.5) * (x5 =='A')\" ) Finally, there is a combinatorical constraint ( NChooseK ) to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe. # Only 2 out of 3 parameters can be greater than zero constr5 = opti . NChooseK ([ \"x1\" , \"x2\" , \"x3\" ], max_active = 2 ) Constraints can be grouped in a container which acts as the union constraints. constraints = opti . Constraints ([ constr1 , constr2 , constr3 , constr4 , constr5 ]) We can check whether a point satisfies individual constraints or the list of constraints. constr2 . satisfied ( X ) . values >>> array ([ False , False , True , True , True ]) The distance to the constraint boundary can also be evaluated for use in numerical optimization methods, where values \\(\\leq 0\\) correspond to a satisified constraint. constr2 ( X ) . values >>> array ([ 0.479001 , 0.89347371 , - 0.10833372 , - 0.05890873 , - 0.22377122 ]) Opti contains a number of methods to draw random samples from constrained spaces, see the sampling reference.","title":"Constraints"},{"location":"problem/#objectives","text":"In an optimization problem we need to define the target direction or target value individually for each output. This is done using objectives \\(s_m(y_m)\\) so that a mixed objective optimization becomes a minimization problem. objectives = opti . Objectives ([ opti . Minimize ( \"y1\" ), opti . Maximize ( \"y2\" ), opti . CloseToTarget ( \"y3\" , target = 7 ) ]) We can compute objective values from output values to see the objective transformation applied. Y = pd . DataFrame ({ \"y1\" : [ 1 , 2 , 3 ], \"y2\" : [ 7 , 4 , 5 ], \"y3\" : [ 5 , 6.9 , 12 ] }) objectives ( Y ) >>> minimize_y1 maximize_y2 closetotarget_y3 0 1 - 7 4.00 1 2 - 4 0.01 2 3 - 5 25.00 Objectives can also be used as output constraints. This is different from an objective in that we want the constraint to be satisfied and not explore possible tradeoffs.","title":"Objectives"},{"location":"problem/#problem","text":"Finally, a problem is the combination of inputs, outputs, objectives, constraints, output_constraints, (true) function and data. problem = opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , objectives = objectives ) Problems can be serialized to and from a dictionary config = problem . to_config () problem = opti . Problem ( ** config ) or to a json file problem . to_json ( \"problem.json\" ) problem = opti . read_json ( \"problem.json\" )","title":"Problem"},{"location":"ref-constraint/","text":"Constraints Constraint Base class to define constraints on the input space, g(x) == 0 or g(x) <= 0. Source code in opti/constraint.py class Constraint : \"\"\"Base class to define constraints on the input space, g(x) == 0 or g(x) <= 0.\"\"\" def __call__ ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint g(x).\"\"\" raise NotImplementedError def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities.\"\"\" raise NotImplementedError def to_config ( self ) -> Dict : raise NotImplementedError __call__ ( self , data ) special Numerically evaluate the constraint g(x). Source code in opti/constraint.py def __call__ ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint g(x).\"\"\" raise NotImplementedError satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities.\"\"\" raise NotImplementedError Constraints List of optimization constraints Source code in opti/constraint.py class Constraints : \"\"\"List of optimization constraints\"\"\" def __init__ ( self , constraints : Sequence ): self . constraints = [] for c in constraints : if not isinstance ( c , Constraint ): if \"names\" in c and len ( c [ \"names\" ]) == 0 : continue # skip empty constraints c = make_constraint ( ** c ) self . constraints . append ( c ) def __repr__ ( self ): return \"Constraints( \\n \" + pprint . pformat ( self . constraints ) + \" \\n )\" def __iter__ ( self ): return iter ( self . constraints ) def __len__ ( self ): return len ( self . constraints ) def __getitem__ ( self , i ): return self . constraints [ i ] def __call__ ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c ( data ) for c in self . constraints ], axis = 1 ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) def to_config ( self ) -> List [ Dict ]: return [ obj . to_config () for obj in self . constraints ] __call__ ( self , data ) special Numerically evaluate all constraints. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description DataFrame Constraint evaluation g(x) for each of the constraints. Source code in opti/constraint.py def __call__ ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c ( data ) for c in self . constraints ], axis = 1 ) satisfied ( self , data ) Check if all constraints are satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description Series Series of booleans indicating if all constraints are satisfied. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) LinearEquality ( Constraint ) Source code in opti/constraint.py class LinearEquality ( Constraint ): def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) def __repr__ ( self ): return ( f \"LinearEquality(names= { self . names } , lhs= { list ( self . lhs ) } , rhs= { self . rhs } )\" ) def to_config ( self ) -> Dict : return dict ( type = \"linear-equality\" , names = self . names , lhs = self . lhs . tolist (), rhs = self . rhs , ) __init__ ( self , names , lhs = 1 , rhs = 0 ) special Linear / affine inequality of the form 'lhs * x == rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, List[float], numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as LinearEquality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients of A, B and C are not 1 they are passed explicitly. LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) LinearInequality ( Constraint ) Source code in opti/constraint.py class LinearInequality ( Constraint ): def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 def __repr__ ( self ): return f \"LinearInequality(names= { self . names } , lhs= { list ( self . lhs ) } , rhs= { self . rhs } )\" def to_config ( self ) -> Dict : return dict ( type = \"linear-inequality\" , names = self . names , lhs = self . lhs . tolist (), rhs = self . rhs , ) __init__ ( self , names , lhs = 1 , rhs = 0 ) special Linear / affine inequality of the form 'lhs * x <= rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, List[float], numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as LinearInequality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients are not 1, they need to be passed explicitly. LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both lhs and rhs need to be multiplied by -1. LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 NChooseK ( Constraint ) Source code in opti/constraint.py class NChooseK ( Constraint ): def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : x = np . abs ( data [ self . names ] . values ) num_zeros = x . shape [ 1 ] - self . max_active violation = np . apply_along_axis ( func1d = lambda r : sum ( sorted ( r )[: num_zeros ]), axis = 1 , arr = x ) return pd . Series ( violation , index = data . index ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self ( data ) <= 0 , index = data . index ) def __repr__ ( self ): return f \"NChooseK(names= { self . names } , max_active= { self . max_active } )\" def to_config ( self ) -> Dict : return dict ( type = \"n-choose-k\" , names = self . names , max_active = self . max_active ) __init__ ( self , names , max_active ) special Only k out of n values are allowed to take nonzero values. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required max_active int Maximium number of non-zero parameter values. required Examples: A choice of 2 or less from A, B, C, D or E can be defined as NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self ( data ) <= 0 , index = data . index ) NonlinearEquality ( Constraint ) Source code in opti/constraint.py class NonlinearEquality ( Constraint ): def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) def __repr__ ( self ): return f \"NonlinearEquality(' { self . expression } ')\" def to_config ( self ) -> Dict : return dict ( type = \"nonlinear-equality\" , expression = self . expression ) __init__ ( self , expression ) special Equality of the form 'expression == 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 = 1, use NonlinearEquality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearEquality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearEquality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) NonlinearInequality ( Constraint ) Source code in opti/constraint.py class NonlinearInequality ( Constraint ): def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 def __repr__ ( self ): return f \"NonlinearInequality(' { self . expression } ')\" def to_config ( self ) -> Dict : return dict ( type = \"nonlinear-inequality\" , expression = self . expression ) __init__ ( self , expression ) special Inequality of the form 'expression <= 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 < 1, use NonlinearInequality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearInequality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearInequality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False satisfied ( self , data ) Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0","title":"Constraint"},{"location":"ref-constraint/#constraints","text":"","title":"Constraints"},{"location":"ref-constraint/#opti.constraint.Constraint","text":"Base class to define constraints on the input space, g(x) == 0 or g(x) <= 0. Source code in opti/constraint.py class Constraint : \"\"\"Base class to define constraints on the input space, g(x) == 0 or g(x) <= 0.\"\"\" def __call__ ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint g(x).\"\"\" raise NotImplementedError def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities.\"\"\" raise NotImplementedError def to_config ( self ) -> Dict : raise NotImplementedError","title":"Constraint"},{"location":"ref-constraint/#opti.constraint.Constraint.__call__","text":"Numerically evaluate the constraint g(x). Source code in opti/constraint.py def __call__ ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluate the constraint g(x).\"\"\" raise NotImplementedError","title":"__call__()"},{"location":"ref-constraint/#opti.constraint.Constraint.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities.\"\"\" raise NotImplementedError","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.Constraints","text":"List of optimization constraints Source code in opti/constraint.py class Constraints : \"\"\"List of optimization constraints\"\"\" def __init__ ( self , constraints : Sequence ): self . constraints = [] for c in constraints : if not isinstance ( c , Constraint ): if \"names\" in c and len ( c [ \"names\" ]) == 0 : continue # skip empty constraints c = make_constraint ( ** c ) self . constraints . append ( c ) def __repr__ ( self ): return \"Constraints( \\n \" + pprint . pformat ( self . constraints ) + \" \\n )\" def __iter__ ( self ): return iter ( self . constraints ) def __len__ ( self ): return len ( self . constraints ) def __getitem__ ( self , i ): return self . constraints [ i ] def __call__ ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c ( data ) for c in self . constraints ], axis = 1 ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) def to_config ( self ) -> List [ Dict ]: return [ obj . to_config () for obj in self . constraints ]","title":"Constraints"},{"location":"ref-constraint/#opti.constraint.Constraints.__call__","text":"Numerically evaluate all constraints. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description DataFrame Constraint evaluation g(x) for each of the constraints. Source code in opti/constraint.py def __call__ ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints. Args: data: Data to evaluate the constraints on. Returns: Constraint evaluation g(x) for each of the constraints. \"\"\" return pd . concat ([ c ( data ) for c in self . constraints ], axis = 1 )","title":"__call__()"},{"location":"ref-constraint/#opti.constraint.Constraints.satisfied","text":"Check if all constraints are satisfied. Parameters: Name Type Description Default data DataFrame Data to evaluate the constraints on. required Returns: Type Description Series Series of booleans indicating if all constraints are satisfied. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are satisfied. Args: data: Data to evaluate the constraints on. Returns: Series of booleans indicating if all constraints are satisfied. \"\"\" return pd . concat ([ c . satisfied ( data ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.LinearEquality","text":"Source code in opti/constraint.py class LinearEquality ( Constraint ): def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) def __repr__ ( self ): return ( f \"LinearEquality(names= { self . names } , lhs= { list ( self . lhs ) } , rhs= { self . rhs } )\" ) def to_config ( self ) -> Dict : return dict ( type = \"linear-equality\" , names = self . names , lhs = self . lhs . tolist (), rhs = self . rhs , )","title":"LinearEquality"},{"location":"ref-constraint/#opti.constraint.LinearEquality.__init__","text":"Linear / affine inequality of the form 'lhs * x == rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, List[float], numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as LinearEquality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients of A, B and C are not 1 they are passed explicitly. LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where A, B and C need to add up to 100 can be defined as ``` LinearEquality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients of A, B and C are not 1 they are passed explicitly. ``` LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = True","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.LinearEquality.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.LinearInequality","text":"Source code in opti/constraint.py class LinearInequality ( Constraint ): def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data [ self . names ] @ self . lhs - self . rhs def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 def __repr__ ( self ): return f \"LinearInequality(names= { self . names } , lhs= { list ( self . lhs ) } , rhs= { self . rhs } )\" def to_config ( self ) -> Dict : return dict ( type = \"linear-inequality\" , names = self . names , lhs = self . lhs . tolist (), rhs = self . rhs , )","title":"LinearInequality"},{"location":"ref-constraint/#opti.constraint.LinearInequality.__init__","text":"Linear / affine inequality of the form 'lhs * x <= rhs'. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required lhs Union[float, List[float], numpy.ndarray] Left-hand side / coefficients of the constraint. 1 rhs float Right-hand side of the constraint. 0 Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as LinearInequality([\"A\", \"B\", \"C\"], rhs=100) If the coefficients are not 1, they need to be passed explicitly. LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both lhs and rhs need to be multiplied by -1. LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], lhs : Union [ float , List [ float ], np . ndarray ] = 1 , rhs : float = 0 , ): \"\"\"Linear / affine inequality of the form 'lhs * x <= rhs'. Args: names: Parameter names that the constraint works on. lhs: Left-hand side / coefficients of the constraint. rhs: Right-hand side of the constraint. Examples: A mixture constraint where the values of A, B and C may not exceed 100 can be defined as ``` LinearInequality([\"A\", \"B\", \"C\"], rhs=100) ``` If the coefficients are not 1, they need to be passed explicitly. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100) ``` Inequalities are alway of the form g(x) <= 0. To define a the constraint g(x) >=0 0, both `lhs` and `rhs` need to be multiplied by -1. ``` LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100) LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100) ``` \"\"\" self . names = names if np . isscalar ( lhs ): self . lhs = lhs * np . ones ( len ( names )) else : self . lhs = np . asarray ( lhs ) if self . lhs . shape != ( len ( names ),): raise ValueError ( \"Number of parameters and coefficients/lhs don't match.\" ) self . rhs = rhs self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.LinearInequality.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NChooseK","text":"Source code in opti/constraint.py class NChooseK ( Constraint ): def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : x = np . abs ( data [ self . names ] . values ) num_zeros = x . shape [ 1 ] - self . max_active violation = np . apply_along_axis ( func1d = lambda r : sum ( sorted ( r )[: num_zeros ]), axis = 1 , arr = x ) return pd . Series ( violation , index = data . index ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self ( data ) <= 0 , index = data . index ) def __repr__ ( self ): return f \"NChooseK(names= { self . names } , max_active= { self . max_active } )\" def to_config ( self ) -> Dict : return dict ( type = \"n-choose-k\" , names = self . names , max_active = self . max_active )","title":"NChooseK"},{"location":"ref-constraint/#opti.constraint.NChooseK.__init__","text":"Only k out of n values are allowed to take nonzero values. Parameters: Name Type Description Default names List[str] Parameter names that the constraint works on. required max_active int Maximium number of non-zero parameter values. required Examples: A choice of 2 or less from A, B, C, D or E can be defined as NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) Source code in opti/constraint.py def __init__ ( self , names : List [ str ], max_active : int ): \"\"\"Only k out of n values are allowed to take nonzero values. Args: names: Parameter names that the constraint works on. max_active: Maximium number of non-zero parameter values. Examples: A choice of 2 or less from A, B, C, D or E can be defined as ``` NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2) ``` \"\"\" self . names = names self . max_active = max_active self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NChooseK.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( self ( data ) <= 0 , index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality","text":"Source code in opti/constraint.py class NonlinearEquality ( Constraint ): def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index ) def __repr__ ( self ): return f \"NonlinearEquality(' { self . expression } ')\" def to_config ( self ) -> Dict : return dict ( type = \"nonlinear-equality\" , expression = self . expression )","title":"NonlinearEquality"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.__init__","text":"Equality of the form 'expression == 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 = 1, use NonlinearEquality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearEquality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearEquality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Equality of the form 'expression == 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 = 1, use ``` NonlinearEquality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearEquality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearEquality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = True","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( data ), 0 ), index = data . index )","title":"satisfied()"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality","text":"Source code in opti/constraint.py class NonlinearInequality ( Constraint ): def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False def __call__ ( self , data : pd . DataFrame ) -> pd . Series : return data . eval ( self . expression ) def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0 def __repr__ ( self ): return f \"NonlinearInequality(' { self . expression } ')\" def to_config ( self ) -> Dict : return dict ( type = \"nonlinear-inequality\" , expression = self . expression )","title":"NonlinearInequality"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.__init__","text":"Inequality of the form 'expression <= 0'. Parameters: Name Type Description Default expression str Mathematical expression that can be evaluated by pandas.eval . required Examples: You can pass any expression that can be evaluated by pd.eval . To define x1 2 + x2 2 < 1, use NonlinearInequality(\"x1**2 + x2**2 - 1\") Standard mathematical operators are supported. NonlinearInequality(\"sin(A) / (exp(B) - 1)\") Parameter names with special characters or spaces need to be enclosed in backticks. NonlinearInequality(\"1 - `weight A` / `weight B`\") Source code in opti/constraint.py def __init__ ( self , expression : str ): \"\"\"Inequality of the form 'expression <= 0'. Args: expression: Mathematical expression that can be evaluated by `pandas.eval`. Examples: You can pass any expression that can be evaluated by `pd.eval`. To define x1**2 + x2**2 < 1, use ``` NonlinearInequality(\"x1**2 + x2**2 - 1\") ``` Standard mathematical operators are supported. ``` NonlinearInequality(\"sin(A) / (exp(B) - 1)\") ``` Parameter names with special characters or spaces need to be enclosed in backticks. ``` NonlinearInequality(\"1 - `weight A` / `weight B`\") ``` \"\"\" self . expression = expression self . is_equality = False","title":"__init__()"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.satisfied","text":"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) <= for inequalities. Source code in opti/constraint.py def satisfied ( self , data : pd . DataFrame ) -> pd . Series : return self ( data ) <= 0","title":"satisfied()"},{"location":"ref-metric/","text":"Metrics crowding_distance ( A ) Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II Parameters: Name Type Description Default A 2D-array Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. required Returns: Type Description array Crowding distance indicator for each point in the front. Source code in opti/metric.py def crowding_distance ( A ): \"\"\"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference: [Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II](https://link.springer.com/chapter/10.1007/3-540-45356-3_83) Args: A (2D-array): Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. Returns: array: Crowding distance indicator for each point in the front. \"\"\" A = pareto_front ( A ) N , m = A . shape # no crowding distance for 2 points if N <= 2 : return np . full ( N , np . inf ) # sort points along each objective sort = np . argsort ( A , axis = 0 ) A = A [ sort , np . arange ( m )] # normalize all objectives norm = np . max ( A , axis = 0 ) - np . min ( A , axis = 0 ) A = A / norm A [:, norm == 0 ] = 0 # handle min = max # distance to previous and to next point along each objective d = np . diff ( A , axis = 0 ) inf = np . full (( 1 , m ), np . inf ) d0 = np . concatenate ([ inf , d ]) d1 = np . concatenate ([ d , inf ]) # TODO: handle cases with duplicate objective values leading to 0 distances # cuboid side length = distance between previous and next point unsort = np . argsort ( sort , axis = 0 ) cuboid = d0 [ unsort , np . arange ( m )] + d1 [ unsort , np . arange ( m )] return np . mean ( cuboid , axis = 1 ) generational_distance ( A , R , p = 1 , clip = True ) Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 clip bool Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. True Returns: Type Description float Generational distance indicator. Source code in opti/metric.py def generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 , clip : bool = True ) -> float : r \"\"\"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference: [David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art](http://dx.doi.org/10.1162/106365600568158) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. clip (bool, optional): Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. Returns: float: Generational distance indicator. \"\"\" A = pareto_front ( A ) distances = A [:, np . newaxis ] - R [ np . newaxis ] if clip : distances = distances . clip ( 0 , None ) distances = np . linalg . norm ( distances , axis = 2 ) . min ( axis = 1 ) return np . linalg . norm ( distances , p ) / len ( A ) inverted_generational_distance ( A , R , p = 1 ) Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 Returns: Type Description float Inverted generational distance indicator. Source code in opti/metric.py def inverted_generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 ) -> float : \"\"\"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference: [CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm](https://doi.org/10.1007/978-3-540-24694-7_71) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. Returns: float: Inverted generational distance indicator. \"\"\" return generational_distance ( R , A , p , clip = False ) is_pareto_efficient ( A ) Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 1D-array of bools Boolean mask for the Pareto efficient points in A. Source code in opti/metric.py def is_pareto_efficient ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 1D-array of bools: Boolean mask for the Pareto efficient points in A. \"\"\" efficient = np . ones ( len ( A ), dtype = bool ) idx = np . arange ( len ( A )) for i , a in enumerate ( A ): if not efficient [ i ]: continue # set all *other* efficent points to False, if they are not strictly better in at least one objective efficient [ efficient ] = np . any ( A [ efficient ] < a , axis = 1 ) | ( i == idx [ efficient ]) return efficient pareto_front ( A ) Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 2D-array Pareto efficient points in A. Source code in opti/metric.py def pareto_front ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 2D-array: Pareto efficient points in A. \"\"\" return A [ is_pareto_efficient ( A )]","title":"Metric"},{"location":"ref-metric/#metrics","text":"","title":"Metrics"},{"location":"ref-metric/#opti.metric.crowding_distance","text":"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II Parameters: Name Type Description Default A 2D-array Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. required Returns: Type Description array Crowding distance indicator for each point in the front. Source code in opti/metric.py def crowding_distance ( A ): \"\"\"Crowding distance indicator. The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points. Reference: [Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II](https://link.springer.com/chapter/10.1007/3-540-45356-3_83) Args: A (2D-array): Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked. Returns: array: Crowding distance indicator for each point in the front. \"\"\" A = pareto_front ( A ) N , m = A . shape # no crowding distance for 2 points if N <= 2 : return np . full ( N , np . inf ) # sort points along each objective sort = np . argsort ( A , axis = 0 ) A = A [ sort , np . arange ( m )] # normalize all objectives norm = np . max ( A , axis = 0 ) - np . min ( A , axis = 0 ) A = A / norm A [:, norm == 0 ] = 0 # handle min = max # distance to previous and to next point along each objective d = np . diff ( A , axis = 0 ) inf = np . full (( 1 , m ), np . inf ) d0 = np . concatenate ([ inf , d ]) d1 = np . concatenate ([ d , inf ]) # TODO: handle cases with duplicate objective values leading to 0 distances # cuboid side length = distance between previous and next point unsort = np . argsort ( sort , axis = 0 ) cuboid = d0 [ unsort , np . arange ( m )] + d1 [ unsort , np . arange ( m )] return np . mean ( cuboid , axis = 1 )","title":"crowding_distance()"},{"location":"ref-metric/#opti.metric.generational_distance","text":"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 clip bool Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. True Returns: Type Description float Generational distance indicator. Source code in opti/metric.py def generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 , clip : bool = True ) -> float : r \"\"\"Generational distance indicator. The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence. Reference: [David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art](http://dx.doi.org/10.1162/106365600568158) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. clip (bool, optional): Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front. Returns: float: Generational distance indicator. \"\"\" A = pareto_front ( A ) distances = A [:, np . newaxis ] - R [ np . newaxis ] if clip : distances = distances . clip ( 0 , None ) distances = np . linalg . norm ( distances , axis = 2 ) . min ( axis = 1 ) return np . linalg . norm ( distances , p ) / len ( A )","title":"generational_distance()"},{"location":"ref-metric/#opti.metric.inverted_generational_distance","text":"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm Parameters: Name Type Description Default A 2D-array Set of points representing an approximate Pareto front. required R 2D-array Set of points representing a reference Pareto front. required p int Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. 1 Returns: Type Description float Inverted generational distance indicator. Source code in opti/metric.py def inverted_generational_distance ( A : np . ndarray , R : np . ndarray , p : float = 1 ) -> float : \"\"\"Inverted generational distance indicator. The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front. Reference: [CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm](https://doi.org/10.1007/978-3-540-24694-7_71) Args: A (2D-array): Set of points representing an approximate Pareto front. R (2D-array): Set of points representing a reference Pareto front. p (int, optional): Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average. Returns: float: Inverted generational distance indicator. \"\"\" return generational_distance ( R , A , p , clip = False )","title":"inverted_generational_distance()"},{"location":"ref-metric/#opti.metric.is_pareto_efficient","text":"Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 1D-array of bools Boolean mask for the Pareto efficient points in A. Source code in opti/metric.py def is_pareto_efficient ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 1D-array of bools: Boolean mask for the Pareto efficient points in A. \"\"\" efficient = np . ones ( len ( A ), dtype = bool ) idx = np . arange ( len ( A )) for i , a in enumerate ( A ): if not efficient [ i ]: continue # set all *other* efficent points to False, if they are not strictly better in at least one objective efficient [ efficient ] = np . any ( A [ efficient ] < a , axis = 1 ) | ( i == idx [ efficient ]) return efficient","title":"is_pareto_efficient()"},{"location":"ref-metric/#opti.metric.pareto_front","text":"Find the Pareto-efficient points in a set of objective vectors. Parameters: Name Type Description Default A 2D-array, shape=(samples, dimension Objective vectors. required Returns: Type Description 2D-array Pareto efficient points in A. Source code in opti/metric.py def pareto_front ( A : np . ndarray ) -> np . ndarray : \"\"\"Find the Pareto-efficient points in a set of objective vectors. Args: A (2D-array, shape=(samples, dimension)): Objective vectors. Returns: 2D-array: Pareto efficient points in A. \"\"\" return A [ is_pareto_efficient ( A )]","title":"pareto_front()"},{"location":"ref-model/","text":"Models CustomModel ( Model ) Source code in opti/model.py class CustomModel ( Model ): def __init__ ( self , names : List [ str ], f : Callable ): \"\"\"Custom model for arbitrary functions. Args: names: names of the modeled outputs. f: Callable that takes a DataFrame of inputs and returns a DataFrame of outputs. \"\"\" super () . __init__ ( names ) self . f = f def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : return self . f ( df ) def __repr__ ( self ): return f \"CustomModel( { self . names } , f= { self . f } )\" __init__ ( self , names , f ) special Custom model for arbitrary functions. Parameters: Name Type Description Default names List[str] names of the modeled outputs. required f Callable Callable that takes a DataFrame of inputs and returns a DataFrame of outputs. required Source code in opti/model.py def __init__ ( self , names : List [ str ], f : Callable ): \"\"\"Custom model for arbitrary functions. Args: names: names of the modeled outputs. f: Callable that takes a DataFrame of inputs and returns a DataFrame of outputs. \"\"\" super () . __init__ ( names ) self . f = f LinearModel ( Model ) Source code in opti/model.py class LinearModel ( Model ): def __init__ ( self , names : List [ str ], coefficients : Dict [ str , float ], offset : float = 0 , ): \"\"\"Model to compute an output as a linear/affine function of the inputs, $y = ax + b$. Args: names: name of the modeled output. coefficients: dictionary mapping input name to the corresponding coefficient a. offset: the offset b. \"\"\" super () . __init__ ( names ) if len ( names ) > 1 : raise ValueError ( \"LinearModel can only describe a single output.\" ) self . coefficients = coefficients self . offset = offset def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : coefficients_values = list ( self . coefficients . values ()) coefficients_names = list ( self . coefficients . keys ()) y = df [ coefficients_names ] . to_numpy () @ coefficients_values + self . offset return pd . DataFrame ( y , columns = self . names ) def __repr__ ( self ): return f \"LinearModel( { self . names } , coefficients= { self . coefficients } , offset= { self . offset } )\" def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , ) __init__ ( self , names , coefficients , offset = 0 ) special Model to compute an output as a linear/affine function of the inputs, \\(y = ax + b\\) . Parameters: Name Type Description Default names List[str] name of the modeled output. required coefficients Dict[str, float] dictionary mapping input name to the corresponding coefficient a. required offset float the offset b. 0 Source code in opti/model.py def __init__ ( self , names : List [ str ], coefficients : Dict [ str , float ], offset : float = 0 , ): \"\"\"Model to compute an output as a linear/affine function of the inputs, $y = ax + b$. Args: names: name of the modeled output. coefficients: dictionary mapping input name to the corresponding coefficient a. offset: the offset b. \"\"\" super () . __init__ ( names ) if len ( names ) > 1 : raise ValueError ( \"LinearModel can only describe a single output.\" ) self . coefficients = coefficients self . offset = offset to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , ) Model Source code in opti/model.py class Model : def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs. \"\"\" for name in names : if not isinstance ( name , str ): TypeError ( \"Model: names must be a list of strings.\" ) self . names = list ( names ) def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error __call__ ( self , df ) special Evaluate the objective values for a given DataFrame. Source code in opti/model.py def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError __init__ ( self , names ) special Base class for models of outputs as function of inputs. Parameters: Name Type Description Default names List[str] names of the modeled outputs. required Source code in opti/model.py def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs. \"\"\" for name in names : if not isinstance ( name , str ): TypeError ( \"Model: names must be a list of strings.\" ) self . names = list ( names ) to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error Models Source code in opti/model.py class Models : def __init__ ( self , models : Union [ List [ Model ], List [ Dict ]]): \"\"\"Container for models. Args: models: list of models or model configurations. \"\"\" _models = [] for m in models : if isinstance ( m , Model ): _models . append ( m ) else : _models . append ( make_model ( ** m )) self . models = _models def __call__ ( self , y : pd . DataFrame ) -> pd . DataFrame : return pd . concat ([ model ( y ) for model in self . models ], axis = 1 ) def __repr__ ( self ): return \"Models( \\n \" + pprint . pformat ( self . models ) + \" \\n )\" def __iter__ ( self ): return iter ( self . models ) def __len__ ( self ): return len ( self . models ) def __getitem__ ( self , i : int ) -> Model : return self . models [ i ] @property def names ( self ): names = [] for model in self . models : names += model . names return names def to_config ( self ) -> List [ Dict ]: return [ model . to_config () for model in self . models if model . to_config () is not None ] __init__ ( self , models ) special Container for models. Parameters: Name Type Description Default models Union[List[opti.model.Model], List[Dict]] list of models or model configurations. required Source code in opti/model.py def __init__ ( self , models : Union [ List [ Model ], List [ Dict ]]): \"\"\"Container for models. Args: models: list of models or model configurations. \"\"\" _models = [] for m in models : if isinstance ( m , Model ): _models . append ( m ) else : _models . append ( make_model ( ** m )) self . models = _models make_model ( type , ** kwargs ) Make a model object from a configuration dict. Source code in opti/model.py def make_model ( type , ** kwargs ): \"\"\"Make a model object from a configuration dict.\"\"\" t = type . lower () if t == \"linear-model\" : return LinearModel ( ** kwargs ) raise ValueError ( f \"Unknown model type: { t } .\" )","title":"Model"},{"location":"ref-model/#models","text":"","title":"Models"},{"location":"ref-model/#opti.model.CustomModel","text":"Source code in opti/model.py class CustomModel ( Model ): def __init__ ( self , names : List [ str ], f : Callable ): \"\"\"Custom model for arbitrary functions. Args: names: names of the modeled outputs. f: Callable that takes a DataFrame of inputs and returns a DataFrame of outputs. \"\"\" super () . __init__ ( names ) self . f = f def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : return self . f ( df ) def __repr__ ( self ): return f \"CustomModel( { self . names } , f= { self . f } )\"","title":"CustomModel"},{"location":"ref-model/#opti.model.CustomModel.__init__","text":"Custom model for arbitrary functions. Parameters: Name Type Description Default names List[str] names of the modeled outputs. required f Callable Callable that takes a DataFrame of inputs and returns a DataFrame of outputs. required Source code in opti/model.py def __init__ ( self , names : List [ str ], f : Callable ): \"\"\"Custom model for arbitrary functions. Args: names: names of the modeled outputs. f: Callable that takes a DataFrame of inputs and returns a DataFrame of outputs. \"\"\" super () . __init__ ( names ) self . f = f","title":"__init__()"},{"location":"ref-model/#opti.model.LinearModel","text":"Source code in opti/model.py class LinearModel ( Model ): def __init__ ( self , names : List [ str ], coefficients : Dict [ str , float ], offset : float = 0 , ): \"\"\"Model to compute an output as a linear/affine function of the inputs, $y = ax + b$. Args: names: name of the modeled output. coefficients: dictionary mapping input name to the corresponding coefficient a. offset: the offset b. \"\"\" super () . __init__ ( names ) if len ( names ) > 1 : raise ValueError ( \"LinearModel can only describe a single output.\" ) self . coefficients = coefficients self . offset = offset def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : coefficients_values = list ( self . coefficients . values ()) coefficients_names = list ( self . coefficients . keys ()) y = df [ coefficients_names ] . to_numpy () @ coefficients_values + self . offset return pd . DataFrame ( y , columns = self . names ) def __repr__ ( self ): return f \"LinearModel( { self . names } , coefficients= { self . coefficients } , offset= { self . offset } )\" def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , )","title":"LinearModel"},{"location":"ref-model/#opti.model.LinearModel.__init__","text":"Model to compute an output as a linear/affine function of the inputs, \\(y = ax + b\\) . Parameters: Name Type Description Default names List[str] name of the modeled output. required coefficients Dict[str, float] dictionary mapping input name to the corresponding coefficient a. required offset float the offset b. 0 Source code in opti/model.py def __init__ ( self , names : List [ str ], coefficients : Dict [ str , float ], offset : float = 0 , ): \"\"\"Model to compute an output as a linear/affine function of the inputs, $y = ax + b$. Args: names: name of the modeled output. coefficients: dictionary mapping input name to the corresponding coefficient a. offset: the offset b. \"\"\" super () . __init__ ( names ) if len ( names ) > 1 : raise ValueError ( \"LinearModel can only describe a single output.\" ) self . coefficients = coefficients self . offset = offset","title":"__init__()"},{"location":"ref-model/#opti.model.LinearModel.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> Dict : return dict ( type = \"linear-model\" , names = self . names , coefficients = self . coefficients , offset = self . offset , )","title":"to_config()"},{"location":"ref-model/#opti.model.Model","text":"Source code in opti/model.py class Model : def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs. \"\"\" for name in names : if not isinstance ( name , str ): TypeError ( \"Model: names must be a list of strings.\" ) self . names = list ( names ) def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error","title":"Model"},{"location":"ref-model/#opti.model.Model.__call__","text":"Evaluate the objective values for a given DataFrame. Source code in opti/model.py def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective values for a given DataFrame.\"\"\" raise NotImplementedError","title":"__call__()"},{"location":"ref-model/#opti.model.Model.__init__","text":"Base class for models of outputs as function of inputs. Parameters: Name Type Description Default names List[str] names of the modeled outputs. required Source code in opti/model.py def __init__ ( self , names : List [ str ]): \"\"\"Base class for models of outputs as function of inputs. Args: names: names of the modeled outputs. \"\"\" for name in names : if not isinstance ( name , str ): TypeError ( \"Model: names must be a list of strings.\" ) self . names = list ( names )","title":"__init__()"},{"location":"ref-model/#opti.model.Model.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/model.py def to_config ( self ) -> None : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" pass # non-serializable models should be ommited without raising an error","title":"to_config()"},{"location":"ref-model/#opti.model.Models","text":"Source code in opti/model.py class Models : def __init__ ( self , models : Union [ List [ Model ], List [ Dict ]]): \"\"\"Container for models. Args: models: list of models or model configurations. \"\"\" _models = [] for m in models : if isinstance ( m , Model ): _models . append ( m ) else : _models . append ( make_model ( ** m )) self . models = _models def __call__ ( self , y : pd . DataFrame ) -> pd . DataFrame : return pd . concat ([ model ( y ) for model in self . models ], axis = 1 ) def __repr__ ( self ): return \"Models( \\n \" + pprint . pformat ( self . models ) + \" \\n )\" def __iter__ ( self ): return iter ( self . models ) def __len__ ( self ): return len ( self . models ) def __getitem__ ( self , i : int ) -> Model : return self . models [ i ] @property def names ( self ): names = [] for model in self . models : names += model . names return names def to_config ( self ) -> List [ Dict ]: return [ model . to_config () for model in self . models if model . to_config () is not None ]","title":"Models"},{"location":"ref-model/#opti.model.Models.__init__","text":"Container for models. Parameters: Name Type Description Default models Union[List[opti.model.Model], List[Dict]] list of models or model configurations. required Source code in opti/model.py def __init__ ( self , models : Union [ List [ Model ], List [ Dict ]]): \"\"\"Container for models. Args: models: list of models or model configurations. \"\"\" _models = [] for m in models : if isinstance ( m , Model ): _models . append ( m ) else : _models . append ( make_model ( ** m )) self . models = _models","title":"__init__()"},{"location":"ref-model/#opti.model.make_model","text":"Make a model object from a configuration dict. Source code in opti/model.py def make_model ( type , ** kwargs ): \"\"\"Make a model object from a configuration dict.\"\"\" t = type . lower () if t == \"linear-model\" : return LinearModel ( ** kwargs ) raise ValueError ( f \"Unknown model type: { t } .\" )","title":"make_model()"},{"location":"ref-objective/","text":"Objectives CloseToTarget ( Objective ) Source code in opti/objective.py class CloseToTarget ( Objective ): def __init__ ( self , name : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: name: output to optimize target: target value exponent: exponent of the difference tolerance: only when used as output constraint. distance to target below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target self . exponent = exponent self . tolerance = tolerance def __call__ ( self , y : pd . Series ) -> pd . Series : return ( y - self . target ) . abs () ** self . exponent - self . tolerance ** self . exponent def __repr__ ( self ): return f \"CloseToTarget(' { self . name } ', target= { self . target } )\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config __init__ ( self , name , target = 0 , exponent = 1 , tolerance = 0 ) special Objective for getting as close as possible to a given value. s(y) = |y - target| exponent - tolerance exponent Parameters: Name Type Description Default name str output to optimize required target float target value 0 exponent float exponent of the difference 1 tolerance float only when used as output constraint. distance to target below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: name: output to optimize target: target value exponent: exponent of the difference tolerance: only when used as output constraint. distance to target below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target self . exponent = exponent self . tolerance = tolerance to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config Maximize ( Objective ) Source code in opti/objective.py class Maximize ( Objective ): def __init__ ( self , name : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: output to maximize target: only when used as output constraint. value above which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target def __call__ ( self , y : pd . Series ) -> pd . Series : return self . target - y def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return self . target - y def __repr__ ( self ): return f \"Maximize(' { self . name } ')\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config __init__ ( self , name , target = 0 ) special Maximization objective s(y) = target - y Parameters: Name Type Description Default name str output to maximize required target float only when used as output constraint. value above which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: output to maximize target: only when used as output constraint. value above which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config untransform ( self , y ) Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return self . target - y Minimize ( Objective ) Source code in opti/objective.py class Minimize ( Objective ): def __init__ ( self , name : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: name: output to minimize target: only when used as output constraint. value below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target def __call__ ( self , y : pd . Series ) -> pd . Series : return y - self . target def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target def __repr__ ( self ): return f \"Minimize(' { self . name } ')\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config __init__ ( self , name , target = 0 ) special Minimization objective s(y) = y - target Parameters: Name Type Description Default name str output to minimize required target float only when used as output constraint. value below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: name: output to minimize target: only when used as output constraint. value below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config untransform ( self , y ) Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target Objective Source code in opti/objective.py class Objective : def __init__ ( self , name : str ): \"\"\"Base class for optimzation objectives.\"\"\" self . name = name def __call__ ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError __call__ ( self , df ) special Evaluate the objective values for given output values. Source code in opti/objective.py def __call__ ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError __init__ ( self , name ) special Base class for optimzation objectives. Source code in opti/objective.py def __init__ ( self , name : str ): \"\"\"Base class for optimzation objectives.\"\"\" self . name = name to_config ( self ) Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError Objectives Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) Source code in opti/objective.py class Objectives : \"\"\"Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) \"\"\" def __init__ ( self , objectives : Union [ List [ Objective ], List [ Dict ]]): _objectives = [] for m in objectives : if isinstance ( m , Objective ): _objectives . append ( m ) else : _objectives . append ( make_objective ( ** m )) self . objectives = _objectives def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : return pd . concat ([ obj ( df [ obj . name ]) for obj in self . objectives ], axis = 1 ) def __repr__ ( self ): return \"Objectives( \\n \" + pprint . pformat ( self . objectives ) + \" \\n )\" def __iter__ ( self ): return iter ( self . objectives ) def __len__ ( self ): return len ( self . objectives ) def __getitem__ ( self , i : int ) -> Objective : return self . objectives [ i ] @property def names ( self ): return [ obj . name for obj in self ] def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds def to_config ( self ) -> List [ Dict ]: return [ obj . to_config () for obj in self . objectives ] bounds ( self , outputs ) Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7) 2 for y in [0, 10] -> ideal = 0, nadir = 7 2 Parameters: Name Type Description Default outputs Parameters Output parameters. required Source code in opti/objective.py def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds make_objective ( type , name , ** kwargs ) Make an objective from a configuration. obj = make_objective(**config) Parameters: Name Type Description Default type str objective type required name str output to optimize required Source code in opti/objective.py def make_objective ( type : str , name : str , ** kwargs ) -> Objective : \"\"\"Make an objective from a configuration. ``` obj = make_objective(**config) ``` Args: type: objective type name: output to optimize \"\"\" objective = { \"minimize\" : Minimize , \"maximize\" : Maximize , \"close-to-target\" : CloseToTarget , }[ type . lower ()] return objective ( name , ** kwargs )","title":"Objective"},{"location":"ref-objective/#objectives","text":"","title":"Objectives"},{"location":"ref-objective/#opti.objective.CloseToTarget","text":"Source code in opti/objective.py class CloseToTarget ( Objective ): def __init__ ( self , name : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: name: output to optimize target: target value exponent: exponent of the difference tolerance: only when used as output constraint. distance to target below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target self . exponent = exponent self . tolerance = tolerance def __call__ ( self , y : pd . Series ) -> pd . Series : return ( y - self . target ) . abs () ** self . exponent - self . tolerance ** self . exponent def __repr__ ( self ): return f \"CloseToTarget(' { self . name } ', target= { self . target } )\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config","title":"CloseToTarget"},{"location":"ref-objective/#opti.objective.CloseToTarget.__init__","text":"Objective for getting as close as possible to a given value. s(y) = |y - target| exponent - tolerance exponent Parameters: Name Type Description Default name str output to optimize required target float target value 0 exponent float exponent of the difference 1 tolerance float only when used as output constraint. distance to target below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 , exponent : float = 1 , tolerance : float = 0 , ): \"\"\"Objective for getting as close as possible to a given value. s(y) = |y - target| ** exponent - tolerance ** exponent Args: name: output to optimize target: target value exponent: exponent of the difference tolerance: only when used as output constraint. distance to target below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target self . exponent = exponent self . tolerance = tolerance","title":"__init__()"},{"location":"ref-objective/#opti.objective.CloseToTarget.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"close-to-target\" , target = self . target ) if self . exponent != 1 : config [ \"exponent\" ] = self . exponent if self . tolerance != 0 : config [ \"tolerance\" ] = self . tolerance return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Maximize","text":"Source code in opti/objective.py class Maximize ( Objective ): def __init__ ( self , name : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: output to maximize target: only when used as output constraint. value above which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target def __call__ ( self , y : pd . Series ) -> pd . Series : return self . target - y def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return self . target - y def __repr__ ( self ): return f \"Maximize(' { self . name } ')\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"Maximize"},{"location":"ref-objective/#opti.objective.Maximize.__init__","text":"Maximization objective s(y) = target - y Parameters: Name Type Description Default name str output to maximize required target float only when used as output constraint. value above which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 ): \"\"\"Maximization objective s(y) = target - y Args: name: output to maximize target: only when used as output constraint. value above which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target","title":"__init__()"},{"location":"ref-objective/#opti.objective.Maximize.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"maximize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Maximize.untransform","text":"Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return self . target - y","title":"untransform()"},{"location":"ref-objective/#opti.objective.Minimize","text":"Source code in opti/objective.py class Minimize ( Objective ): def __init__ ( self , name : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: name: output to minimize target: only when used as output constraint. value below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target def __call__ ( self , y : pd . Series ) -> pd . Series : return y - self . target def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target def __repr__ ( self ): return f \"Minimize(' { self . name } ')\" def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"Minimize"},{"location":"ref-objective/#opti.objective.Minimize.__init__","text":"Minimization objective s(y) = y - target Parameters: Name Type Description Default name str output to minimize required target float only when used as output constraint. value below which no further improvement is required 0 Source code in opti/objective.py def __init__ ( self , name : str , target : float = 0 ): \"\"\"Minimization objective s(y) = y - target Args: name: output to minimize target: only when used as output constraint. value below which no further improvement is required \"\"\" super () . __init__ ( name ) self . target = target","title":"__init__()"},{"location":"ref-objective/#opti.objective.Minimize.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : config = dict ( name = self . name , type = \"minimize\" ) if self . target != 0 : config [ \"target\" ] = str ( self . target ) return config","title":"to_config()"},{"location":"ref-objective/#opti.objective.Minimize.untransform","text":"Undo the transformation from output to objective value Source code in opti/objective.py def untransform ( self , y : pd . Series ) -> pd . Series : \"\"\"Undo the transformation from output to objective value\"\"\" return y + self . target","title":"untransform()"},{"location":"ref-objective/#opti.objective.Objective","text":"Source code in opti/objective.py class Objective : def __init__ ( self , name : str ): \"\"\"Base class for optimzation objectives.\"\"\" self . name = name def __call__ ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError","title":"Objective"},{"location":"ref-objective/#opti.objective.Objective.__call__","text":"Evaluate the objective values for given output values. Source code in opti/objective.py def __call__ ( self , df : pd . DataFrame ) -> pd . Series : \"\"\"Evaluate the objective values for given output values.\"\"\" raise NotImplementedError","title":"__call__()"},{"location":"ref-objective/#opti.objective.Objective.__init__","text":"Base class for optimzation objectives. Source code in opti/objective.py def __init__ ( self , name : str ): \"\"\"Base class for optimzation objectives.\"\"\" self . name = name","title":"__init__()"},{"location":"ref-objective/#opti.objective.Objective.to_config","text":"Return a json-serializable dictionary of the objective. Source code in opti/objective.py def to_config ( self ) -> Dict : \"\"\"Return a json-serializable dictionary of the objective.\"\"\" raise NotImplementedError","title":"to_config()"},{"location":"ref-objective/#opti.objective.Objectives","text":"Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) Source code in opti/objective.py class Objectives : \"\"\"Container for optimization objectives. Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint) \"\"\" def __init__ ( self , objectives : Union [ List [ Objective ], List [ Dict ]]): _objectives = [] for m in objectives : if isinstance ( m , Objective ): _objectives . append ( m ) else : _objectives . append ( make_objective ( ** m )) self . objectives = _objectives def __call__ ( self , df : pd . DataFrame ) -> pd . DataFrame : return pd . concat ([ obj ( df [ obj . name ]) for obj in self . objectives ], axis = 1 ) def __repr__ ( self ): return \"Objectives( \\n \" + pprint . pformat ( self . objectives ) + \" \\n )\" def __iter__ ( self ): return iter ( self . objectives ) def __len__ ( self ): return len ( self . objectives ) def __getitem__ ( self , i : int ) -> Objective : return self . objectives [ i ] @property def names ( self ): return [ obj . name for obj in self ] def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds def to_config ( self ) -> List [ Dict ]: return [ obj . to_config () for obj in self . objectives ]","title":"Objectives"},{"location":"ref-objective/#opti.objective.Objectives.bounds","text":"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7) 2 for y in [0, 10] -> ideal = 0, nadir = 7 2 Parameters: Name Type Description Default outputs Parameters Output parameters. required Source code in opti/objective.py def bounds ( self , outputs : Parameters ) -> pd . DataFrame : \"\"\"Compute the bounds in objective space based on the output space bounds. The bounds can be interpreted as the ideal and nadir points. Examples for continuous parameters: min y for y in [0, 10] -> ideal = 0, nadir = 10 max y for y in [0, 10] -> ideal = -10, nadir = 0 min (y - 7)**2 for y in [0, 10] -> ideal = 0, nadir = 7**2 Args: outputs: Output parameters. \"\"\" Z = self ( outputs . bounds ) bounds = pd . DataFrame ( columns = self . names , dtype = float ) bounds . loc [ \"min\" ] = Z . min ( axis = 0 ) bounds . loc [ \"max\" ] = Z . max ( axis = 0 ) for name , obj in zip ( self . names , self ): if isinstance ( obj , CloseToTarget ): bounds . loc [ \"min\" , name ] = 0 return bounds","title":"bounds()"},{"location":"ref-objective/#opti.objective.make_objective","text":"Make an objective from a configuration. obj = make_objective(**config) Parameters: Name Type Description Default type str objective type required name str output to optimize required Source code in opti/objective.py def make_objective ( type : str , name : str , ** kwargs ) -> Objective : \"\"\"Make an objective from a configuration. ``` obj = make_objective(**config) ``` Args: type: objective type name: output to optimize \"\"\" objective = { \"minimize\" : Minimize , \"maximize\" : Maximize , \"close-to-target\" : CloseToTarget , }[ type . lower ()] return objective ( name , ** kwargs )","title":"make_objective()"},{"location":"ref-parameter/","text":"Parameters Categorical ( Parameter ) Variable with a categorical domain (nominal scale, values cannot be put into order). Attributes: Name Type Description name str name of the parameter domain list list possible values Source code in opti/parameter.py class Categorical ( Parameter ): \"\"\"Variable with a categorical domain (nominal scale, values cannot be put into order). Attributes: name (str): name of the parameter domain (list): list possible values \"\"\" def __init__ ( self , name : str , domain : List [ str ], ** kwargs ): if not isinstance ( domain , list ): raise TypeError ( f \" { name } : Domain must be of type list.\" ) if len ( domain ) < 2 : raise ValueError ( f \" { name } : Domain must a least contain 2 values.\" ) if len ( set ( domain )) != len ( domain ): raise ValueError ( f \" { name } : Domain contains duplicates.\" ) super () . __init__ ( name , domain , type = \"categorical\" , ** kwargs ) def __repr__ ( self ): return f \"Categorical(' { self . name } ', domain= { self . domain } )\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return np . nan , np . nan def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( f \" { self . name } : Cannot round values for categorical parameter.\" ) return point def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float ) def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \" { self . name } : Column names don't match categorical levels: { points . columns } , { cat_cols } .\" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s def to_dummy_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain [ 1 :]}, dtype = float , ) def from_dummy_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from dummy encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols [ 1 :] for c in points . columns ]): raise ValueError ( f \" { self . name } : Column names don't match categorical levels: { points . columns } , { cat_cols } .\" ) points = points . copy () points [ cat_cols [ 0 ]] = 1 - points [ cat_cols [ 1 :]] . sum ( axis = 1 ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index ) bounds : Tuple [ float , float ] property readonly Return the domain bounds. contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) from_dummy_encoding ( self , points ) Convert points back from dummy encoding. Source code in opti/parameter.py def from_dummy_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from dummy encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols [ 1 :] for c in points . columns ]): raise ValueError ( f \" { self . name } : Column names don't match categorical levels: { points . columns } , { cat_cols } .\" ) points = points . copy () points [ cat_cols [ 0 ]] = 1 - points [ cat_cols [ 1 :]] . sum ( axis = 1 ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s from_label_encoding ( self , points ) Convert points back from label-encoding. Source code in opti/parameter.py def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index ) from_onehot_encoding ( self , points ) Convert points back from one-hot encoding. Source code in opti/parameter.py def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \" { self . name } : Column names don't match categorical levels: { points . columns } , { cat_cols } .\" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point . Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( f \" { self . name } : Cannot round values for categorical parameter.\" ) return point sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) to_dummy_encoding ( self , points ) Convert points to a dummy-hot encoding, dropping the first categorical level. Source code in opti/parameter.py def to_dummy_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain [ 1 :]}, dtype = float , ) to_label_encoding ( self , points ) Convert points to label-encoding. Source code in opti/parameter.py def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s to_onehot_encoding ( self , points ) Convert points to a one-hot encoding. Source code in opti/parameter.py def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float ) Continuous ( Parameter ) Variable that can take on any real value in the specified domain. Attributes: Name Type Description name str name of the parameter domain list [lower bound, upper bound] Source code in opti/parameter.py class Continuous ( Parameter ): \"\"\"Variable that can take on any real value in the specified domain. Attributes: name (str): name of the parameter domain (list): [lower bound, upper bound] \"\"\" def __init__ ( self , name : str , domain : Optional [ Sequence ] = None , ** kwargs , ): if domain is None : domain = [ - np . inf , np . inf ] else : if len ( domain ) != 2 : raise ValueError ( f \" { name } : Domain must consist of two values [low, high].\" ) # convert None to +/- inf and string to float low = - np . inf if domain [ 0 ] is None else float ( domain [ 0 ]) high = np . inf if domain [ 1 ] is None else float ( domain [ 1 ]) if high < low : raise ValueError ( f \" { name } : Lower bound { low } must be less than upper bound { high } .\" ) self . low = low self . high = high super () . __init__ ( name = name , domain = [ low , high ], type = \"continuous\" , ** kwargs ) def __repr__ ( self ): if np . isfinite ( self . low ) or np . isfinite ( self . high ): return f \"Continuous(' { self . name } ', domain= { self . domain } )\" else : return f \"Continuous(' { self . name } ')\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return self . low , self . high def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high ) def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n )) def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type ) low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) if low is not None or high is not None : conf . update ({ \"domain\" : [ low , high ]}) conf . update ( self . extra_fields ) return conf def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return points * ( self . high - self . low ) + self . low bounds : Tuple [ float , float ] property readonly Return the domain bounds. contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high ) from_unit_range ( self , points ) Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. Source code in opti/parameter.py def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return points * ( self . high - self . low ) + self . low round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high ) sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n )) to_config ( self ) Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type ) low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) if low is not None or high is not None : conf . update ({ \"domain\" : [ low , high ]}) conf . update ( self . extra_fields ) return conf to_unit_range ( self , points ) Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) Discrete ( Parameter ) Variable with a discrete domain (ordinal scale). Attributes: Name Type Description name str name of the parameter domain list list of possible numeric values Source code in opti/parameter.py class Discrete ( Parameter ): \"\"\"Variable with a discrete domain (ordinal scale). Attributes: name (str): name of the parameter domain (list): list of possible numeric values \"\"\" def __init__ ( self , name : str , domain : Sequence , ** kwargs ): if len ( domain ) < 1 : raise ValueError ( f \" { name } : Domain must contain at least one value.\" ) try : # convert to a sorted list of floats domain = np . sort ( np . array ( domain ) . astype ( float )) . tolist () except ValueError : raise ValueError ( f \" { name } : Domain contains non-numeric values.\" ) if len ( set ( domain )) != len ( domain ): raise ValueError ( f \" { name } : Domain contains duplicates.\" ) self . low = min ( domain ) self . high = max ( domain ) super () . __init__ ( name , domain , type = \"discrete\" , ** kwargs ) def __repr__ ( self ): return f \"Discrete(' { self . name } ', domain= { self . domain } )\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return self . low , self . high def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index ) def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : points = points * ( self . high - self . low ) + self . low return self . round ( points ) bounds : Tuple [ float , float ] property readonly Return the domain bounds. contains ( self , point ) Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) from_unit_range ( self , points ) Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. Source code in opti/parameter.py def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : points = points * ( self . high - self . low ) + self . low return self . round ( points ) round ( self , point ) Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index ) sample ( self , n = 1 ) Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) to_unit_range ( self , points ) Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) Parameter Parameter base class. Source code in opti/parameter.py class Parameter : \"\"\"Parameter base class.\"\"\" def __init__ ( self , name : str , domain : Sequence , type : str = None , ** kwargs ): self . name = name self . domain = domain self . type = type self . extra_fields = kwargs def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf to_config ( self ) Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf Parameters Set of parameters representing either the input or the output parameter space. Source code in opti/parameter.py class Parameters : \"\"\"Set of parameters representing either the input or the output parameter space.\"\"\" def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Parameters([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Parameters([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Parameters expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d def __repr__ ( self ): return \"Parameters( \\n \" + pprint . pformat ( list ( self . parameters . values ())) + \" \\n )\" def __iter__ ( self ): return iter ( self . parameters . values ()) def __getitem__ ( self , name ): return self . parameters [ name ] def __len__ ( self ): return len ( self . parameters ) def __add__ ( self , other ): parameters = list ( self . parameters . values ()) + list ( other . parameters . values ()) return Parameters ( parameters ) @property def names ( self ): return list ( self . parameters . keys ()) @property def bounds ( self ) -> pd . DataFrame : \"\"\"Return the parameter bounds.\"\"\" return pd . DataFrame ({ p . name : p . bounds for p in self }, index = [ \"min\" , \"max\" ]) def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the domain of each parameter.\"\"\" if isinstance ( points , pd . DataFrame ): points = points [ self . names ] b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis = 1 ) return b . all ( axis = 1 ) def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 ) def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 ) def transform ( self , points : pd . DataFrame , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"onehot-encode\" , ) -> pd . DataFrame : \"\"\"Transfrom the given dataframe according to a set of transformation rules. Args: points (pd.DataFrame): Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns. continuous (str, optional): Transform for continuous parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] discrete (str, optional): Transform for discrete parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] categorical (str, optional): Transform for categoricals. Options are - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1] - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1] - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2] - \"none\": keep values unchanged Raises: ValueError: Unknown transform. Returns: pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped. \"\"\" transformed = [] for p in self : s = points [ p . name ] if isinstance ( p , Continuous ): if continuous == \"none\" : transformed . append ( s ) elif continuous == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown continuous transform { continuous } .\" ) if isinstance ( p , Discrete ): if discrete == \"none\" : transformed . append ( s ) elif discrete == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown discrete transform { discrete } .\" ) if isinstance ( p , Categorical ): if categorical == \"none\" : transformed . append ( s ) elif categorical == \"onehot-encode\" : transformed . append ( p . to_onehot_encoding ( s )) elif categorical == \"dummy-encode\" : transformed . append ( p . to_dummy_encoding ( s )) elif categorical == \"label-encode\" : transformed . append ( p . to_label_encoding ( s )) else : raise ValueError ( f \"Unknown categorical transform { categorical } .\" ) return pd . concat ( transformed , axis = 1 ) def to_config ( self ) -> List [ dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()] bounds : DataFrame property readonly Return the parameter bounds. __init__ ( self , parameters ) special It can be constructed either from a list / tuple (of at least one) Parameter objects Parameters([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) or from a list / tuple of dicts Parameters([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) In particular, Parameters(). init and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf Source code in opti/parameter.py def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Parameters([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Parameters([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Parameters expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d contains ( self , points ) Check if points are inside the domain of each parameter. Source code in opti/parameter.py def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the domain of each parameter.\"\"\" if isinstance ( points , pd . DataFrame ): points = points [ self . names ] b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis = 1 ) return b . all ( axis = 1 ) round ( self , points ) Round points to the closest contained values. Source code in opti/parameter.py def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 ) sample ( self , n = 1 ) Draw uniformly distributed random samples. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 ) to_config ( self ) Configuration of the parameter space. Source code in opti/parameter.py def to_config ( self ) -> List [ dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()] transform ( self , points , continuous = 'none' , discrete = 'none' , categorical = 'onehot-encode' ) Transfrom the given dataframe according to a set of transformation rules. Parameters: Name Type Description Default points pd.DataFrame Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns. required continuous str Transform for continuous parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] 'none' discrete str Transform for discrete parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] 'none' categorical str Transform for categoricals. Options are - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1] - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1] - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2] - \"none\": keep values unchanged 'onehot-encode' Exceptions: Type Description ValueError Unknown transform. Returns: Type Description pd.DataFrame Transformed points. Columns that don't correspond to parameters are dropped. Source code in opti/parameter.py def transform ( self , points : pd . DataFrame , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"onehot-encode\" , ) -> pd . DataFrame : \"\"\"Transfrom the given dataframe according to a set of transformation rules. Args: points (pd.DataFrame): Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns. continuous (str, optional): Transform for continuous parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] discrete (str, optional): Transform for discrete parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] categorical (str, optional): Transform for categoricals. Options are - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1] - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1] - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2] - \"none\": keep values unchanged Raises: ValueError: Unknown transform. Returns: pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped. \"\"\" transformed = [] for p in self : s = points [ p . name ] if isinstance ( p , Continuous ): if continuous == \"none\" : transformed . append ( s ) elif continuous == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown continuous transform { continuous } .\" ) if isinstance ( p , Discrete ): if discrete == \"none\" : transformed . append ( s ) elif discrete == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown discrete transform { discrete } .\" ) if isinstance ( p , Categorical ): if categorical == \"none\" : transformed . append ( s ) elif categorical == \"onehot-encode\" : transformed . append ( p . to_onehot_encoding ( s )) elif categorical == \"dummy-encode\" : transformed . append ( p . to_dummy_encoding ( s )) elif categorical == \"label-encode\" : transformed . append ( p . to_label_encoding ( s )) else : raise ValueError ( f \"Unknown categorical transform { categorical } .\" ) return pd . concat ( transformed , axis = 1 ) make_parameter ( name , type , domain = None , ** kwargs ) Make a parameter object from a configuration p = make_parameter(**config) Parameters: Name Type Description Default type str \"continuous\", \"discrete\" or \"categorical\" required name str Name of the parameter required domain list Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] None Source code in opti/parameter.py def make_parameter ( name : str , type : str , domain : Optional [ Sequence ] = None , ** kwargs , ): \"\"\"Make a parameter object from a configuration p = make_parameter(**config) Args: type (str): \"continuous\", \"discrete\" or \"categorical\" name (str): Name of the parameter domain (list): Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] \"\"\" parameter = { \"continuous\" : Continuous , \"discrete\" : Discrete , \"categorical\" : Categorical , }[ type . lower ()] if domain is None and parameter is not Continuous : raise ValueError ( f \"Domain not given for parameter { name } .\" ) return parameter ( name = name , domain = domain , ** kwargs )","title":"Parameter"},{"location":"ref-parameter/#parameters","text":"","title":"Parameters"},{"location":"ref-parameter/#opti.parameter.Categorical","text":"Variable with a categorical domain (nominal scale, values cannot be put into order). Attributes: Name Type Description name str name of the parameter domain list list possible values Source code in opti/parameter.py class Categorical ( Parameter ): \"\"\"Variable with a categorical domain (nominal scale, values cannot be put into order). Attributes: name (str): name of the parameter domain (list): list possible values \"\"\" def __init__ ( self , name : str , domain : List [ str ], ** kwargs ): if not isinstance ( domain , list ): raise TypeError ( f \" { name } : Domain must be of type list.\" ) if len ( domain ) < 2 : raise ValueError ( f \" { name } : Domain must a least contain 2 values.\" ) if len ( set ( domain )) != len ( domain ): raise ValueError ( f \" { name } : Domain contains duplicates.\" ) super () . __init__ ( name , domain , type = \"categorical\" , ** kwargs ) def __repr__ ( self ): return f \"Categorical(' { self . name } ', domain= { self . domain } )\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return np . nan , np . nan def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( f \" { self . name } : Cannot round values for categorical parameter.\" ) return point def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float ) def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \" { self . name } : Column names don't match categorical levels: { points . columns } , { cat_cols } .\" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s def to_dummy_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain [ 1 :]}, dtype = float , ) def from_dummy_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from dummy encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols [ 1 :] for c in points . columns ]): raise ValueError ( f \" { self . name } : Column names don't match categorical levels: { points . columns } , { cat_cols } .\" ) points = points . copy () points [ cat_cols [ 0 ]] = 1 - points [ cat_cols [ 1 :]] . sum ( axis = 1 ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index )","title":"Categorical"},{"location":"ref-parameter/#opti.parameter.Categorical.bounds","text":"Return the domain bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Categorical.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Categorical.from_dummy_encoding","text":"Convert points back from dummy encoding. Source code in opti/parameter.py def from_dummy_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from dummy encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols [ 1 :] for c in points . columns ]): raise ValueError ( f \" { self . name } : Column names don't match categorical levels: { points . columns } , { cat_cols } .\" ) points = points . copy () points [ cat_cols [ 0 ]] = 1 - points [ cat_cols [ 1 :]] . sum ( axis = 1 ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s","title":"from_dummy_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.from_label_encoding","text":"Convert points back from label-encoding. Source code in opti/parameter.py def from_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points back from label-encoding.\"\"\" enc = np . array ( self . domain ) return pd . Series ( enc [ points ], index = points . index )","title":"from_label_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.from_onehot_encoding","text":"Convert points back from one-hot encoding. Source code in opti/parameter.py def from_onehot_encoding ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Convert points back from one-hot encoding.\"\"\" cat_cols = [ f \" { self . name }{ _CAT_SEP }{ c } \" for c in self . domain ] if np . any ([ c not in cat_cols for c in points . columns ]): raise ValueError ( f \" { self . name } : Column names don't match categorical levels: { points . columns } , { cat_cols } .\" ) s = points . idxmax ( 1 ) . str . split ( _CAT_SEP , expand = True )[ 1 ] s . name = self . name return s","title":"from_onehot_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point . Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point`. \"\"\" if not np . all ( self . contains ( point )): raise ValueError ( f \" { self . name } : Cannot round values for categorical parameter.\" ) return point","title":"round()"},{"location":"ref-parameter/#opti.parameter.Categorical.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Categorical.to_dummy_encoding","text":"Convert points to a dummy-hot encoding, dropping the first categorical level. Source code in opti/parameter.py def to_dummy_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain [ 1 :]}, dtype = float , )","title":"to_dummy_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.to_label_encoding","text":"Convert points to label-encoding. Source code in opti/parameter.py def to_label_encoding ( self , points : pd . Series ) -> pd . Series : \"\"\"Convert points to label-encoding.\"\"\" enc = pd . Series ( range ( len ( self . domain )), index = list ( self . domain )) s = enc [ points ] s . index = points . index s . name = self . name return s","title":"to_label_encoding()"},{"location":"ref-parameter/#opti.parameter.Categorical.to_onehot_encoding","text":"Convert points to a one-hot encoding. Source code in opti/parameter.py def to_onehot_encoding ( self , points : pd . Series ) -> pd . DataFrame : \"\"\"Convert points to a one-hot encoding.\"\"\" return pd . DataFrame ( { f \" { self . name }{ _CAT_SEP }{ c } \" : points == c for c in self . domain }, dtype = float )","title":"to_onehot_encoding()"},{"location":"ref-parameter/#opti.parameter.Continuous","text":"Variable that can take on any real value in the specified domain. Attributes: Name Type Description name str name of the parameter domain list [lower bound, upper bound] Source code in opti/parameter.py class Continuous ( Parameter ): \"\"\"Variable that can take on any real value in the specified domain. Attributes: name (str): name of the parameter domain (list): [lower bound, upper bound] \"\"\" def __init__ ( self , name : str , domain : Optional [ Sequence ] = None , ** kwargs , ): if domain is None : domain = [ - np . inf , np . inf ] else : if len ( domain ) != 2 : raise ValueError ( f \" { name } : Domain must consist of two values [low, high].\" ) # convert None to +/- inf and string to float low = - np . inf if domain [ 0 ] is None else float ( domain [ 0 ]) high = np . inf if domain [ 1 ] is None else float ( domain [ 1 ]) if high < low : raise ValueError ( f \" { name } : Lower bound { low } must be less than upper bound { high } .\" ) self . low = low self . high = high super () . __init__ ( name = name , domain = [ low , high ], type = \"continuous\" , ** kwargs ) def __repr__ ( self ): if np . isfinite ( self . low ) or np . isfinite ( self . high ): return f \"Continuous(' { self . name } ', domain= { self . domain } )\" else : return f \"Continuous(' { self . name } ')\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return self . low , self . high def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high ) def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n )) def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type ) low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) if low is not None or high is not None : conf . update ({ \"domain\" : [ low , high ]}) conf . update ( self . extra_fields ) return conf def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return points * ( self . high - self . low ) + self . low","title":"Continuous"},{"location":"ref-parameter/#opti.parameter.Continuous.bounds","text":"Return the domain bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Continuous.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" return ( self . low <= point ) & ( point <= self . high )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Continuous.from_unit_range","text":"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. Source code in opti/parameter.py def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return points * ( self . high - self . low ) + self . low","title":"from_unit_range()"},{"location":"ref-parameter/#opti.parameter.Continuous.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" return np . clip ( point , self . low , self . high )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Continuous.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" low = max ( self . low , np . finfo ( np . float32 ) . min ) high = min ( self . high , np . finfo ( np . float32 ) . max ) return pd . Series ( name = self . name , data = np . random . uniform ( low , high , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Continuous.to_config","text":"Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type ) low = None if np . isinf ( self . low ) else float ( self . low ) high = None if np . isinf ( self . high ) else float ( self . high ) if low is not None or high is not None : conf . update ({ \"domain\" : [ low , high ]}) conf . update ( self . extra_fields ) return conf","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.Continuous.to_unit_range","text":"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low )","title":"to_unit_range()"},{"location":"ref-parameter/#opti.parameter.Discrete","text":"Variable with a discrete domain (ordinal scale). Attributes: Name Type Description name str name of the parameter domain list list of possible numeric values Source code in opti/parameter.py class Discrete ( Parameter ): \"\"\"Variable with a discrete domain (ordinal scale). Attributes: name (str): name of the parameter domain (list): list of possible numeric values \"\"\" def __init__ ( self , name : str , domain : Sequence , ** kwargs ): if len ( domain ) < 1 : raise ValueError ( f \" { name } : Domain must contain at least one value.\" ) try : # convert to a sorted list of floats domain = np . sort ( np . array ( domain ) . astype ( float )) . tolist () except ValueError : raise ValueError ( f \" { name } : Domain contains non-numeric values.\" ) if len ( set ( domain )) != len ( domain ): raise ValueError ( f \" { name } : Domain contains duplicates.\" ) self . low = min ( domain ) self . high = max ( domain ) super () . __init__ ( name , domain , type = \"discrete\" , ** kwargs ) def __repr__ ( self ): return f \"Discrete(' { self . name } ', domain= { self . domain } )\" @property def bounds ( self ) -> Tuple [ float , float ]: \"\"\"Return the domain bounds.\"\"\" return self . low , self . high def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain ) def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index ) def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n )) def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low ) def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : points = points * ( self . high - self . low ) + self . low return self . round ( points )","title":"Discrete"},{"location":"ref-parameter/#opti.parameter.Discrete.bounds","text":"Return the domain bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Discrete.contains","text":"Check if a point is in contained in the domain. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with boolean datatype. Source code in opti/parameter.py def contains ( self , point ): \"\"\"Check if a point is in contained in the domain. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with boolean datatype. \"\"\" if not np . isscalar ( point ): point = np . array ( point ) return np . isin ( point , self . domain )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Discrete.from_unit_range","text":"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. Source code in opti/parameter.py def from_unit_range ( self , points ): \"\"\"Transform points from the unit range: [0, 1] -> [low, high]. A rounding is applied to correct for numerical precision. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : points = points * ( self . high - self . low ) + self . low return self . round ( points )","title":"from_unit_range()"},{"location":"ref-parameter/#opti.parameter.Discrete.round","text":"Round a point to the closest contained values. Parameters: Name Type Description Default point float, np.ndarray, pd.Series or pd.Dataframe parameter value(s). required Returns: Type Description Object of the same type as point with values clipped to parameter bounds. Source code in opti/parameter.py def round ( self , point ): \"\"\"Round a point to the closest contained values. Args: point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s). Returns: Object of the same type as `point` with values clipped to parameter bounds. \"\"\" if np . isscalar ( point ): i = np . argmin ( np . abs ( np . array ( self . domain ) - point )) return self . domain [ i ] closest = [ np . argmin ( np . abs ( np . array ( self . domain ) - p )) for p in point ] rounded = np . array ( self . domain )[ closest ] if isinstance ( point , np . ndarray ): return rounded elif isinstance ( point , pd . Series ): return pd . Series ( name = self . name , data = rounded , index = point . index ) elif isinstance ( point , pd . DataFrame ): return pd . DataFrame ({ self . name : rounded }, index = point . index )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Discrete.sample","text":"Draw random samples from the domain. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . Series : \"\"\"Draw random samples from the domain.\"\"\" return pd . Series ( name = self . name , data = np . random . choice ( self . domain , n ))","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Discrete.to_unit_range","text":"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. Source code in opti/parameter.py def to_unit_range ( self , points ): \"\"\"Transform points to the unit range: [low, high] -> [0, 1]. Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high. \"\"\" if np . isclose ( self . low , self . high ): return points else : return ( points - self . low ) / ( self . high - self . low )","title":"to_unit_range()"},{"location":"ref-parameter/#opti.parameter.Parameter","text":"Parameter base class. Source code in opti/parameter.py class Parameter : \"\"\"Parameter base class.\"\"\" def __init__ ( self , name : str , domain : Sequence , type : str = None , ** kwargs ): self . name = name self . domain = domain self . type = type self . extra_fields = kwargs def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf","title":"Parameter"},{"location":"ref-parameter/#opti.parameter.Parameter.to_config","text":"Return a json-serializable configuration dict. Source code in opti/parameter.py def to_config ( self ) -> dict : \"\"\"Return a json-serializable configuration dict.\"\"\" conf = dict ( name = self . name , type = self . type , domain = self . domain ) conf . update ( self . extra_fields ) return conf","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.Parameters","text":"Set of parameters representing either the input or the output parameter space. Source code in opti/parameter.py class Parameters : \"\"\"Set of parameters representing either the input or the output parameter space.\"\"\" def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Parameters([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Parameters([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Parameters expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d def __repr__ ( self ): return \"Parameters( \\n \" + pprint . pformat ( list ( self . parameters . values ())) + \" \\n )\" def __iter__ ( self ): return iter ( self . parameters . values ()) def __getitem__ ( self , name ): return self . parameters [ name ] def __len__ ( self ): return len ( self . parameters ) def __add__ ( self , other ): parameters = list ( self . parameters . values ()) + list ( other . parameters . values ()) return Parameters ( parameters ) @property def names ( self ): return list ( self . parameters . keys ()) @property def bounds ( self ) -> pd . DataFrame : \"\"\"Return the parameter bounds.\"\"\" return pd . DataFrame ({ p . name : p . bounds for p in self }, index = [ \"min\" , \"max\" ]) def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the domain of each parameter.\"\"\" if isinstance ( points , pd . DataFrame ): points = points [ self . names ] b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis = 1 ) return b . all ( axis = 1 ) def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 ) def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 ) def transform ( self , points : pd . DataFrame , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"onehot-encode\" , ) -> pd . DataFrame : \"\"\"Transfrom the given dataframe according to a set of transformation rules. Args: points (pd.DataFrame): Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns. continuous (str, optional): Transform for continuous parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] discrete (str, optional): Transform for discrete parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] categorical (str, optional): Transform for categoricals. Options are - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1] - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1] - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2] - \"none\": keep values unchanged Raises: ValueError: Unknown transform. Returns: pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped. \"\"\" transformed = [] for p in self : s = points [ p . name ] if isinstance ( p , Continuous ): if continuous == \"none\" : transformed . append ( s ) elif continuous == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown continuous transform { continuous } .\" ) if isinstance ( p , Discrete ): if discrete == \"none\" : transformed . append ( s ) elif discrete == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown discrete transform { discrete } .\" ) if isinstance ( p , Categorical ): if categorical == \"none\" : transformed . append ( s ) elif categorical == \"onehot-encode\" : transformed . append ( p . to_onehot_encoding ( s )) elif categorical == \"dummy-encode\" : transformed . append ( p . to_dummy_encoding ( s )) elif categorical == \"label-encode\" : transformed . append ( p . to_label_encoding ( s )) else : raise ValueError ( f \"Unknown categorical transform { categorical } .\" ) return pd . concat ( transformed , axis = 1 ) def to_config ( self ) -> List [ dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()]","title":"Parameters"},{"location":"ref-parameter/#opti.parameter.Parameters.bounds","text":"Return the parameter bounds.","title":"bounds"},{"location":"ref-parameter/#opti.parameter.Parameters.__init__","text":"It can be constructed either from a list / tuple (of at least one) Parameter objects Parameters([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) or from a list / tuple of dicts Parameters([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) In particular, Parameters(). init and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf Source code in opti/parameter.py def __init__ ( self , parameters ): \"\"\" It can be constructed either from a list / tuple (of at least one) Parameter objects ``` Parameters([ Continuous(name=\"foo\", domain=[1, 10]), Discrete(name=\"bar\", domain=[1, 2, 3, 4]), Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]), ]) ``` or from a list / tuple of dicts ``` Parameters([ {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]}, {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]}, {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"}, ]) ``` In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf \"\"\" if not isinstance ( parameters , ( list , tuple )): raise TypeError ( \"Parameters expects a list or tuple of parameters.\" ) self . parameters = {} for d in parameters : if not isinstance ( d , Parameter ): d = make_parameter ( ** d ) if d . name in self . parameters : raise ValueError ( f \"Duplicate parameter name { d . name } \" ) self . parameters [ d . name ] = d","title":"__init__()"},{"location":"ref-parameter/#opti.parameter.Parameters.contains","text":"Check if points are inside the domain of each parameter. Source code in opti/parameter.py def contains ( self , points : pd . DataFrame ) -> pd . Series : \"\"\"Check if points are inside the domain of each parameter.\"\"\" if isinstance ( points , pd . DataFrame ): points = points [ self . names ] b = np . stack ([ self [ k ] . contains ( v ) for k , v in points . iteritems ()], axis = 1 ) return b . all ( axis = 1 )","title":"contains()"},{"location":"ref-parameter/#opti.parameter.Parameters.round","text":"Round points to the closest contained values. Source code in opti/parameter.py def round ( self , points : pd . DataFrame ) -> pd . DataFrame : \"\"\"Round points to the closest contained values.\"\"\" return pd . concat ([ self [ k ] . round ( v ) for k , v in points . iteritems ()], axis = 1 )","title":"round()"},{"location":"ref-parameter/#opti.parameter.Parameters.sample","text":"Draw uniformly distributed random samples. Source code in opti/parameter.py def sample ( self , n : int = 1 ) -> pd . DataFrame : \"\"\"Draw uniformly distributed random samples.\"\"\" return pd . concat ([ param . sample ( n ) for param in self ], axis = 1 )","title":"sample()"},{"location":"ref-parameter/#opti.parameter.Parameters.to_config","text":"Configuration of the parameter space. Source code in opti/parameter.py def to_config ( self ) -> List [ dict ]: \"\"\"Configuration of the parameter space.\"\"\" return [ param . to_config () for param in self . parameters . values ()]","title":"to_config()"},{"location":"ref-parameter/#opti.parameter.Parameters.transform","text":"Transfrom the given dataframe according to a set of transformation rules. Parameters: Name Type Description Default points pd.DataFrame Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns. required continuous str Transform for continuous parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] 'none' discrete str Transform for discrete parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] 'none' categorical str Transform for categoricals. Options are - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1] - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1] - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2] - \"none\": keep values unchanged 'onehot-encode' Exceptions: Type Description ValueError Unknown transform. Returns: Type Description pd.DataFrame Transformed points. Columns that don't correspond to parameters are dropped. Source code in opti/parameter.py def transform ( self , points : pd . DataFrame , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"onehot-encode\" , ) -> pd . DataFrame : \"\"\"Transfrom the given dataframe according to a set of transformation rules. Args: points (pd.DataFrame): Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns. continuous (str, optional): Transform for continuous parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] discrete (str, optional): Transform for discrete parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1] categorical (str, optional): Transform for categoricals. Options are - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1] - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1] - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2] - \"none\": keep values unchanged Raises: ValueError: Unknown transform. Returns: pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped. \"\"\" transformed = [] for p in self : s = points [ p . name ] if isinstance ( p , Continuous ): if continuous == \"none\" : transformed . append ( s ) elif continuous == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown continuous transform { continuous } .\" ) if isinstance ( p , Discrete ): if discrete == \"none\" : transformed . append ( s ) elif discrete == \"normalize\" : transformed . append ( p . to_unit_range ( s )) else : raise ValueError ( f \"Unknown discrete transform { discrete } .\" ) if isinstance ( p , Categorical ): if categorical == \"none\" : transformed . append ( s ) elif categorical == \"onehot-encode\" : transformed . append ( p . to_onehot_encoding ( s )) elif categorical == \"dummy-encode\" : transformed . append ( p . to_dummy_encoding ( s )) elif categorical == \"label-encode\" : transformed . append ( p . to_label_encoding ( s )) else : raise ValueError ( f \"Unknown categorical transform { categorical } .\" ) return pd . concat ( transformed , axis = 1 )","title":"transform()"},{"location":"ref-parameter/#opti.parameter.make_parameter","text":"Make a parameter object from a configuration p = make_parameter(**config) Parameters: Name Type Description Default type str \"continuous\", \"discrete\" or \"categorical\" required name str Name of the parameter required domain list Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] None Source code in opti/parameter.py def make_parameter ( name : str , type : str , domain : Optional [ Sequence ] = None , ** kwargs , ): \"\"\"Make a parameter object from a configuration p = make_parameter(**config) Args: type (str): \"continuous\", \"discrete\" or \"categorical\" name (str): Name of the parameter domain (list): Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"] \"\"\" parameter = { \"continuous\" : Continuous , \"discrete\" : Discrete , \"categorical\" : Categorical , }[ type . lower ()] if domain is None and parameter is not Continuous : raise ValueError ( f \"Domain not given for parameter { name } .\" ) return parameter ( name = name , domain = domain , ** kwargs )","title":"make_parameter()"},{"location":"ref-problem/","text":"Problem Problem Source code in opti/problem.py class Problem : def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ DataFrameLike ] = None , optima : Optional [ DataFrameLike ] = None , name : Optional [ str ] = None , ** kwargs , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f if isinstance ( data , dict ): data = pd . read_json ( json . dumps ( data ), orient = \"split\" ) if isinstance ( optima , dict ): optima = pd . read_json ( json . dumps ( optima ), orient = \"split\" ) self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models () @property def n_inputs ( self ) -> int : return len ( self . inputs ) @property def n_outputs ( self ) -> int : return len ( self . outputs ) @property def n_objectives ( self ) -> int : return len ( self . objectives ) @property def n_constraints ( self ) -> int : return 0 if self . constraints is None else len ( self . constraints ) def __repr__ ( self ): return self . __str__ () def __str__ ( self ): s = \"Problem( \\n \" s += f \"name= { self . name } , \\n \" s += f \"inputs= { self . inputs } , \\n \" s += f \"outputs= { self . outputs } , \\n \" s += f \"objectives= { self . objectives } , \\n \" if self . output_constraints is not None : s += f \"output_constraints= { self . output_constraints } , \\n \" if self . constraints is not None : s += f \"constraints= { self . constraints } , \\n \" if self . models is not None : s += f \"models= { self . models } , \\n \" if self . data is not None : s += f \"data= \\n { self . data . head () } \\n \" if self . optima is not None : s += f \"optima= \\n { self . optima . head () } \\n \" return s + \")\" @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" return Problem ( ** config ) def to_config ( self ) -> dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem ( ** config ) def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , separators = ( \",\" , \":\" )) outfile . write ( b . encode ( \"utf-8\" )) def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check for duplicate names duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if all objectives refer to an output for obj in self . objectives : if obj . name not in self . outputs . names : raise ValueError ( f \"Objective refers to unknown parameter: { obj . name } \" ) def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # data must contain all parameters if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } is missing. Data must contain all parameters.\" ) # data for continuous / discrete parameters must be numeric if isinstance ( p , ( Continuous , Discrete )): ok = is_numeric_dtype ( data [ p . name ]) or data [ p . name ] . isnull () . all () if not ok : raise ValueError ( f \"Parameter { p . name } contains non-numeric values. Data for continuous / discrete parameters must be numeric.\" ) # categorical levels in data must be specified elif isinstance ( p , Categorical ): ok = p . contains ( data [ p . name ]) | data [ p . name ] . isna () if not ok . all (): unknowns = data [ p . name ][ ~ ok ] . unique () . tolist () raise ValueError ( f \"Data for parameter { p . name } contains unknown values: { unknowns } . All categorical levels must be specified.\" ) # inputs must be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Input parameter { p . name } has missing data. Inputs must be complete.\" ) # outputs must have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"Output parameter { p . name } has no data. Outputs must have at least one observation.\" ) def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" ) def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : for p in self . inputs : # Categorical levels are required to be strings. Ensure that the corresponding data is as well. if isinstance ( p , Categorical ): nulls = data [ p . name ] . isna () data [ p . name ] = data [ p . name ] . astype ( str ) . mask ( nulls , np . nan ) self . check_data ( data ) self . data = data def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 ) def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints ) def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\" if self . f is None : raise NotImplementedError ( \"problem.f is not implemented for the problem.\" ) X = self . sample_inputs ( n_samples ) Y = self . f ( X ) self . data = pd . concat ([ X , Y ], axis = 1 ) __init__ ( self , inputs , outputs , objectives = None , constraints = None , output_constraints = None , f = None , models = None , data = None , optima = None , name = None , ** kwargs ) special An optimization problem. Parameters: Name Type Description Default inputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Input parameters. required outputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Output parameters. required objectives Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Optimization objectives. Defaults to minimization. None constraints Union[opti.constraint.Constraints, List[opti.constraint.Constraint], List[Dict]] Constraints on the inputs. None output_constraints Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Constraints on the outputs. None f Optional[Callable] Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame None data Union[pandas.core.frame.DataFrame, Dict] Experimental data. None optima Union[pandas.core.frame.DataFrame, Dict] Pareto optima. None name Optional[str] Name of the problem. None Source code in opti/problem.py def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ DataFrameLike ] = None , optima : Optional [ DataFrameLike ] = None , name : Optional [ str ] = None , ** kwargs , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f if isinstance ( data , dict ): data = pd . read_json ( json . dumps ( data ), orient = \"split\" ) if isinstance ( optima , dict ): optima = pd . read_json ( json . dumps ( optima ), orient = \"split\" ) self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models () add_data ( self , data ) Add a number of data points. Source code in opti/problem.py def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 ) check_data ( self , data ) Check if data is consistent with input and output parameters. Source code in opti/problem.py def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # data must contain all parameters if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } is missing. Data must contain all parameters.\" ) # data for continuous / discrete parameters must be numeric if isinstance ( p , ( Continuous , Discrete )): ok = is_numeric_dtype ( data [ p . name ]) or data [ p . name ] . isnull () . all () if not ok : raise ValueError ( f \"Parameter { p . name } contains non-numeric values. Data for continuous / discrete parameters must be numeric.\" ) # categorical levels in data must be specified elif isinstance ( p , Categorical ): ok = p . contains ( data [ p . name ]) | data [ p . name ] . isna () if not ok . all (): unknowns = data [ p . name ][ ~ ok ] . unique () . tolist () raise ValueError ( f \"Data for parameter { p . name } contains unknown values: { unknowns } . All categorical levels must be specified.\" ) # inputs must be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Input parameter { p . name } has missing data. Inputs must be complete.\" ) # outputs must have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"Output parameter { p . name } has no data. Outputs must have at least one observation.\" ) check_models ( self ) Check if the models are well defined Source code in opti/problem.py def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" ) check_problem ( self ) Check if input and output parameters are consistent. Source code in opti/problem.py def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check for duplicate names duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if all objectives refer to an output for obj in self . objectives : if obj . name not in self . outputs . names : raise ValueError ( f \"Objective refers to unknown parameter: { obj . name } \" ) create_initial_data ( self , n_samples = 10 ) Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs. Source code in opti/problem.py def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\" if self . f is None : raise NotImplementedError ( \"problem.f is not implemented for the problem.\" ) X = self . sample_inputs ( n_samples ) Y = self . f ( X ) self . data = pd . concat ([ X , Y ], axis = 1 ) from_config ( config ) staticmethod Create a Problem instance from a configuration dict. Source code in opti/problem.py @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" return Problem ( ** config ) from_json ( fname ) staticmethod Read a problem from a JSON file. Source code in opti/problem.py @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem ( ** config ) get_X ( self , data = None ) Return the input values in data or self.data . Source code in opti/problem.py def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values get_XY ( self , outputs = None , data = None , continuous = 'none' , discrete = 'none' , categorical = 'none' ) Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Parameters: Name Type Description Default outputs optional Subset of the outputs to consider. None data optional Dataframe to consider instead of problem.data None Source code in opti/problem.py def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y get_X_bounds ( self ) Return the lower and upper data bounds. Source code in opti/problem.py def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi get_Y ( self , data = None ) Return the output values in data or self.data . Source code in opti/problem.py def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values get_data ( self ) Return self.data if it exists or an empty dataframe. Source code in opti/problem.py def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data sample_inputs ( self , n_samples = 10 ) Uniformly sample points from the input space subject to the constraints. Source code in opti/problem.py def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints ) set_data ( self , data ) Set the data. Source code in opti/problem.py def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : for p in self . inputs : # Categorical levels are required to be strings. Ensure that the corresponding data is as well. if isinstance ( p , Categorical ): nulls = data [ p . name ] . isna () data [ p . name ] = data [ p . name ] . astype ( str ) . mask ( nulls , np . nan ) self . check_data ( data ) self . data = data set_optima ( self , optima ) Set the optima / Pareto front. Source code in opti/problem.py def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima to_config ( self ) Return json-serializable configuration dict. Source code in opti/problem.py def to_config ( self ) -> dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config to_json ( self , fname ) Save a problem from a JSON file. Source code in opti/problem.py def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , separators = ( \",\" , \":\" )) outfile . write ( b . encode ( \"utf-8\" )) read_json ( filepath ) Read a problem specification from a JSON file. Source code in opti/problem.py def read_json ( filepath : PathLike ) -> Problem : \"\"\"Read a problem specification from a JSON file.\"\"\" return Problem . from_json ( filepath )","title":"Problem"},{"location":"ref-problem/#problem","text":"","title":"Problem"},{"location":"ref-problem/#opti.problem.Problem","text":"Source code in opti/problem.py class Problem : def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ DataFrameLike ] = None , optima : Optional [ DataFrameLike ] = None , name : Optional [ str ] = None , ** kwargs , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f if isinstance ( data , dict ): data = pd . read_json ( json . dumps ( data ), orient = \"split\" ) if isinstance ( optima , dict ): optima = pd . read_json ( json . dumps ( optima ), orient = \"split\" ) self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models () @property def n_inputs ( self ) -> int : return len ( self . inputs ) @property def n_outputs ( self ) -> int : return len ( self . outputs ) @property def n_objectives ( self ) -> int : return len ( self . objectives ) @property def n_constraints ( self ) -> int : return 0 if self . constraints is None else len ( self . constraints ) def __repr__ ( self ): return self . __str__ () def __str__ ( self ): s = \"Problem( \\n \" s += f \"name= { self . name } , \\n \" s += f \"inputs= { self . inputs } , \\n \" s += f \"outputs= { self . outputs } , \\n \" s += f \"objectives= { self . objectives } , \\n \" if self . output_constraints is not None : s += f \"output_constraints= { self . output_constraints } , \\n \" if self . constraints is not None : s += f \"constraints= { self . constraints } , \\n \" if self . models is not None : s += f \"models= { self . models } , \\n \" if self . data is not None : s += f \"data= \\n { self . data . head () } \\n \" if self . optima is not None : s += f \"optima= \\n { self . optima . head () } \\n \" return s + \")\" @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" return Problem ( ** config ) def to_config ( self ) -> dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem ( ** config ) def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , separators = ( \",\" , \":\" )) outfile . write ( b . encode ( \"utf-8\" )) def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check for duplicate names duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if all objectives refer to an output for obj in self . objectives : if obj . name not in self . outputs . names : raise ValueError ( f \"Objective refers to unknown parameter: { obj . name } \" ) def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # data must contain all parameters if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } is missing. Data must contain all parameters.\" ) # data for continuous / discrete parameters must be numeric if isinstance ( p , ( Continuous , Discrete )): ok = is_numeric_dtype ( data [ p . name ]) or data [ p . name ] . isnull () . all () if not ok : raise ValueError ( f \"Parameter { p . name } contains non-numeric values. Data for continuous / discrete parameters must be numeric.\" ) # categorical levels in data must be specified elif isinstance ( p , Categorical ): ok = p . contains ( data [ p . name ]) | data [ p . name ] . isna () if not ok . all (): unknowns = data [ p . name ][ ~ ok ] . unique () . tolist () raise ValueError ( f \"Data for parameter { p . name } contains unknown values: { unknowns } . All categorical levels must be specified.\" ) # inputs must be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Input parameter { p . name } has missing data. Inputs must be complete.\" ) # outputs must have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"Output parameter { p . name } has no data. Outputs must have at least one observation.\" ) def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" ) def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : for p in self . inputs : # Categorical levels are required to be strings. Ensure that the corresponding data is as well. if isinstance ( p , Categorical ): nulls = data [ p . name ] . isna () data [ p . name ] = data [ p . name ] . astype ( str ) . mask ( nulls , np . nan ) self . check_data ( data ) self . data = data def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 ) def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints ) def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\" if self . f is None : raise NotImplementedError ( \"problem.f is not implemented for the problem.\" ) X = self . sample_inputs ( n_samples ) Y = self . f ( X ) self . data = pd . concat ([ X , Y ], axis = 1 )","title":"Problem"},{"location":"ref-problem/#opti.problem.Problem.__init__","text":"An optimization problem. Parameters: Name Type Description Default inputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Input parameters. required outputs Union[opti.parameter.Parameters, List[opti.parameter.Parameter], List[Dict]] Output parameters. required objectives Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Optimization objectives. Defaults to minimization. None constraints Union[opti.constraint.Constraints, List[opti.constraint.Constraint], List[Dict]] Constraints on the inputs. None output_constraints Union[opti.objective.Objectives, List[opti.objective.Objective], List[Dict]] Constraints on the outputs. None f Optional[Callable] Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame None data Union[pandas.core.frame.DataFrame, Dict] Experimental data. None optima Union[pandas.core.frame.DataFrame, Dict] Pareto optima. None name Optional[str] Name of the problem. None Source code in opti/problem.py def __init__ ( self , inputs : ParametersLike , outputs : ParametersLike , objectives : Optional [ ObjectivesLike ] = None , constraints : Optional [ ConstraintsLike ] = None , output_constraints : Optional [ ObjectivesLike ] = None , f : Optional [ Callable ] = None , models : Optional [ ModelsLike ] = None , data : Optional [ DataFrameLike ] = None , optima : Optional [ DataFrameLike ] = None , name : Optional [ str ] = None , ** kwargs , ): \"\"\"An optimization problem. Args: inputs: Input parameters. outputs: Output parameters. objectives: Optimization objectives. Defaults to minimization. constraints: Constraints on the inputs. output_constraints: Constraints on the outputs. f: Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -> pd.DataFrame data: Experimental data. optima: Pareto optima. name: Name of the problem. \"\"\" self . name = name if name is not None else \"Problem\" self . inputs = inputs if isinstance ( inputs , Parameters ) else Parameters ( inputs ) self . outputs = ( outputs if isinstance ( outputs , Parameters ) else Parameters ( outputs ) ) if objectives is None : self . objectives = Objectives ([ Minimize ( m ) for m in self . outputs . names ]) elif isinstance ( objectives , Objectives ): self . objectives = objectives else : self . objectives = Objectives ( objectives ) if isinstance ( constraints , Constraints ): pass elif not constraints : constraints = None else : constraints = Constraints ( constraints ) if len ( constraints ) == 0 : # no valid constraints constraints = None self . constraints = constraints if isinstance ( output_constraints , Objectives ) or output_constraints is None : self . output_constraints = output_constraints else : self . output_constraints = Objectives ( output_constraints ) if isinstance ( models , Models ) or models is None : self . models = models else : self . models = Models ( models ) if f is not None : self . f = f if isinstance ( data , dict ): data = pd . read_json ( json . dumps ( data ), orient = \"split\" ) if isinstance ( optima , dict ): optima = pd . read_json ( json . dumps ( optima ), orient = \"split\" ) self . set_data ( data ) self . set_optima ( optima ) self . check_problem () self . check_models ()","title":"__init__()"},{"location":"ref-problem/#opti.problem.Problem.add_data","text":"Add a number of data points. Source code in opti/problem.py def add_data ( self , data : pd . DataFrame ) -> None : \"\"\"Add a number of data points.\"\"\" self . check_data ( data ) self . data = pd . concat ([ self . data , data ], axis = 0 )","title":"add_data()"},{"location":"ref-problem/#opti.problem.Problem.check_data","text":"Check if data is consistent with input and output parameters. Source code in opti/problem.py def check_data ( self , data : pd . DataFrame ) -> None : \"\"\"Check if data is consistent with input and output parameters.\"\"\" for p in self . inputs + self . outputs : # data must contain all parameters if p . name not in data . columns : raise ValueError ( f \"Parameter { p . name } is missing. Data must contain all parameters.\" ) # data for continuous / discrete parameters must be numeric if isinstance ( p , ( Continuous , Discrete )): ok = is_numeric_dtype ( data [ p . name ]) or data [ p . name ] . isnull () . all () if not ok : raise ValueError ( f \"Parameter { p . name } contains non-numeric values. Data for continuous / discrete parameters must be numeric.\" ) # categorical levels in data must be specified elif isinstance ( p , Categorical ): ok = p . contains ( data [ p . name ]) | data [ p . name ] . isna () if not ok . all (): unknowns = data [ p . name ][ ~ ok ] . unique () . tolist () raise ValueError ( f \"Data for parameter { p . name } contains unknown values: { unknowns } . All categorical levels must be specified.\" ) # inputs must be complete for p in self . inputs : if data [ p . name ] . isnull () . any (): raise ValueError ( f \"Input parameter { p . name } has missing data. Inputs must be complete.\" ) # outputs must have at least one observation for p in self . outputs : if data [ p . name ] . isnull () . all (): raise ValueError ( f \"Output parameter { p . name } has no data. Outputs must have at least one observation.\" )","title":"check_data()"},{"location":"ref-problem/#opti.problem.Problem.check_models","text":"Check if the models are well defined Source code in opti/problem.py def check_models ( self ) -> None : \"\"\"Check if the models are well defined\"\"\" if self . models is None : return for model in self . models : # models need to refer to output parameters for n in model . names : if n not in self . outputs . names : raise ValueError ( f \"Model { model } refers to unknown outputs\" ) if isinstance ( model , LinearModel ): if len ( model . coefficients ) != self . n_inputs : raise ValueError ( f \"Model { model } has wrong number of coefficients.\" )","title":"check_models()"},{"location":"ref-problem/#opti.problem.Problem.check_problem","text":"Check if input and output parameters are consistent. Source code in opti/problem.py def check_problem ( self ) -> None : \"\"\"Check if input and output parameters are consistent.\"\"\" # check for duplicate names duplicates = set ( self . inputs . names ) . intersection ( self . outputs . names ) if duplicates : raise ValueError ( f \"Parameter name in both inputs and outputs: { duplicates } \" ) # check if all objectives refer to an output for obj in self . objectives : if obj . name not in self . outputs . names : raise ValueError ( f \"Objective refers to unknown parameter: { obj . name } \" )","title":"check_problem()"},{"location":"ref-problem/#opti.problem.Problem.create_initial_data","text":"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs. Source code in opti/problem.py def create_initial_data ( self , n_samples : int = 10 ) -> None : \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\" if self . f is None : raise NotImplementedError ( \"problem.f is not implemented for the problem.\" ) X = self . sample_inputs ( n_samples ) Y = self . f ( X ) self . data = pd . concat ([ X , Y ], axis = 1 )","title":"create_initial_data()"},{"location":"ref-problem/#opti.problem.Problem.from_config","text":"Create a Problem instance from a configuration dict. Source code in opti/problem.py @staticmethod def from_config ( config : dict ) -> \"Problem\" : \"\"\"Create a Problem instance from a configuration dict.\"\"\" return Problem ( ** config )","title":"from_config()"},{"location":"ref-problem/#opti.problem.Problem.from_json","text":"Read a problem from a JSON file. Source code in opti/problem.py @staticmethod def from_json ( fname : PathLike ) -> \"Problem\" : \"\"\"Read a problem from a JSON file.\"\"\" with open ( fname , \"rb\" ) as infile : config = json . loads ( infile . read ()) return Problem ( ** config )","title":"from_json()"},{"location":"ref-problem/#opti.problem.Problem.get_X","text":"Return the input values in data or self.data . Source code in opti/problem.py def get_X ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the input values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . inputs . names ] . values return self . get_data ()[ self . inputs . names ] . values","title":"get_X()"},{"location":"ref-problem/#opti.problem.Problem.get_XY","text":"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Parameters: Name Type Description Default outputs optional Subset of the outputs to consider. None data optional Dataframe to consider instead of problem.data None Source code in opti/problem.py def get_XY ( self , outputs : Optional [ List [ str ]] = None , data : Optional [ pd . DataFrame ] = None , continuous : str = \"none\" , discrete : str = \"none\" , categorical : str = \"none\" , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the input and output values as numeric numpy arrays. Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded. Args: outputs (optional): Subset of the outputs to consider. data (optional): Dataframe to consider instead of problem.data \"\"\" if outputs is None : outputs = self . outputs . names if data is None : data = self . get_data () notna = data [ outputs ] . notna () . all ( axis = 1 ) X = self . inputs . transform ( data , continuous = continuous , discrete = discrete , categorical = categorical )[ notna ] . values Y = data [ outputs ][ notna ] . values return X , Y","title":"get_XY()"},{"location":"ref-problem/#opti.problem.Problem.get_X_bounds","text":"Return the lower and upper data bounds. Source code in opti/problem.py def get_X_bounds ( self ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Return the lower and upper data bounds.\"\"\" X = self . get_X () xlo = X . min ( axis = 0 ) xhi = X . max ( axis = 0 ) b = xlo == xhi xhi [ b ] = xlo [ b ] + 1 # prevent division by zero when dividing by (xhi - xlo) return xlo , xhi","title":"get_X_bounds()"},{"location":"ref-problem/#opti.problem.Problem.get_Y","text":"Return the output values in data or self.data . Source code in opti/problem.py def get_Y ( self , data : Optional [ pd . DataFrame ] = None ) -> np . ndarray : \"\"\"Return the output values in `data` or `self.data`.\"\"\" if data is not None : return data [ self . outputs . names ] . values return self . get_data ()[ self . outputs . names ] . values","title":"get_Y()"},{"location":"ref-problem/#opti.problem.Problem.get_data","text":"Return self.data if it exists or an empty dataframe. Source code in opti/problem.py def get_data ( self ) -> pd . DataFrame : \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\" if self . data is None : return pd . DataFrame ( columns = self . inputs . names + self . outputs . names ) return self . data","title":"get_data()"},{"location":"ref-problem/#opti.problem.Problem.sample_inputs","text":"Uniformly sample points from the input space subject to the constraints. Source code in opti/problem.py def sample_inputs ( self , n_samples = 10 ) -> pd . DataFrame : \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\" if self . constraints is None : return sobol_sampling ( n_samples , self . inputs ) return constrained_sampling ( n_samples , self . inputs , self . constraints )","title":"sample_inputs()"},{"location":"ref-problem/#opti.problem.Problem.set_data","text":"Set the data. Source code in opti/problem.py def set_data ( self , data : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the data.\"\"\" if data is not None : for p in self . inputs : # Categorical levels are required to be strings. Ensure that the corresponding data is as well. if isinstance ( p , Categorical ): nulls = data [ p . name ] . isna () data [ p . name ] = data [ p . name ] . astype ( str ) . mask ( nulls , np . nan ) self . check_data ( data ) self . data = data","title":"set_data()"},{"location":"ref-problem/#opti.problem.Problem.set_optima","text":"Set the optima / Pareto front. Source code in opti/problem.py def set_optima ( self , optima : Optional [ pd . DataFrame ]) -> None : \"\"\"Set the optima / Pareto front.\"\"\" if optima is not None : self . check_data ( optima ) self . optima = optima","title":"set_optima()"},{"location":"ref-problem/#opti.problem.Problem.to_config","text":"Return json-serializable configuration dict. Source code in opti/problem.py def to_config ( self ) -> dict : \"\"\"Return json-serializable configuration dict.\"\"\" config = { \"name\" : self . name , \"inputs\" : self . inputs . to_config (), \"outputs\" : self . outputs . to_config (), \"objectives\" : self . objectives . to_config (), } if self . output_constraints is not None : config [ \"output_constraints\" ] = self . output_constraints . to_config () if self . constraints is not None : config [ \"constraints\" ] = self . constraints . to_config () if self . models is not None : config [ \"models\" ] = self . models . to_config () if self . data is not None : config [ \"data\" ] = self . data . replace ({ np . nan : None }) . to_dict ( \"split\" ) if self . optima is not None : config [ \"optima\" ] = self . optima . replace ({ np . nan : None }) . to_dict ( \"split\" ) return config","title":"to_config()"},{"location":"ref-problem/#opti.problem.Problem.to_json","text":"Save a problem from a JSON file. Source code in opti/problem.py def to_json ( self , fname : PathLike ) -> None : \"\"\"Save a problem from a JSON file.\"\"\" with open ( fname , \"wb\" ) as outfile : b = json . dumps ( self . to_config (), ensure_ascii = False , separators = ( \",\" , \":\" )) outfile . write ( b . encode ( \"utf-8\" ))","title":"to_json()"},{"location":"ref-problem/#opti.problem.read_json","text":"Read a problem specification from a JSON file. Source code in opti/problem.py def read_json ( filepath : PathLike ) -> Problem : \"\"\"Read a problem specification from a JSON file.\"\"\" return Problem . from_json ( filepath )","title":"read_json()"},{"location":"ref-problems/","text":"Test Problems datasets Chemical datasets. These problems contain observed data but don't come with a ground truth. Alkox ( Problem ) Alkoxylation dataset This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx). The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective. Reference F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Alkox ( Problem ): \"\"\"Alkoxylation dataset This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx). The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective. Reference: F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. [DOI](https://doi.org/10.1088/2632-2153/abedc8). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Alkox\" , inputs = [ Continuous ( \"residence_time\" , domain = [ 0.05 , 1 ]), Continuous ( \"ratio\" , domain = [ 0.5 , 10 ]), Continuous ( \"concentration\" , domain = [ 2 , 8 ]), Continuous ( \"temperature\" , domain = [ 6 , 8 ]), ], outputs = [ Continuous ( \"conversion\" )], objectives = [ Maximize ( \"conversion\" )], data = get_data ( \"alkox.csv\" ), ) BaumgartnerAniline ( Problem ) Aniline C-N cross-coupling dataset. Reference Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI . Data obtained from Summit . Source code in opti/problems/datasets.py class BaumgartnerAniline ( Problem ): \"\"\"Aniline C-N cross-coupling dataset. Reference: Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases [DOI](https://doi.org/10.1021/acs.oprd.9b00236). Data obtained from [Summit](https://github.com/sustainable-processes/summit). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Aniline cross-coupling, Baumgartner 2019\" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"tBuXPhos\" , \"tBuBrettPhos\" , \"AlPhos\" ]), Categorical ( \"base\" , domain = [ \"TEA\" , \"TMG\" , \"BTMG\" , \"DBU\" ]), Continuous ( \"base_equivalents\" , domain = [ 1.0 , 2.5 ]), Continuous ( \"temperature\" , domain = [ 30 , 100 ]), Continuous ( \"residence_time\" , domain = [ 60 , 1800 ]), ], outputs = [ Continuous ( \"yield\" , domain = [ 0 , 1 ])], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"baumgartner_aniline.csv\" ), ) BaumgartnerBenzamide ( Problem ) Benzamide C-N cross-coupling dataset. Reference Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI . Data obtained from Summit . Source code in opti/problems/datasets.py class BaumgartnerBenzamide ( Problem ): \"\"\"Benzamide C-N cross-coupling dataset. Reference: Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases [DOI](https://doi.org/10.1021/acs.oprd.9b00236). Data obtained from [Summit](https://github.com/sustainable-processes/summit). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Benzamide cross-coupling, Baumgartner 2019\" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"tBuXPhos\" , \"tBuBrettPhos\" ]), Categorical ( \"base\" , domain = [ \"TMG\" , \"BTMG\" , \"DBU\" , \"MTBD\" ]), Continuous ( \"base_equivalents\" , domain = [ 1.0 , 2.1 ]), Continuous ( \"temperature\" , domain = [ 30 , 100 ]), Continuous ( \"residence_time\" , domain = [ 60 , 1850 ]), ], outputs = [ Continuous ( \"yield\" , domain = [ 0 , 1 ])], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"baumgartner_benzamide.csv\" ), ) Benzylation ( Problem ) Benzylation dataset. This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction. Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective. Reference A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Benzylation ( Problem ): \"\"\"Benzylation dataset. This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction. Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective. Reference: A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. [DOI](https://doi.org/10.1016/j.cej.2018.07.031). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Benzylation\" , inputs = [ Continuous ( \"flow_rate\" , domain = [ 0.2 , 0.4 ]), Continuous ( \"ratio\" , domain = [ 1.0 , 5.0 ]), Continuous ( \"solvent\" , domain = [ 0.5 , 1.0 ]), Continuous ( \"temperature\" , domain = [ 110.0 , 150.0 ]), ], outputs = [ Continuous ( \"impurity\" )], objectives = [ Minimize ( \"impurity\" )], data = get_data ( \"benzylation.csv\" ), ) Cake ( Problem ) Cake recipe optimization with mixed objectives. Source code in opti/problems/datasets.py class Cake ( Problem ): \"\"\"Cake recipe optimization with mixed objectives.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Cake\" , inputs = [ Continuous ( \"wheat_flour\" , domain = [ 0 , 1 ]), Continuous ( \"spelt_flour\" , domain = [ 0 , 1 ]), Continuous ( \"sugar\" , domain = [ 0 , 1 ]), Continuous ( \"chocolate\" , domain = [ 0 , 1 ]), Continuous ( \"nuts\" , domain = [ 0 , 1 ]), Continuous ( \"carrot\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( \"calories\" , domain = [ 300 , 600 ]), Continuous ( \"taste\" , domain = [ 0 , 5 ]), Continuous ( \"browning\" , domain = [ 0 , 2 ]), ], objectives = [ Minimize ( \"calories\" ), Maximize ( \"taste\" ), CloseToTarget ( \"browning\" , target = 1.4 ), ], constraints = [ LinearEquality ( [ \"wheat_flour\" , \"spelt_flour\" , \"sugar\" , \"chocolate\" , \"nuts\" , \"carrot\" , ], rhs = 1 , ) ], data = get_data ( \"cake.csv\" ), ) Fullerenes ( Problem ) Buckminsterfullerene dataset. This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes. Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product. Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective. Reference B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Fullerenes ( Problem ): \"\"\"Buckminsterfullerene dataset. This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes. Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product. Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective. Reference: B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798. [DOI](https://doi.org/10.1039/C7RE00123A). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Fullerenes\" , inputs = [ Continuous ( \"reaction_time\" , domain = [ 3.0 , 31.0 ]), Continuous ( \"sultine\" , domain = [ 1.5 , 6.0 ]), Continuous ( \"temperature\" , domain = [ 100.0 , 150.0 ]), ], outputs = [ Continuous ( \"product\" )], objectives = [ Maximize ( \"product\" )], data = get_data ( \"fullerenes.csv\" ), ) HPLC ( Problem ) High-performance liquid chromatography dataset. This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective. Reference L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class HPLC ( Problem ): \"\"\"High-performance liquid chromatography dataset. This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective. Reference: L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) [DOI](https://doi.org/10.26434/chemrxiv.5953606.v1). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"HPLC\" , inputs = [ Continuous ( \"sample_loop\" , domain = [ 0.0 , 0.08 ]), Continuous ( \"additional_volume\" , domain = [ 0.0 , 0.06 ]), Continuous ( \"tubing_volume\" , domain = [ 0.1 , 0.9 ]), Continuous ( \"sample_flow\" , domain = [ 0.5 , 2.5 ]), Continuous ( \"push_speed\" , domain = [ 80.0 , 150 ]), Continuous ( \"wait_time\" , domain = [ 0.5 , 10.0 ]), ], outputs = [ Continuous ( \"peak_area\" )], objectives = [ Maximize ( \"peak_area\" )], data = get_data ( \"hplc.csv\" ), ) Photodegradation ( Problem ) Photodegration dataset. This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective. Reference S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Photodegradation ( Problem ): \"\"\"Photodegration dataset. This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective. Reference: S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. [DOI](https://doi.org/10.1002/adma.201907801). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Photodegradation\" , inputs = [ Continuous ( \"PCE10\" , domain = [ 0 , 1 ]), Continuous ( \"WF3\" , domain = [ 0 , 1 ]), Continuous ( \"P3HT\" , domain = [ 0 , 1 ]), Continuous ( \"PCBM\" , domain = [ 0 , 1 ]), Continuous ( \"oIDTBR\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( \"degradation\" )], objectives = [ Minimize ( \"degradation\" )], constraints = [ LinearEquality ( [ \"PCE10\" , \"WF3\" , \"P3HT\" , \"PCBM\" , \"oIDTBR\" ], rhs = 1 , lhs = 1 ), NChooseK ([ \"PCE10\" , \"WF3\" ], max_active = 1 ), ], data = get_data ( \"photodegradation.csv\" ), ) ReizmanSuzuki ( Problem ) Suzuki-Miyaura cross-coupling optimization. Each case was has a different set of substrates but the same possible catalysts. Reference Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry & engineering, 1(6), 658-666 DOI . Data obtained from Summit . Source code in opti/problems/datasets.py class ReizmanSuzuki ( Problem ): \"\"\"Suzuki-Miyaura cross-coupling optimization. Each case was has a different set of substrates but the same possible catalysts. Reference: Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry & engineering, 1(6), 658-666 [DOI](https://doi.org/10.1039/C6RE00153J). Data obtained from [Summit](https://github.com/sustainable-processes/summit). \"\"\" def __init__ ( self , case = 1 ): assert case in [ 1 , 2 , 3 , 4 ] super () . __init__ ( name = f \"Reizman 2016 - Suzuki Case { case } \" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"P1-L1\" , \"P2-L1\" , \"P1-L2\" , \"P1-L3\" , \"P1-L4\" , \"P1-L5\" , \"P1-L6\" , \"P1-L7\" , ], ), Continuous ( \"t_res\" , domain = [ 60 , 600 ]), Continuous ( \"temperature\" , domain = [ 30 , 110 ]), Continuous ( \"catalyst_loading\" , domain = [ 0.496 , 2.515 ]), ], outputs = [ Continuous ( \"ton\" , domain = [ 0 , 100 ]), Continuous ( \"yield\" , domain = [ 0 , 100 ]), ], objectives = [ Maximize ( \"ton\" ), Maximize ( \"yield\" )], data = get_data ( f \"reizman_suzuki { case } .csv\" ), ) SnAr ( Problem ) SnAr reaction optimization. This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism. Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective. Reference A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class SnAr ( Problem ): \"\"\"SnAr reaction optimization. This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism. Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective. Reference: A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. [DOI](https://doi.org/10.1016/j.cej.2018.07.031). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"SnAr\" , inputs = [ Continuous ( \"residence_time\" , domain = [ 0.5 , 2.0 ]), Continuous ( \"ratio\" , domain = [ 1.0 , 5.0 ]), Continuous ( \"concentration\" , domain = [ 0.1 , 0.5 ]), Continuous ( \"temperature\" , domain = [ 60.0 , 140.0 ]), ], outputs = [ Continuous ( \"impurity\" )], objectives = [ Minimize ( \"impurity\" )], data = get_data ( \"snar.csv\" ), ) Suzuki ( Problem ) Suzuki reaction dataset. This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate. Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective. Reference F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Suzuki ( Problem ): \"\"\"Suzuki reaction dataset. This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate. Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective. Reference: F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. [DOI](https://doi.org/10.1088/2632-2153/abedc8). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Suzuki\" , inputs = [ Continuous ( \"temperature\" , domain = [ 75.0 , 90.0 ]), Continuous ( \"pd_mol\" , domain = [ 0.5 , 5.0 ]), Continuous ( \"arbpin\" , domain = [ 1.0 , 1.8 ]), Continuous ( \"k3po4\" , domain = [ 1.5 , 3.0 ]), ], outputs = [ Continuous ( \"yield\" )], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"suzuki.csv\" ), ) detergent Detergent ( Problem ) Detergent formulation problem. There are 5 outputs representing the washing performance on different stain types. Each output is modeled as a second degree polynomial. The formulation consists of 5 components. The sixth input is a filler (water) and is factored out and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. Source code in opti/problems/detergent.py class Detergent ( Problem ): \"\"\"Detergent formulation problem. There are 5 outputs representing the washing performance on different stain types. Each output is modeled as a second degree polynomial. The formulation consists of 5 components. The sixth input is a filler (water) and is factored out and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. \"\"\" def __init__ ( self ): # coefficients for the 2-order polynomial; generated with # base = 3 * np.ones((1, 5)) # scale = PolynomialFeatures(degree=2).fit_transform(base).T # coef = np.random.RandomState(42).normal(scale=scale, size=(len(scale), 5)) # coef = np.clip(coef, 0, None) self . coef = np . array ( [ [ 0.4967 , 0.0 , 0.6477 , 1.523 , 0.0 ], [ 0.0 , 4.7376 , 2.3023 , 0.0 , 1.6277 ], [ 0.0 , 0.0 , 0.7259 , 0.0 , 0.0 ], [ 0.0 , 0.0 , 0.9427 , 0.0 , 0.0 ], [ 4.3969 , 0.0 , 0.2026 , 0.0 , 0.0 ], [ 0.3328 , 0.0 , 1.1271 , 0.0 , 0.0 ], [ 0.0 , 16.6705 , 0.0 , 0.0 , 7.4029 ], [ 0.0 , 1.8798 , 0.0 , 0.0 , 1.7718 ], [ 6.6462 , 1.5423 , 0.0 , 0.0 , 0.0 ], [ 0.0 , 0.0 , 9.5141 , 3.0926 , 0.0 ], [ 2.9168 , 0.0 , 0.0 , 5.5051 , 9.279 ], [ 8.3815 , 0.0 , 0.0 , 2.9814 , 8.7799 ], [ 0.0 , 0.0 , 0.0 , 0.0 , 7.3127 ], [ 12.2062 , 0.0 , 9.0318 , 3.2547 , 0.0 ], [ 3.2526 , 13.8423 , 0.0 , 14.0818 , 0.0 ], [ 7.3971 , 0.7834 , 0.0 , 0.8258 , 0.0 ], [ 0.0 , 3.214 , 13.301 , 0.0 , 0.0 ], [ 0.0 , 8.2386 , 2.9588 , 0.0 , 4.6194 ], [ 0.8737 , 8.7178 , 0.0 , 0.0 , 0.0 ], [ 0.0 , 2.6651 , 2.3495 , 0.046 , 0.0 ], [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ], ] ) super () . __init__ ( name = \"Detergent optimization\" , inputs = [ Continuous ( \"x1\" , domain = [ 0.0 , 0.2 ]), Continuous ( \"x2\" , domain = [ 0.0 , 0.3 ]), Continuous ( \"x3\" , domain = [ 0.02 , 0.2 ]), Continuous ( \"x4\" , domain = [ 0.0 , 0.06 ]), Continuous ( \"x5\" , domain = [ 0.0 , 0.04 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" , domain = [ 0 , 3 ]) for i in range ( 5 )], objectives = [ Maximize ( f \"y { i + 1 } \" ) for i in range ( 5 )], constraints = [ LinearInequality ([ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"x5\" ], lhs =- 1 , rhs =- 0.2 ), LinearInequality ([ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"x5\" ], lhs = 1 , rhs = 0.4 ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = np . atleast_2d ( X [ self . inputs . names ]) xp = np . stack ([ _poly2 ( xi ) for xi in x ], axis = 0 ) return pd . DataFrame ( xp @ self . coef , columns = self . outputs . names , index = X . index ) Detergent_NChooseKConstraint ( Problem ) Variant of the Detergent problem where only 3 of the 5 formulation components are allowed to be active (n-choose-k constraint). Source code in opti/problems/detergent.py class Detergent_NChooseKConstraint ( Problem ): \"\"\"Variant of the Detergent problem where only 3 of the 5 formulation components are allowed to be active (n-choose-k constraint).\"\"\" def __init__ ( self ): base = Detergent () super () . __init__ ( name = \"Detergent optimization with n-choose-k constraint\" , inputs = base . inputs , outputs = base . outputs , objectives = base . objectives , constraints = list ( base . constraints ) + [ NChooseK ( names = base . inputs . names , max_active = 3 )], f = base . f , ) Detergent_OutputConstraint ( Problem ) Variant of the Detergent problem with an additional output/black-box constraint. In addition to the 5 washing performances there is a sixth output reflecting the stability of the formulation. If discrete=True the stability can only be measured qualitatively (0: not stable, 1: stable). If discrete=True the stability can be measured quantively with smaller values indicating less stable formulations. Source code in opti/problems/detergent.py class Detergent_OutputConstraint ( Problem ): \"\"\"Variant of the Detergent problem with an additional output/black-box constraint. In addition to the 5 washing performances there is a sixth output reflecting the stability of the formulation. If `discrete=True` the stability can only be measured qualitatively (0: not stable, 1: stable). If `discrete=True` the stability can be measured quantively with smaller values indicating less stable formulations. \"\"\" def __init__ ( self , discrete = False ): base = Detergent () def f ( X ): Y = base . f ( X ) if discrete : Y [ \"stable\" ] = ( X . sum ( axis = 1 ) < 0.3 ) . astype ( int ) else : Y [ \"stable\" ] = ( 0.4 - X . sum ( axis = 1 )) / 0.2 return Y outputs = list ( base . outputs ) if discrete : outputs += [ Discrete ( \"stable\" , domain = [ 0 , 1 ])] else : outputs += [ Continuous ( \"stable\" , domain = [ 0 , 1 ])] super () . __init__ ( name = \"Detergent optimization with stability constraint\" , inputs = base . inputs , outputs = outputs , objectives = base . objectives , output_constraints = [ Maximize ( \"stable\" , target = 0.5 )], constraints = base . constraints , f = f , ) Detergent_TwoOutputConstraints ( Problem ) Variant of the Detergent problem with two outputs constraint. In addition to the 5 washing performances there are two more outputs measuring the formulation stability. The first, stability 1, measures the immediate stability. If not stable, the other properties cannot be measured, except for stability 2. The second, stability 2, measures the long-term stability. Source code in opti/problems/detergent.py class Detergent_TwoOutputConstraints ( Problem ): \"\"\"Variant of the Detergent problem with two outputs constraint. In addition to the 5 washing performances there are two more outputs measuring the formulation stability. The first, stability 1, measures the immediate stability. If not stable, the other properties cannot be measured, except for stability 2. The second, stability 2, measures the long-term stability. \"\"\" def __init__ ( self ): base = Detergent () def f ( X : pd . DataFrame ) -> pd . DataFrame : Y = base . f ( X ) x = self . get_X ( X ) stable1 = ( x . sum ( axis = 1 ) < 0.3 ) . astype ( int ) stable2 = ( x [:, : - 1 ] . sum ( axis = 1 ) < 0.25 ) . astype ( int ) Y [ stable1 == 0 ] = np . nan Y [ \"stability 1\" ] = stable1 Y [ \"stability 2\" ] = stable2 return Y outputs = list ( base . outputs ) + [ Discrete ( \"stability 1\" , domain = [ 0 , 1 ]), Discrete ( \"stability 2\" , domain = [ 0 , 1 ]), ] super () . __init__ ( name = \"Detergent optimization with two output constraint\" , inputs = base . inputs , outputs = outputs , objectives = base . objectives , output_constraints = [ Maximize ( \"stability 1\" , target = 0.5 ), Maximize ( \"stability 2\" , target = 0.5 ), ], constraints = base . constraints , f = f , ) mixed Mixed variables single and multi-objective test problems. DiscreteFuelInjector ( Problem ) Fuel injector test problem, modified to contain an integer variable. 4 objectives, mixed variables, unconstrained See Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 Source code in opti/problems/mixed.py class DiscreteFuelInjector ( Problem ): \"\"\"Fuel injector test problem, modified to contain an integer variable. * 4 objectives, * mixed variables, unconstrained See: Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Discrete fuel injector test problem\" , inputs = [ Discrete ( \"x1\" , [ 0 , 1 , 2 , 3 ]), Continuous ( \"x2\" , [ - 2 , 2 ]), Continuous ( \"x3\" , [ - 2 , 2 ]), Continuous ( \"x4\" , [ - 2 , 2 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 4 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 = X [ \"x1\" ] . to_numpy () . astype ( float ) x2 = X [ \"x2\" ] . to_numpy () . astype ( float ) x3 = X [ \"x3\" ] . to_numpy () . astype ( float ) x4 = X [ \"x4\" ] . to_numpy () . astype ( float ) x1 *= 0.2 y1 = ( 0.692 + 0.4771 * x1 - 0.687 * x4 - 0.08 * x3 - 0.065 * x2 - 0.167 * x1 ** 2 - 0.0129 * x1 * x4 + 0.0796 * x4 ** 2 - 0.0634 * x1 * x3 - 0.0257 * x3 * x4 + 0.0877 * x3 ** 2 - 0.0521 * x1 * x2 + 0.00156 * x2 * x4 + 0.00198 * x2 * x3 + 0.0184 * x2 ** 2 ) y2 = ( 0.37 - 0.205 * x1 + 0.0307 * x4 + 0.108 * x3 + 1.019 * x2 - 0.135 * x1 ** 2 + 0.0141 * x1 * x4 + 0.0998 * x4 ** 2 + 0.208 * x1 * x3 - 0.0301 * x3 * x4 - 0.226 * x3 ** 2 + 0.353 * x1 * x2 - 0.0497 * x2 * x3 - 0.423 * x2 ** 2 + 0.202 * x1 ** 2 * x4 - 0.281 * x1 ** 2 * x3 - 0.342 * x1 * x4 ** 2 - 0.245 * x3 * x4 ** 2 + 0.281 * x3 ** 2 * x4 - 0.184 * x1 * x2 ** 2 + 0.281 * x1 * x3 * x4 ) y3 = ( 0.153 - 0.322 * x1 + 0.396 * x4 + 0.424 * x3 + 0.0226 * x2 + 0.175 * x1 ** 2 + 0.0185 * x1 * x4 - 0.0701 * x4 ** 2 - 0.251 * x1 * x3 + 0.179 * x3 * x4 + 0.015 * x3 ** 2 + 0.0134 * x1 * x2 + 0.0296 * x2 * x4 + 0.0752 * x2 * x3 + 0.0192 * x2 ** 2 ) y4 = ( 0.758 + 0.358 * x1 - 0.807 * x4 + 0.0925 * x3 - 0.0468 * x2 - 0.172 * x1 ** 2 + 0.0106 * x1 * x4 + 0.0697 * x4 ** 2 - 0.146 * x1 * x3 - 0.0416 * x3 * x4 + 0.102 * x3 ** 2 - 0.0694 * x1 * x2 - 0.00503 * x2 * x4 + 0.0151 * x2 * x3 + 0.0173 * x2 ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 , \"y3\" : y3 , \"y4\" : y4 }) DiscreteVLMOP2 ( Problem ) VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. 2 minimization objectives 1 categorical and n continuous inputs, unconstrained See Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 Source code in opti/problems/mixed.py class DiscreteVLMOP2 ( Problem ): \"\"\"VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. * 2 minimization objectives * 1 categorical and n continuous inputs, unconstrained See: Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 \"\"\" def __init__ ( self , n_inputs : int = 3 ): assert n_inputs >= 2 super () . __init__ ( name = \"Discrete VLMOP2 test problem\" , inputs = [ Categorical ( \"x1\" , [ \"a\" , \"b\" ])] + [ Continuous ( f \"x { i + 1 } \" , [ - 2 , 2 ]) for i in range ( 1 , n_inputs )], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : d = X [ self . inputs . names [ 0 ]] . values x = X [ self . inputs . names [ 1 :]] . values n = self . n_inputs y1 = np . exp ( - np . sum (( x - n **- 0.5 ) ** 2 , axis = 1 )) y2 = np . exp ( - np . sum (( x + n **- 0.5 ) ** 2 , axis = 1 )) y1 = np . where ( d == \"a\" , 1 - y1 , 1.25 - y1 ) y2 = np . where ( d == \"a\" , 1 - y2 , 0.75 - y2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) multi Daechert1 ( Problem ) Problem with a non-convex Pareto front. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. Source code in opti/problems/multi.py class Daechert1 ( Problem ): \"\"\"Problem with a non-convex Pareto front. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-1\" , inputs = [ Continuous ( \"x1\" , domain = [ 0 , np . pi ]), Continuous ( \"x2\" , domain = [ 0 , 10 ]), Continuous ( \"x3\" , domain = [ 1.2 , 10 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], constraints = [ NonlinearInequality ( \"- cos(x1) - exp(-x2) + x3\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y1\" : - X [ \"x1\" ], \"y2\" : - X [ \"x2\" ], \"y3\" : - X [ \"x3\" ] ** 2 }) Daechert2 ( Problem ) Unconstrained problem with a Pareto front resembling a comet. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. Source code in opti/problems/multi.py class Daechert2 ( Problem ): \"\"\"Unconstrained problem with a Pareto front resembling a comet. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-2\" , inputs = [ Continuous ( \"x1\" , domain = [ 1 , 3.5 ]), Continuous ( \"x2\" , domain = [ - 2 , 2 ]), Continuous ( \"x3\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y1\" : X . eval ( \"(1 + x3) * (x1**3 * x2**2 - 10 * x1 - 4 * x2)\" ), \"y2\" : X . eval ( \"(1 + x3) * (x1**3 * x2**2 - 10 * x1 + 4 * x2)\" ), \"y3\" : X . eval ( \"3 * (1 + x3) * x1**2\" ), } ) Daechert3 ( Problem ) Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. Source code in opti/problems/multi.py class Daechert3 ( Problem ): \"\"\"Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-3\" , inputs = [ Continuous ( f \"x { i + 1 } \" , domain = [ 0 , 1 ]) for i in range ( 2 )], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] return pd . DataFrame ( { \"y1\" : X [ \"x1\" ], \"y2\" : X [ \"x2\" ], \"y3\" : 6 - np . sum ( x * ( 1 + np . sin ( 3 * np . pi * x )), axis = 1 ), } ) Hyperellipsoid ( Problem ) Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Parameters: Name Type Description Default n int Dimension of the hyperellipsoid. Defaults to 5. 5 a list-like Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. None Source code in opti/problems/multi.py class Hyperellipsoid ( Problem ): \"\"\"Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Args: n (int, optional): Dimension of the hyperellipsoid. Defaults to 5. a (list-like, optional): Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. \"\"\" def __init__ ( self , n : int = 5 , a : Optional [ Union [ list , np . ndarray ]] = None ): if a is None : a = np . ones ( n ) constr = \" + \" . join ([ f \"x { i + 1 } **2\" for i in range ( n )]) + \" - 1\" else : a = np . array ( a ) . squeeze () if len ( a ) != n : raise ValueError ( \"Dimension of half axes doesn't match input dimension\" ) constr = \" + \" . join ([ f \"(x { i + 1 } / { a [ i ] } )**2\" for i in range ( n )]) + \" - 1\" self . a = a super () . __init__ ( name = \"Hyperellipsoid\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - a [ i ], a [ i ]]) for i in range ( n )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - a [ i ], a [ i ]]) for i in range ( n )], constraints = [ NonlinearInequality ( constr )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : y = X [ self . inputs . names ] y . columns = self . outputs . names return y def get_optima ( self , n = 10 ) -> pd . DataFrame : X = opti . sampling . sphere . sample ( self . n_inputs , n , positive = True ) X = np . concatenate ([ - np . eye ( self . n_inputs ), - X ], axis = 0 )[: n ] Y = self . a * X return pd . DataFrame ( data = np . column_stack ([ X , Y ]), columns = self . inputs . names + self . outputs . names , ) OmniTest ( Problem ) Bi-objective benchmark problem with D inputs and a multi-modal Pareto set. It has 3^D Pareto subsets in the decision space corresponding to the same Pareto front. Reference Deb & Tiwari \"Omni-optimizer: A generic evolutionary algorithm for single and multi-objective optimization\" Source code in opti/problems/multi.py class OmniTest ( Problem ): \"\"\"Bi-objective benchmark problem with D inputs and a multi-modal Pareto set. It has 3^D Pareto subsets in the decision space corresponding to the same Pareto front. Reference: Deb & Tiwari \"Omni-optimizer: A generic evolutionary algorithm for single and multi-objective optimization\" \"\"\" def __init__ ( self , n_inputs : int = 2 ): super () . __init__ ( name = \"Omni\" , inputs = [ Continuous ( f \"x { i + 1 } \" , domain = [ 0 , 6 ]) for i in range ( 2 )], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : X = X [ self . inputs . names ] return pd . DataFrame ( { \"y1\" : np . sum ( np . sin ( np . pi * X ), axis = 1 ), \"y2\" : np . sum ( np . cos ( np . pi * X ), axis = 1 ), } ) def get_optima ( self ) -> pd . DataFrame : n = 11 # points per set (3^D sets) s = [ np . linspace ( 1 , 1.5 , n ) + 2 * i for i in range ( 3 )] C = list ( product ( * [ s , ] * self . n_inputs ) ) C = np . moveaxis ( C , 1 , 2 ) . reshape ( - 1 , 2 ) X = pd . DataFrame ( C , columns = self . inputs . names ) XY = pd . concat ([ X , self . f ( X )], axis = 1 ) XY [ \"_patch\" ] = np . repeat ( np . arange ( 3 ** self . n_inputs ), n ) return XY Poloni ( Problem ) Poloni benchmark problem. Source code in opti/problems/multi.py class Poloni ( Problem ): \"\"\"Poloni benchmark problem.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Poloni function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - np . pi , np . pi ]) for i in range ( 2 )], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 , x2 = self . get_X ( X ) . T A1 = 0.5 * np . sin ( 1 ) - 2 * np . cos ( 1 ) + np . sin ( 2 ) - 1.5 * np . cos ( 2 ) A2 = 1.5 * np . sin ( 1 ) - np . cos ( 1 ) + 2 * np . sin ( 2 ) - 0.5 * np . cos ( 2 ) B1 = 0.5 * np . sin ( x1 ) - 2 * np . cos ( x1 ) + np . sin ( x2 ) - 1.5 * np . cos ( x2 ) B2 = 1.5 * np . sin ( x1 ) - np . cos ( x1 ) + 2 * np . sin ( x2 ) - 0.5 * np . cos ( x2 ) return pd . DataFrame ( { \"y1\" : 1 + ( A1 - B1 ) ** 2 + ( A2 - B2 ) ** 2 , \"y2\" : ( x1 + 3 ) ** 2 + ( x2 + 1 ) ** 2 , }, index = X . index , ) Qapi1 ( Problem ) Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. Source code in opti/problems/multi.py class Qapi1 ( Problem ): \"\"\"Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Constrained bi-objective problem\" , inputs = [ Continuous ( \"x1\" , [ 0 , 10 ]), Continuous ( \"x2\" , [ - 10 , 10 ])], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], constraints = [ NonlinearInequality ( \"x2 - x1**2\" ), NonlinearInequality ( \"2 - x1 - x2\" ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y1\" : X . eval ( \"(x1 - 2)**2 + (x2 - 1)**2\" ), \"y2\" : X . eval ( \"x1**2 + (x2 - 3)**2\" ), } ) WeldedBeam ( Problem ) Design optimization of a welded beam. This is a bi-objective problem with 4 inputs and 3 (non-)linear inequality constraints. The two objectives are the fabrication cost of the beam and the deflection of the end of the beam under the applied load P. The load P is fixed at 6000 lbs, and the distance L is fixed at 14 inch. Note that for simplicity the constraint shear stress < 13600 psi is not included. See https://www.mathworks.com/help/gads/multiobjective-optimization-welded-beam.html Source code in opti/problems/multi.py class WeldedBeam ( Problem ): \"\"\"Design optimization of a welded beam. This is a bi-objective problem with 4 inputs and 3 (non-)linear inequality constraints. The two objectives are the fabrication cost of the beam and the deflection of the end of the beam under the applied load P. The load P is fixed at 6000 lbs, and the distance L is fixed at 14 inch. Note that for simplicity the constraint shear stress < 13600 psi is not included. See https://www.mathworks.com/help/gads/multiobjective-optimization-welded-beam.html \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Welded beam problem\" , inputs = [ Continuous ( \"h\" , [ 0.125 , 5 ]), # thickness of welds Continuous ( \"l\" , [ 0.1 , 10 ]), # length of welds Continuous ( \"t\" , [ 0.1 , 10 ]), # height of beam Continuous ( \"b\" , [ 0.125 , 5 ]), # width of beam ], outputs = [ Continuous ( \"cost\" ), Continuous ( \"deflection\" )], constraints = [ # h <= b, weld thickness cannot exceed beam width LinearInequality ([ \"h\" , \"b\" ], lhs = [ 1 , - 1 ], rhs = 0 ), # normal stress on the welds cannot exceed 30000 psi NonlinearInequality ( \"6000 * 6 * 14 / b / t**3 - 30000\" ), # buckling load capacity must exceed 6000 lbs NonlinearInequality ( \"6000 - 60746.022 * (1 - 0.0282346 * t) * t * b**4\" ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 , x2 , x3 , x4 = self . get_X ( X ) . T return pd . DataFrame ( { \"cost\" : 1.10471 * x1 ** 2 * x2 + 0.04811 * x3 * x4 * ( 14 + x2 ), \"deflection\" : 2.1952 / ( x4 * x3 ** 3 ), }, index = X . index , ) single Single objective benchmark problems. Ackley ( Problem ) Ackley benchmark problem. Source code in opti/problems/single.py class Ackley ( Problem ): \"\"\"Ackley benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Ackley problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 32.768 , + 32.768 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : a = 20 b = 1 / 5 c = 2 * np . pi n = self . n_inputs x = self . get_X ( X ) part1 = - a * np . exp ( - b * np . sqrt (( 1 / n ) * np . sum ( x ** 2 , axis =- 1 ))) part2 = - np . exp (( 1 / n ) * np . sum ( np . cos ( c * x ), axis =- 1 )) y = part1 + part2 + a + np . exp ( 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Branin ( Problem ) The Branin (Branin-Hoo) benchmark problem. f(x) = a(x2 - b x1^2 + cx1 - r)^2 + s(1 - t) cos(x1) + s a = 1, b = 5.1 / (4 pi^2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8pi) It has 3 global optima. Source code in opti/problems/single.py class Branin ( Problem ): \"\"\"The Branin (Branin-Hoo) benchmark problem. f(x) = a(x2 - b x1^2 + cx1 - r)^2 + s(1 - t) cos(x1) + s a = 1, b = 5.1 / (4 pi^2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8pi) It has 3 global optima. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Branin function\" , inputs = [ Continuous ( \"x1\" , [ - 5 , 10 ]), Continuous ( \"x2\" , [ 0 , 15 ])], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 , x2 = self . get_X ( X ) . T y = ( ( x2 - 5.1 / ( 4 * np . pi ** 2 ) * x1 ** 2 + 5 / np . pi * x1 - 6 ) ** 2 + 10 * ( 1 - 1 / ( 8 * np . pi )) * np . cos ( x1 ) + 10 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : return pd . DataFrame ( [ [ - np . pi , 12.275 , 0.397887 ], [ np . pi , 2.275 , 0.397887 ], [ 9.42478 , 2.475 , 0.397887 ], ], columns = self . inputs . names + self . outputs . names , ) Himmelblau ( Problem ) Himmelblau benchmark problem Source code in opti/problems/single.py class Himmelblau ( Problem ): \"\"\"Himmelblau benchmark problem\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Himmelblau function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 6 , 6 ]) for i in range ( 2 )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x0 , x1 = self . get_X ( X ) . T y = ( x0 ** 2 + x1 - 11 ) ** 2 + ( x0 + x1 ** 2 - 7 ) ** 2 return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . array ( [ [ 3.0 , 2.0 ], [ - 2.805118 , 3.131312 ], [ - 3.779310 , - 3.283186 ], [ 3.584428 , - 1.848126 ], ] ) y = np . zeros ( 4 ) return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Michalewicz ( Problem ) Michalewicz benchmark problem. The Michalewicz function has d! local minima, and it is multimodal. The parameter m (m=10 is used here) defines the steepness of they valleys and a larger m leads to a more difficult search. Source code in opti/problems/single.py class Michalewicz ( Problem ): \"\"\"Michalewicz benchmark problem. The Michalewicz function has d! local minima, and it is multimodal. The parameter m (m=10 is used here) defines the steepness of they valleys and a larger m leads to a more difficult search. \"\"\" def __init__ ( self , n_inputs : int = 2 ): super () . __init__ ( name = \"Michalewicz function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , np . pi ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) m = 10 i = np . arange ( 1 , self . n_inputs + 1 ) y = - np . sum ( np . sin ( x ) * np . sin ( i * x ** 2 / np . pi ) ** ( 2 * m ), axis = 1 ) return pd . DataFrame ({ \"y\" : y }, index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = pd . DataFrame ([[ 2.2 , 1.57 ]], columns = self . inputs . names ) return pd . concat ([ x , self . f ( x )], axis = 1 ) Rastrigin ( Problem ) Rastrigin benchmark problem. Source code in opti/problems/single.py class Rastrigin ( Problem ): \"\"\"Rastrigin benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Rastrigin function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 5 , 5 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) a = 10 y = a * self . n_inputs + np . sum ( x ** 2 - a * np . cos ( 2 * np . pi * x ), axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Rosenbrock ( Problem ) Rosenbrock benchmark problem. Source code in opti/problems/single.py class Rosenbrock ( Problem ): \"\"\"Rosenbrock benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Rosenbrock function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 2.048 , 2.048 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) . T y = np . sum ( 100 * ( x [ 1 :] - x [: - 1 ] ** 2 ) ** 2 + ( 1 - x [: - 1 ]) ** 2 , axis = 0 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . ones (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Schwefel ( Problem ) Schwefel benchmark problem Source code in opti/problems/single.py class Schwefel ( Problem ): \"\"\"Schwefel benchmark problem\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Schwefel function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 500 , 500 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) y = 418.9829 * self . n_inputs - np . sum ( x * np . sin ( np . abs ( x ) ** 0.5 ), axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . full (( 1 , self . n_inputs ), 420.9687 ) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Sphere ( Problem ) Sphere benchmark problem. Source code in opti/problems/single.py class Sphere ( Problem ): \"\"\"Sphere benchmark problem.\"\"\" def __init__ ( self , n_inputs = 10 ): super () . __init__ ( name = \"Sphere function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ 0 , 2 ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) y = np . sum (( x - 0.5 ) ** 2 , axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . full (( 1 , self . n_inputs ), 0.5 ) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) ThreeHumpCamel ( Problem ) Three-hump camel benchmark problem. Source code in opti/problems/single.py class ThreeHumpCamel ( Problem ): \"\"\"Three-hump camel benchmark problem.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Three-hump camel function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 5 , 5 ]) for i in range ( 2 )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 , x2 = self . get_X ( X ) . T y = 2 * x1 ** 2 - 1.05 * x1 ** 4 + x1 ** 6 / 6 + x1 * x2 + x2 ** 2 return pd . DataFrame ( y , columns = [ \"y\" ], index = X . index ) def get_optima ( self ) -> pd . DataFrame : return pd . DataFrame ( np . zeros (( 1 , 3 )), columns = self . inputs . names + self . outputs . names ) Zakharov ( Problem ) Zakharov benchmark problem. Source code in opti/problems/single.py class Zakharov ( Problem ): \"\"\"Zakharov benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Zakharov function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 10 , 10 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ): x = self . get_X ( X ) a = 0.5 * np . sum ( np . arange ( 1 , self . n_inputs + 1 ) * x , axis = 1 ) y = np . sum ( x ** 2 , axis = 1 ) + a ** 2 + a ** 4 return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names ) Zakharov_Categorical ( Problem ) Zakharov problem with one categorical input Source code in opti/problems/single.py class Zakharov_Categorical ( Problem ): \"\"\"Zakharov problem with one categorical input\"\"\" def __init__ ( self , n_inputs = 3 ): base = Zakharov ( n_inputs ) super () . __init__ ( name = \"Zakharov function with one categorical input\" , inputs = [ Continuous ( f \"x { i } \" , [ - 10 , 10 ]) for i in range ( n_inputs - 1 )] + [ Categorical ( \"expon_switch\" , [ \"one\" , \"two\" ])], outputs = base . outputs , ) def f ( self , X : pd . DataFrame ): x_conti = X [ self . inputs . names [: - 1 ]] . values # just the continuous inputs a = 0.5 * np . sum ( np . arange ( 1 , self . n_inputs ) * x_conti , axis = 1 ) powers = np . repeat ( np . expand_dims ([ 2.0 , 2.0 , 4.0 ], 0 ), repeats = len ( X ), axis = 0 ) modify_powers = X [ self . inputs . names [ - 1 ]] == \"two\" powers [ modify_powers , :] += powers [ modify_powers , :] res = ( np . sum ( x_conti ** np . expand_dims ( powers [:, 0 ], 1 ), axis = 1 ) + a ** np . expand_dims ( powers [:, 1 ], 0 ) + a ** np . expand_dims ( powers [:, 2 ], 0 ) ) res_float_array = np . array ( res , dtype = np . float64 ) . ravel () y = res_float_array return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = list ( np . zeros ( self . n_inputs - 1 )) + [ \"one\" ] y = [ 0 ] return pd . DataFrame ([ x + y ], columns = self . inputs . names + self . outputs . names ) Zakharov_Constrained ( Problem ) Zakharov problem with one linear constraint Source code in opti/problems/single.py class Zakharov_Constrained ( Problem ): \"\"\"Zakharov problem with one linear constraint\"\"\" def __init__ ( self , n_inputs = 5 ): base = Zakharov ( n_inputs ) self . base = base super () . __init__ ( name = \"Zakharov with one linear constraint\" , inputs = base . inputs , outputs = base . outputs , constraints = [ LinearInequality ( base . inputs . names , lhs = 1 , rhs = 10 )], f = base . f , ) def get_optima ( self ): return self . base . get_optima () Zakharov_NChooseKConstraint ( Problem ) Zakharov problem with an n-choose-k constraint Source code in opti/problems/single.py class Zakharov_NChooseKConstraint ( Problem ): \"\"\"Zakharov problem with an n-choose-k constraint\"\"\" def __init__ ( self , n_inputs = 5 , n_max_active = 3 ): base = Zakharov ( n_inputs ) self . base = base super () . __init__ ( name = \"Zakharov with n-choose-k constraint\" , inputs = base . inputs , outputs = base . outputs , constraints = [ NChooseK ( names = base . inputs . names , max_active = n_max_active )], f = base . f , ) def get_optima ( self ): return self . base . get_optima () univariate Simple 1D problems for assessing probabilistic surrogate models. Note: these problems should be output-noisified, e.g. import opti problem = opti.problems.noisify_problem_with_gaussian( opti.problems.Line1D(), sigma=0.1 ) Line1D ( Problem ) A line. Source code in opti/problems/univariate.py class Line1D ( Problem ): \"\"\"A line.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y\" : X . eval ( \"0.1 * x + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) Parabola1D ( Problem ) A parabola. Source code in opti/problems/univariate.py class Parabola1D ( Problem ): \"\"\"A parabola.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"0.025 * (x - 5) ** 2 + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) Sigmoid1D ( Problem ) A smooth step at x=5. Source code in opti/problems/univariate.py class Sigmoid1D ( Problem ): \"\"\"A smooth step at x=5.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"1 / (1 + exp(-2 * (x - 5))) + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) Sinus1D ( Problem ) A sinus-function with one full period over the domain. Source code in opti/problems/univariate.py class Sinus1D ( Problem ): \"\"\"A sinus-function with one full period over the domain.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"sin(x * 2 * 3.14159 / 10) / 2 + 2\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) Step1D ( Problem ) A discrete step at x=1.1. Source code in opti/problems/univariate.py class Step1D ( Problem ): \"\"\"A discrete step at x=1.1.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y\" : X . eval ( \"x > 1.1\" ) . astype ( float )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Discrete ( \"y\" , [ 0 , 1 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), ) zdt ZDT benchmark problem suite. All problems are bi-objective, have D continuous inputs and are unconstrained. Zitzler, Deb, Thiele 2000 - Comparison of Multiobjective Evolutionary Algorithms: Empirical Results http://dx.doi.org/10.1162/106365600568202 ZDT1 ( Problem ) ZDT-1 benchmark problem. Source code in opti/problems/zdt.py class ZDT1 ( Problem ): \"\"\"ZDT-1 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-1 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 0.5 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . sqrt ( x )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names ) ZDT2 ( Problem ) ZDT-2 benchmark problem. Source code in opti/problems/zdt.py class ZDT2 ( Problem ): \"\"\"ZDT-2 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-2 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . power ( x , 2 )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names ) ZDT3 ( Problem ) ZDT-3 benchmark problem. Source code in opti/problems/zdt.py class ZDT3 ( Problem ): \"\"\"ZDT-3 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-3 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - np . inf , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 0.5 - ( y1 / g ) * np . sin ( 10 * np . pi * y1 )) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): regions = [ [ 0 , 0.0830015349 ], [ 0.182228780 , 0.2577623634 ], [ 0.4093136748 , 0.4538821041 ], [ 0.6183967944 , 0.6525117038 ], [ 0.8233317983 , 0.8518328654 ], ] pf = [] for r in regions : x1 = np . linspace ( r [ 0 ], r [ 1 ], int ( points / len ( regions ))) x2 = 1 - np . sqrt ( x1 ) - x1 * np . sin ( 10 * np . pi * x1 ) pf . append ( np . stack ([ x1 , x2 ], axis = 1 )) y = np . concatenate ( pf , axis = 0 ) return pd . DataFrame ( y , columns = self . outputs . names ) ZDT4 ( Problem ) ZDT-4 benchmark problem. Source code in opti/problems/zdt.py class ZDT4 ( Problem ): \"\"\"ZDT-4 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 10 ): super () . __init__ ( name = \"ZDT-4 problem\" , inputs = [ Continuous ( \"x1\" , [ 0 , 1 ])] + [ Continuous ( f \"x { i + 1 } \" , [ - 5 , 5 ]) for i in range ( 1 , n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . to_numpy () g = 1 + 10 * ( self . n_inputs - 1 ) for i in range ( 1 , self . n_inputs ): g += x [:, i ] ** 2 - 10 * np . cos ( 4.0 * np . pi * x [:, i ]) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - np . sqrt ( y1 / g )) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . sqrt ( x )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names ) ZDT6 ( Problem ) ZDT-6 benchmark problem. Source code in opti/problems/zdt.py class ZDT6 ( Problem ): \"\"\"ZDT-6 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-6 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - np . inf , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . to_numpy () n = self . n_inputs g = 1 + 9 * ( np . sum ( x [:, 1 :], axis = 1 ) / ( n - 1 )) ** 0.25 y1 = 1 - np . exp ( - 4 * x [:, 0 ]) * ( np . sin ( 6 * np . pi * x [:, 0 ])) ** 6 y2 = g * ( 1 - ( y1 / g ) ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0.2807753191 , 1 , points ) y = np . stack ([ x , 1 - x ** 2 ], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"Test Problems"},{"location":"ref-problems/#test-problems","text":"","title":"Test Problems"},{"location":"ref-problems/#opti.problems.datasets","text":"Chemical datasets. These problems contain observed data but don't come with a ground truth.","title":"datasets"},{"location":"ref-problems/#opti.problems.datasets.Alkox","text":"Alkoxylation dataset This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx). The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective. Reference F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Alkox ( Problem ): \"\"\"Alkoxylation dataset This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx). The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective. Reference: F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. [DOI](https://doi.org/10.1088/2632-2153/abedc8). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Alkox\" , inputs = [ Continuous ( \"residence_time\" , domain = [ 0.05 , 1 ]), Continuous ( \"ratio\" , domain = [ 0.5 , 10 ]), Continuous ( \"concentration\" , domain = [ 2 , 8 ]), Continuous ( \"temperature\" , domain = [ 6 , 8 ]), ], outputs = [ Continuous ( \"conversion\" )], objectives = [ Maximize ( \"conversion\" )], data = get_data ( \"alkox.csv\" ), )","title":"Alkox"},{"location":"ref-problems/#opti.problems.datasets.BaumgartnerAniline","text":"Aniline C-N cross-coupling dataset. Reference Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI . Data obtained from Summit . Source code in opti/problems/datasets.py class BaumgartnerAniline ( Problem ): \"\"\"Aniline C-N cross-coupling dataset. Reference: Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases [DOI](https://doi.org/10.1021/acs.oprd.9b00236). Data obtained from [Summit](https://github.com/sustainable-processes/summit). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Aniline cross-coupling, Baumgartner 2019\" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"tBuXPhos\" , \"tBuBrettPhos\" , \"AlPhos\" ]), Categorical ( \"base\" , domain = [ \"TEA\" , \"TMG\" , \"BTMG\" , \"DBU\" ]), Continuous ( \"base_equivalents\" , domain = [ 1.0 , 2.5 ]), Continuous ( \"temperature\" , domain = [ 30 , 100 ]), Continuous ( \"residence_time\" , domain = [ 60 , 1800 ]), ], outputs = [ Continuous ( \"yield\" , domain = [ 0 , 1 ])], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"baumgartner_aniline.csv\" ), )","title":"BaumgartnerAniline"},{"location":"ref-problems/#opti.problems.datasets.BaumgartnerBenzamide","text":"Benzamide C-N cross-coupling dataset. Reference Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI . Data obtained from Summit . Source code in opti/problems/datasets.py class BaumgartnerBenzamide ( Problem ): \"\"\"Benzamide C-N cross-coupling dataset. Reference: Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases [DOI](https://doi.org/10.1021/acs.oprd.9b00236). Data obtained from [Summit](https://github.com/sustainable-processes/summit). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Benzamide cross-coupling, Baumgartner 2019\" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"tBuXPhos\" , \"tBuBrettPhos\" ]), Categorical ( \"base\" , domain = [ \"TMG\" , \"BTMG\" , \"DBU\" , \"MTBD\" ]), Continuous ( \"base_equivalents\" , domain = [ 1.0 , 2.1 ]), Continuous ( \"temperature\" , domain = [ 30 , 100 ]), Continuous ( \"residence_time\" , domain = [ 60 , 1850 ]), ], outputs = [ Continuous ( \"yield\" , domain = [ 0 , 1 ])], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"baumgartner_benzamide.csv\" ), )","title":"BaumgartnerBenzamide"},{"location":"ref-problems/#opti.problems.datasets.Benzylation","text":"Benzylation dataset. This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction. Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective. Reference A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Benzylation ( Problem ): \"\"\"Benzylation dataset. This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction. Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective. Reference: A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. [DOI](https://doi.org/10.1016/j.cej.2018.07.031). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Benzylation\" , inputs = [ Continuous ( \"flow_rate\" , domain = [ 0.2 , 0.4 ]), Continuous ( \"ratio\" , domain = [ 1.0 , 5.0 ]), Continuous ( \"solvent\" , domain = [ 0.5 , 1.0 ]), Continuous ( \"temperature\" , domain = [ 110.0 , 150.0 ]), ], outputs = [ Continuous ( \"impurity\" )], objectives = [ Minimize ( \"impurity\" )], data = get_data ( \"benzylation.csv\" ), )","title":"Benzylation"},{"location":"ref-problems/#opti.problems.datasets.Cake","text":"Cake recipe optimization with mixed objectives. Source code in opti/problems/datasets.py class Cake ( Problem ): \"\"\"Cake recipe optimization with mixed objectives.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Cake\" , inputs = [ Continuous ( \"wheat_flour\" , domain = [ 0 , 1 ]), Continuous ( \"spelt_flour\" , domain = [ 0 , 1 ]), Continuous ( \"sugar\" , domain = [ 0 , 1 ]), Continuous ( \"chocolate\" , domain = [ 0 , 1 ]), Continuous ( \"nuts\" , domain = [ 0 , 1 ]), Continuous ( \"carrot\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( \"calories\" , domain = [ 300 , 600 ]), Continuous ( \"taste\" , domain = [ 0 , 5 ]), Continuous ( \"browning\" , domain = [ 0 , 2 ]), ], objectives = [ Minimize ( \"calories\" ), Maximize ( \"taste\" ), CloseToTarget ( \"browning\" , target = 1.4 ), ], constraints = [ LinearEquality ( [ \"wheat_flour\" , \"spelt_flour\" , \"sugar\" , \"chocolate\" , \"nuts\" , \"carrot\" , ], rhs = 1 , ) ], data = get_data ( \"cake.csv\" ), )","title":"Cake"},{"location":"ref-problems/#opti.problems.datasets.Fullerenes","text":"Buckminsterfullerene dataset. This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes. Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product. Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective. Reference B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Fullerenes ( Problem ): \"\"\"Buckminsterfullerene dataset. This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes. Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product. Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective. Reference: B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798. [DOI](https://doi.org/10.1039/C7RE00123A). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Fullerenes\" , inputs = [ Continuous ( \"reaction_time\" , domain = [ 3.0 , 31.0 ]), Continuous ( \"sultine\" , domain = [ 1.5 , 6.0 ]), Continuous ( \"temperature\" , domain = [ 100.0 , 150.0 ]), ], outputs = [ Continuous ( \"product\" )], objectives = [ Maximize ( \"product\" )], data = get_data ( \"fullerenes.csv\" ), )","title":"Fullerenes"},{"location":"ref-problems/#opti.problems.datasets.HPLC","text":"High-performance liquid chromatography dataset. This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective. Reference L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class HPLC ( Problem ): \"\"\"High-performance liquid chromatography dataset. This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective. Reference: L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) [DOI](https://doi.org/10.26434/chemrxiv.5953606.v1). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"HPLC\" , inputs = [ Continuous ( \"sample_loop\" , domain = [ 0.0 , 0.08 ]), Continuous ( \"additional_volume\" , domain = [ 0.0 , 0.06 ]), Continuous ( \"tubing_volume\" , domain = [ 0.1 , 0.9 ]), Continuous ( \"sample_flow\" , domain = [ 0.5 , 2.5 ]), Continuous ( \"push_speed\" , domain = [ 80.0 , 150 ]), Continuous ( \"wait_time\" , domain = [ 0.5 , 10.0 ]), ], outputs = [ Continuous ( \"peak_area\" )], objectives = [ Maximize ( \"peak_area\" )], data = get_data ( \"hplc.csv\" ), )","title":"HPLC"},{"location":"ref-problems/#opti.problems.datasets.Photodegradation","text":"Photodegration dataset. This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective. Reference S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Photodegradation ( Problem ): \"\"\"Photodegration dataset. This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective. Reference: S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. [DOI](https://doi.org/10.1002/adma.201907801). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Photodegradation\" , inputs = [ Continuous ( \"PCE10\" , domain = [ 0 , 1 ]), Continuous ( \"WF3\" , domain = [ 0 , 1 ]), Continuous ( \"P3HT\" , domain = [ 0 , 1 ]), Continuous ( \"PCBM\" , domain = [ 0 , 1 ]), Continuous ( \"oIDTBR\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( \"degradation\" )], objectives = [ Minimize ( \"degradation\" )], constraints = [ LinearEquality ( [ \"PCE10\" , \"WF3\" , \"P3HT\" , \"PCBM\" , \"oIDTBR\" ], rhs = 1 , lhs = 1 ), NChooseK ([ \"PCE10\" , \"WF3\" ], max_active = 1 ), ], data = get_data ( \"photodegradation.csv\" ), )","title":"Photodegradation"},{"location":"ref-problems/#opti.problems.datasets.ReizmanSuzuki","text":"Suzuki-Miyaura cross-coupling optimization. Each case was has a different set of substrates but the same possible catalysts. Reference Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry & engineering, 1(6), 658-666 DOI . Data obtained from Summit . Source code in opti/problems/datasets.py class ReizmanSuzuki ( Problem ): \"\"\"Suzuki-Miyaura cross-coupling optimization. Each case was has a different set of substrates but the same possible catalysts. Reference: Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry & engineering, 1(6), 658-666 [DOI](https://doi.org/10.1039/C6RE00153J). Data obtained from [Summit](https://github.com/sustainable-processes/summit). \"\"\" def __init__ ( self , case = 1 ): assert case in [ 1 , 2 , 3 , 4 ] super () . __init__ ( name = f \"Reizman 2016 - Suzuki Case { case } \" , inputs = [ Categorical ( \"catalyst\" , domain = [ \"P1-L1\" , \"P2-L1\" , \"P1-L2\" , \"P1-L3\" , \"P1-L4\" , \"P1-L5\" , \"P1-L6\" , \"P1-L7\" , ], ), Continuous ( \"t_res\" , domain = [ 60 , 600 ]), Continuous ( \"temperature\" , domain = [ 30 , 110 ]), Continuous ( \"catalyst_loading\" , domain = [ 0.496 , 2.515 ]), ], outputs = [ Continuous ( \"ton\" , domain = [ 0 , 100 ]), Continuous ( \"yield\" , domain = [ 0 , 100 ]), ], objectives = [ Maximize ( \"ton\" ), Maximize ( \"yield\" )], data = get_data ( f \"reizman_suzuki { case } .csv\" ), )","title":"ReizmanSuzuki"},{"location":"ref-problems/#opti.problems.datasets.SnAr","text":"SnAr reaction optimization. This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism. Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective. Reference A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class SnAr ( Problem ): \"\"\"SnAr reaction optimization. This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism. Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective. Reference: A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. [DOI](https://doi.org/10.1016/j.cej.2018.07.031). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"SnAr\" , inputs = [ Continuous ( \"residence_time\" , domain = [ 0.5 , 2.0 ]), Continuous ( \"ratio\" , domain = [ 1.0 , 5.0 ]), Continuous ( \"concentration\" , domain = [ 0.1 , 0.5 ]), Continuous ( \"temperature\" , domain = [ 60.0 , 140.0 ]), ], outputs = [ Continuous ( \"impurity\" )], objectives = [ Minimize ( \"impurity\" )], data = get_data ( \"snar.csv\" ), )","title":"SnAr"},{"location":"ref-problems/#opti.problems.datasets.Suzuki","text":"Suzuki reaction dataset. This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate. Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective. Reference F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI . Obtained from Olympus . Source code in opti/problems/datasets.py class Suzuki ( Problem ): \"\"\"Suzuki reaction dataset. This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate. Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective. Reference: F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. [DOI](https://doi.org/10.1088/2632-2153/abedc8). Obtained from [Olympus](https://github.com/aspuru-guzik-group/olympus). \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Suzuki\" , inputs = [ Continuous ( \"temperature\" , domain = [ 75.0 , 90.0 ]), Continuous ( \"pd_mol\" , domain = [ 0.5 , 5.0 ]), Continuous ( \"arbpin\" , domain = [ 1.0 , 1.8 ]), Continuous ( \"k3po4\" , domain = [ 1.5 , 3.0 ]), ], outputs = [ Continuous ( \"yield\" )], objectives = [ Maximize ( \"yield\" )], data = get_data ( \"suzuki.csv\" ), )","title":"Suzuki"},{"location":"ref-problems/#opti.problems.detergent","text":"","title":"detergent"},{"location":"ref-problems/#opti.problems.detergent.Detergent","text":"Detergent formulation problem. There are 5 outputs representing the washing performance on different stain types. Each output is modeled as a second degree polynomial. The formulation consists of 5 components. The sixth input is a filler (water) and is factored out and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. Source code in opti/problems/detergent.py class Detergent ( Problem ): \"\"\"Detergent formulation problem. There are 5 outputs representing the washing performance on different stain types. Each output is modeled as a second degree polynomial. The formulation consists of 5 components. The sixth input is a filler (water) and is factored out and it's parameter bounds 0.6 < water < 0.8 result in 2 linear inequality constraints for the other parameters. \"\"\" def __init__ ( self ): # coefficients for the 2-order polynomial; generated with # base = 3 * np.ones((1, 5)) # scale = PolynomialFeatures(degree=2).fit_transform(base).T # coef = np.random.RandomState(42).normal(scale=scale, size=(len(scale), 5)) # coef = np.clip(coef, 0, None) self . coef = np . array ( [ [ 0.4967 , 0.0 , 0.6477 , 1.523 , 0.0 ], [ 0.0 , 4.7376 , 2.3023 , 0.0 , 1.6277 ], [ 0.0 , 0.0 , 0.7259 , 0.0 , 0.0 ], [ 0.0 , 0.0 , 0.9427 , 0.0 , 0.0 ], [ 4.3969 , 0.0 , 0.2026 , 0.0 , 0.0 ], [ 0.3328 , 0.0 , 1.1271 , 0.0 , 0.0 ], [ 0.0 , 16.6705 , 0.0 , 0.0 , 7.4029 ], [ 0.0 , 1.8798 , 0.0 , 0.0 , 1.7718 ], [ 6.6462 , 1.5423 , 0.0 , 0.0 , 0.0 ], [ 0.0 , 0.0 , 9.5141 , 3.0926 , 0.0 ], [ 2.9168 , 0.0 , 0.0 , 5.5051 , 9.279 ], [ 8.3815 , 0.0 , 0.0 , 2.9814 , 8.7799 ], [ 0.0 , 0.0 , 0.0 , 0.0 , 7.3127 ], [ 12.2062 , 0.0 , 9.0318 , 3.2547 , 0.0 ], [ 3.2526 , 13.8423 , 0.0 , 14.0818 , 0.0 ], [ 7.3971 , 0.7834 , 0.0 , 0.8258 , 0.0 ], [ 0.0 , 3.214 , 13.301 , 0.0 , 0.0 ], [ 0.0 , 8.2386 , 2.9588 , 0.0 , 4.6194 ], [ 0.8737 , 8.7178 , 0.0 , 0.0 , 0.0 ], [ 0.0 , 2.6651 , 2.3495 , 0.046 , 0.0 ], [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ], ] ) super () . __init__ ( name = \"Detergent optimization\" , inputs = [ Continuous ( \"x1\" , domain = [ 0.0 , 0.2 ]), Continuous ( \"x2\" , domain = [ 0.0 , 0.3 ]), Continuous ( \"x3\" , domain = [ 0.02 , 0.2 ]), Continuous ( \"x4\" , domain = [ 0.0 , 0.06 ]), Continuous ( \"x5\" , domain = [ 0.0 , 0.04 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" , domain = [ 0 , 3 ]) for i in range ( 5 )], objectives = [ Maximize ( f \"y { i + 1 } \" ) for i in range ( 5 )], constraints = [ LinearInequality ([ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"x5\" ], lhs =- 1 , rhs =- 0.2 ), LinearInequality ([ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"x5\" ], lhs = 1 , rhs = 0.4 ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = np . atleast_2d ( X [ self . inputs . names ]) xp = np . stack ([ _poly2 ( xi ) for xi in x ], axis = 0 ) return pd . DataFrame ( xp @ self . coef , columns = self . outputs . names , index = X . index )","title":"Detergent"},{"location":"ref-problems/#opti.problems.detergent.Detergent_NChooseKConstraint","text":"Variant of the Detergent problem where only 3 of the 5 formulation components are allowed to be active (n-choose-k constraint). Source code in opti/problems/detergent.py class Detergent_NChooseKConstraint ( Problem ): \"\"\"Variant of the Detergent problem where only 3 of the 5 formulation components are allowed to be active (n-choose-k constraint).\"\"\" def __init__ ( self ): base = Detergent () super () . __init__ ( name = \"Detergent optimization with n-choose-k constraint\" , inputs = base . inputs , outputs = base . outputs , objectives = base . objectives , constraints = list ( base . constraints ) + [ NChooseK ( names = base . inputs . names , max_active = 3 )], f = base . f , )","title":"Detergent_NChooseKConstraint"},{"location":"ref-problems/#opti.problems.detergent.Detergent_OutputConstraint","text":"Variant of the Detergent problem with an additional output/black-box constraint. In addition to the 5 washing performances there is a sixth output reflecting the stability of the formulation. If discrete=True the stability can only be measured qualitatively (0: not stable, 1: stable). If discrete=True the stability can be measured quantively with smaller values indicating less stable formulations. Source code in opti/problems/detergent.py class Detergent_OutputConstraint ( Problem ): \"\"\"Variant of the Detergent problem with an additional output/black-box constraint. In addition to the 5 washing performances there is a sixth output reflecting the stability of the formulation. If `discrete=True` the stability can only be measured qualitatively (0: not stable, 1: stable). If `discrete=True` the stability can be measured quantively with smaller values indicating less stable formulations. \"\"\" def __init__ ( self , discrete = False ): base = Detergent () def f ( X ): Y = base . f ( X ) if discrete : Y [ \"stable\" ] = ( X . sum ( axis = 1 ) < 0.3 ) . astype ( int ) else : Y [ \"stable\" ] = ( 0.4 - X . sum ( axis = 1 )) / 0.2 return Y outputs = list ( base . outputs ) if discrete : outputs += [ Discrete ( \"stable\" , domain = [ 0 , 1 ])] else : outputs += [ Continuous ( \"stable\" , domain = [ 0 , 1 ])] super () . __init__ ( name = \"Detergent optimization with stability constraint\" , inputs = base . inputs , outputs = outputs , objectives = base . objectives , output_constraints = [ Maximize ( \"stable\" , target = 0.5 )], constraints = base . constraints , f = f , )","title":"Detergent_OutputConstraint"},{"location":"ref-problems/#opti.problems.detergent.Detergent_TwoOutputConstraints","text":"Variant of the Detergent problem with two outputs constraint. In addition to the 5 washing performances there are two more outputs measuring the formulation stability. The first, stability 1, measures the immediate stability. If not stable, the other properties cannot be measured, except for stability 2. The second, stability 2, measures the long-term stability. Source code in opti/problems/detergent.py class Detergent_TwoOutputConstraints ( Problem ): \"\"\"Variant of the Detergent problem with two outputs constraint. In addition to the 5 washing performances there are two more outputs measuring the formulation stability. The first, stability 1, measures the immediate stability. If not stable, the other properties cannot be measured, except for stability 2. The second, stability 2, measures the long-term stability. \"\"\" def __init__ ( self ): base = Detergent () def f ( X : pd . DataFrame ) -> pd . DataFrame : Y = base . f ( X ) x = self . get_X ( X ) stable1 = ( x . sum ( axis = 1 ) < 0.3 ) . astype ( int ) stable2 = ( x [:, : - 1 ] . sum ( axis = 1 ) < 0.25 ) . astype ( int ) Y [ stable1 == 0 ] = np . nan Y [ \"stability 1\" ] = stable1 Y [ \"stability 2\" ] = stable2 return Y outputs = list ( base . outputs ) + [ Discrete ( \"stability 1\" , domain = [ 0 , 1 ]), Discrete ( \"stability 2\" , domain = [ 0 , 1 ]), ] super () . __init__ ( name = \"Detergent optimization with two output constraint\" , inputs = base . inputs , outputs = outputs , objectives = base . objectives , output_constraints = [ Maximize ( \"stability 1\" , target = 0.5 ), Maximize ( \"stability 2\" , target = 0.5 ), ], constraints = base . constraints , f = f , )","title":"Detergent_TwoOutputConstraints"},{"location":"ref-problems/#opti.problems.mixed","text":"Mixed variables single and multi-objective test problems.","title":"mixed"},{"location":"ref-problems/#opti.problems.mixed.DiscreteFuelInjector","text":"Fuel injector test problem, modified to contain an integer variable. 4 objectives, mixed variables, unconstrained See Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 Source code in opti/problems/mixed.py class DiscreteFuelInjector ( Problem ): \"\"\"Fuel injector test problem, modified to contain an integer variable. * 4 objectives, * mixed variables, unconstrained See: Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Discrete fuel injector test problem\" , inputs = [ Discrete ( \"x1\" , [ 0 , 1 , 2 , 3 ]), Continuous ( \"x2\" , [ - 2 , 2 ]), Continuous ( \"x3\" , [ - 2 , 2 ]), Continuous ( \"x4\" , [ - 2 , 2 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 4 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 = X [ \"x1\" ] . to_numpy () . astype ( float ) x2 = X [ \"x2\" ] . to_numpy () . astype ( float ) x3 = X [ \"x3\" ] . to_numpy () . astype ( float ) x4 = X [ \"x4\" ] . to_numpy () . astype ( float ) x1 *= 0.2 y1 = ( 0.692 + 0.4771 * x1 - 0.687 * x4 - 0.08 * x3 - 0.065 * x2 - 0.167 * x1 ** 2 - 0.0129 * x1 * x4 + 0.0796 * x4 ** 2 - 0.0634 * x1 * x3 - 0.0257 * x3 * x4 + 0.0877 * x3 ** 2 - 0.0521 * x1 * x2 + 0.00156 * x2 * x4 + 0.00198 * x2 * x3 + 0.0184 * x2 ** 2 ) y2 = ( 0.37 - 0.205 * x1 + 0.0307 * x4 + 0.108 * x3 + 1.019 * x2 - 0.135 * x1 ** 2 + 0.0141 * x1 * x4 + 0.0998 * x4 ** 2 + 0.208 * x1 * x3 - 0.0301 * x3 * x4 - 0.226 * x3 ** 2 + 0.353 * x1 * x2 - 0.0497 * x2 * x3 - 0.423 * x2 ** 2 + 0.202 * x1 ** 2 * x4 - 0.281 * x1 ** 2 * x3 - 0.342 * x1 * x4 ** 2 - 0.245 * x3 * x4 ** 2 + 0.281 * x3 ** 2 * x4 - 0.184 * x1 * x2 ** 2 + 0.281 * x1 * x3 * x4 ) y3 = ( 0.153 - 0.322 * x1 + 0.396 * x4 + 0.424 * x3 + 0.0226 * x2 + 0.175 * x1 ** 2 + 0.0185 * x1 * x4 - 0.0701 * x4 ** 2 - 0.251 * x1 * x3 + 0.179 * x3 * x4 + 0.015 * x3 ** 2 + 0.0134 * x1 * x2 + 0.0296 * x2 * x4 + 0.0752 * x2 * x3 + 0.0192 * x2 ** 2 ) y4 = ( 0.758 + 0.358 * x1 - 0.807 * x4 + 0.0925 * x3 - 0.0468 * x2 - 0.172 * x1 ** 2 + 0.0106 * x1 * x4 + 0.0697 * x4 ** 2 - 0.146 * x1 * x3 - 0.0416 * x3 * x4 + 0.102 * x3 ** 2 - 0.0694 * x1 * x2 - 0.00503 * x2 * x4 + 0.0151 * x2 * x3 + 0.0173 * x2 ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 , \"y3\" : y3 , \"y4\" : y4 })","title":"DiscreteFuelInjector"},{"location":"ref-problems/#opti.problems.mixed.DiscreteVLMOP2","text":"VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. 2 minimization objectives 1 categorical and n continuous inputs, unconstrained See Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 Source code in opti/problems/mixed.py class DiscreteVLMOP2 ( Problem ): \"\"\"VLMOP2 problem (also known as Fonzeca & Fleming), modified to contain a discrete variable. * 2 minimization objectives * 1 categorical and n continuous inputs, unconstrained See: Manson2021, MVMOO: Mixed variable multi-objective optimisation https://doi.org/10.1007/s10898-021-01052-9 \"\"\" def __init__ ( self , n_inputs : int = 3 ): assert n_inputs >= 2 super () . __init__ ( name = \"Discrete VLMOP2 test problem\" , inputs = [ Categorical ( \"x1\" , [ \"a\" , \"b\" ])] + [ Continuous ( f \"x { i + 1 } \" , [ - 2 , 2 ]) for i in range ( 1 , n_inputs )], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : d = X [ self . inputs . names [ 0 ]] . values x = X [ self . inputs . names [ 1 :]] . values n = self . n_inputs y1 = np . exp ( - np . sum (( x - n **- 0.5 ) ** 2 , axis = 1 )) y2 = np . exp ( - np . sum (( x + n **- 0.5 ) ** 2 , axis = 1 )) y1 = np . where ( d == \"a\" , 1 - y1 , 1.25 - y1 ) y2 = np . where ( d == \"a\" , 1 - y2 , 0.75 - y2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index )","title":"DiscreteVLMOP2"},{"location":"ref-problems/#opti.problems.multi","text":"","title":"multi"},{"location":"ref-problems/#opti.problems.multi.Daechert1","text":"Problem with a non-convex Pareto front. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. Source code in opti/problems/multi.py class Daechert1 ( Problem ): \"\"\"Problem with a non-convex Pareto front. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [-1.37, -1.61, -4] and the nadir is [0, 0, -1.44]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-1\" , inputs = [ Continuous ( \"x1\" , domain = [ 0 , np . pi ]), Continuous ( \"x2\" , domain = [ 0 , 10 ]), Continuous ( \"x3\" , domain = [ 1.2 , 10 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], constraints = [ NonlinearInequality ( \"- cos(x1) - exp(-x2) + x3\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y1\" : - X [ \"x1\" ], \"y2\" : - X [ \"x2\" ], \"y3\" : - X [ \"x3\" ] ** 2 })","title":"Daechert1"},{"location":"ref-problems/#opti.problems.multi.Daechert2","text":"Unconstrained problem with a Pareto front resembling a comet. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. Source code in opti/problems/multi.py class Daechert2 ( Problem ): \"\"\"Unconstrained problem with a Pareto front resembling a comet. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 minimize f1(x) = (1 + x3) (x1^3 x2^2 - 10 x1 - 4 x2) f2(x) = (1 + x3) (x1^3 x2^2 - 10 x1 + 4 x2) f3(x) = 3 (1 + x3) x1^2 s.t. 1 <= x1 <= 3.5 -2 <= x2 <= 2 0 <= x3 <= 1 The ideal point is [-70.19, -70.19, 3] and the nadir is [4, 4, 73.5]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-2\" , inputs = [ Continuous ( \"x1\" , domain = [ 1 , 3.5 ]), Continuous ( \"x2\" , domain = [ - 2 , 2 ]), Continuous ( \"x3\" , domain = [ 0 , 1 ]), ], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y1\" : X . eval ( \"(1 + x3) * (x1**3 * x2**2 - 10 * x1 - 4 * x2)\" ), \"y2\" : X . eval ( \"(1 + x3) * (x1**3 * x2**2 - 10 * x1 + 4 * x2)\" ), \"y3\" : X . eval ( \"3 * (1 + x3) * x1**2\" ), } )","title":"Daechert2"},{"location":"ref-problems/#opti.problems.multi.Daechert3","text":"Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. Source code in opti/problems/multi.py class Daechert3 ( Problem ): \"\"\"Modification of DTLZ7, with a Pareto consisting of 4 disconnected parts. From D\u00e4chert & Teichert 2020, An improved hyperboxing algorithm for calculating a Pareto front representation, https://arxiv.org/abs/2003.14249 The ideal point is [0, 0, 2.61] and the nadir is [0.86, 0.86, 6]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Daechert-3\" , inputs = [ Continuous ( f \"x { i + 1 } \" , domain = [ 0 , 1 ]) for i in range ( 2 )], outputs = [ Continuous ( f \"y { i + 1 } \" ) for i in range ( 3 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] return pd . DataFrame ( { \"y1\" : X [ \"x1\" ], \"y2\" : X [ \"x2\" ], \"y3\" : 6 - np . sum ( x * ( 1 + np . sin ( 3 * np . pi * x )), axis = 1 ), } )","title":"Daechert3"},{"location":"ref-problems/#opti.problems.multi.Hyperellipsoid","text":"Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Parameters: Name Type Description Default n int Dimension of the hyperellipsoid. Defaults to 5. 5 a list-like Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. None Source code in opti/problems/multi.py class Hyperellipsoid ( Problem ): \"\"\"Hyperellipsoid in n dimensions minimize f_m(x) = x_m m = 1, ... n for x in R^n s.t. sum((x / a)^2) - 1 <= 0 The ideal point is -a and the is nadir 0^n. Args: n (int, optional): Dimension of the hyperellipsoid. Defaults to 5. a (list-like, optional): Half length of principal axes. a = None or a = [1, ...] results in a hypersphere. \"\"\" def __init__ ( self , n : int = 5 , a : Optional [ Union [ list , np . ndarray ]] = None ): if a is None : a = np . ones ( n ) constr = \" + \" . join ([ f \"x { i + 1 } **2\" for i in range ( n )]) + \" - 1\" else : a = np . array ( a ) . squeeze () if len ( a ) != n : raise ValueError ( \"Dimension of half axes doesn't match input dimension\" ) constr = \" + \" . join ([ f \"(x { i + 1 } / { a [ i ] } )**2\" for i in range ( n )]) + \" - 1\" self . a = a super () . __init__ ( name = \"Hyperellipsoid\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - a [ i ], a [ i ]]) for i in range ( n )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - a [ i ], a [ i ]]) for i in range ( n )], constraints = [ NonlinearInequality ( constr )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : y = X [ self . inputs . names ] y . columns = self . outputs . names return y def get_optima ( self , n = 10 ) -> pd . DataFrame : X = opti . sampling . sphere . sample ( self . n_inputs , n , positive = True ) X = np . concatenate ([ - np . eye ( self . n_inputs ), - X ], axis = 0 )[: n ] Y = self . a * X return pd . DataFrame ( data = np . column_stack ([ X , Y ]), columns = self . inputs . names + self . outputs . names , )","title":"Hyperellipsoid"},{"location":"ref-problems/#opti.problems.multi.OmniTest","text":"Bi-objective benchmark problem with D inputs and a multi-modal Pareto set. It has 3^D Pareto subsets in the decision space corresponding to the same Pareto front. Reference Deb & Tiwari \"Omni-optimizer: A generic evolutionary algorithm for single and multi-objective optimization\" Source code in opti/problems/multi.py class OmniTest ( Problem ): \"\"\"Bi-objective benchmark problem with D inputs and a multi-modal Pareto set. It has 3^D Pareto subsets in the decision space corresponding to the same Pareto front. Reference: Deb & Tiwari \"Omni-optimizer: A generic evolutionary algorithm for single and multi-objective optimization\" \"\"\" def __init__ ( self , n_inputs : int = 2 ): super () . __init__ ( name = \"Omni\" , inputs = [ Continuous ( f \"x { i + 1 } \" , domain = [ 0 , 6 ]) for i in range ( 2 )], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : X = X [ self . inputs . names ] return pd . DataFrame ( { \"y1\" : np . sum ( np . sin ( np . pi * X ), axis = 1 ), \"y2\" : np . sum ( np . cos ( np . pi * X ), axis = 1 ), } ) def get_optima ( self ) -> pd . DataFrame : n = 11 # points per set (3^D sets) s = [ np . linspace ( 1 , 1.5 , n ) + 2 * i for i in range ( 3 )] C = list ( product ( * [ s , ] * self . n_inputs ) ) C = np . moveaxis ( C , 1 , 2 ) . reshape ( - 1 , 2 ) X = pd . DataFrame ( C , columns = self . inputs . names ) XY = pd . concat ([ X , self . f ( X )], axis = 1 ) XY [ \"_patch\" ] = np . repeat ( np . arange ( 3 ** self . n_inputs ), n ) return XY","title":"OmniTest"},{"location":"ref-problems/#opti.problems.multi.Poloni","text":"Poloni benchmark problem. Source code in opti/problems/multi.py class Poloni ( Problem ): \"\"\"Poloni benchmark problem.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Poloni function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - np . pi , np . pi ]) for i in range ( 2 )], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 , x2 = self . get_X ( X ) . T A1 = 0.5 * np . sin ( 1 ) - 2 * np . cos ( 1 ) + np . sin ( 2 ) - 1.5 * np . cos ( 2 ) A2 = 1.5 * np . sin ( 1 ) - np . cos ( 1 ) + 2 * np . sin ( 2 ) - 0.5 * np . cos ( 2 ) B1 = 0.5 * np . sin ( x1 ) - 2 * np . cos ( x1 ) + np . sin ( x2 ) - 1.5 * np . cos ( x2 ) B2 = 1.5 * np . sin ( x1 ) - np . cos ( x1 ) + 2 * np . sin ( x2 ) - 0.5 * np . cos ( x2 ) return pd . DataFrame ( { \"y1\" : 1 + ( A1 - B1 ) ** 2 + ( A2 - B2 ) ** 2 , \"y2\" : ( x1 + 3 ) ** 2 + ( x2 + 1 ) ** 2 , }, index = X . index , )","title":"Poloni"},{"location":"ref-problems/#opti.problems.multi.Qapi1","text":"Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. Source code in opti/problems/multi.py class Qapi1 ( Problem ): \"\"\"Constrained problem from the Qriteria API tests. Note that while the function is convex, the constraints are not. minimize f1(x) = (x1 - 2)^2 + (x2 - 1)^2 f2(x) = x1^2 + (x2 - 3)^2 for x1 in [0, inf) x2 in (-inf, inf) s.t. 0 <= x1 c1(x) = - x1^2 + x2 <= 0 c2(x) = - x1 - x2 + 2 <= 0 The ideal point is [0, 0] and the nadir is [8, 8]. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Constrained bi-objective problem\" , inputs = [ Continuous ( \"x1\" , [ 0 , 10 ]), Continuous ( \"x2\" , [ - 10 , 10 ])], outputs = [ Continuous ( \"y1\" ), Continuous ( \"y2\" )], constraints = [ NonlinearInequality ( \"x2 - x1**2\" ), NonlinearInequality ( \"2 - x1 - x2\" ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y1\" : X . eval ( \"(x1 - 2)**2 + (x2 - 1)**2\" ), \"y2\" : X . eval ( \"x1**2 + (x2 - 3)**2\" ), } )","title":"Qapi1"},{"location":"ref-problems/#opti.problems.multi.WeldedBeam","text":"Design optimization of a welded beam. This is a bi-objective problem with 4 inputs and 3 (non-)linear inequality constraints. The two objectives are the fabrication cost of the beam and the deflection of the end of the beam under the applied load P. The load P is fixed at 6000 lbs, and the distance L is fixed at 14 inch. Note that for simplicity the constraint shear stress < 13600 psi is not included. See https://www.mathworks.com/help/gads/multiobjective-optimization-welded-beam.html Source code in opti/problems/multi.py class WeldedBeam ( Problem ): \"\"\"Design optimization of a welded beam. This is a bi-objective problem with 4 inputs and 3 (non-)linear inequality constraints. The two objectives are the fabrication cost of the beam and the deflection of the end of the beam under the applied load P. The load P is fixed at 6000 lbs, and the distance L is fixed at 14 inch. Note that for simplicity the constraint shear stress < 13600 psi is not included. See https://www.mathworks.com/help/gads/multiobjective-optimization-welded-beam.html \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Welded beam problem\" , inputs = [ Continuous ( \"h\" , [ 0.125 , 5 ]), # thickness of welds Continuous ( \"l\" , [ 0.1 , 10 ]), # length of welds Continuous ( \"t\" , [ 0.1 , 10 ]), # height of beam Continuous ( \"b\" , [ 0.125 , 5 ]), # width of beam ], outputs = [ Continuous ( \"cost\" ), Continuous ( \"deflection\" )], constraints = [ # h <= b, weld thickness cannot exceed beam width LinearInequality ([ \"h\" , \"b\" ], lhs = [ 1 , - 1 ], rhs = 0 ), # normal stress on the welds cannot exceed 30000 psi NonlinearInequality ( \"6000 * 6 * 14 / b / t**3 - 30000\" ), # buckling load capacity must exceed 6000 lbs NonlinearInequality ( \"6000 - 60746.022 * (1 - 0.0282346 * t) * t * b**4\" ), ], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 , x2 , x3 , x4 = self . get_X ( X ) . T return pd . DataFrame ( { \"cost\" : 1.10471 * x1 ** 2 * x2 + 0.04811 * x3 * x4 * ( 14 + x2 ), \"deflection\" : 2.1952 / ( x4 * x3 ** 3 ), }, index = X . index , )","title":"WeldedBeam"},{"location":"ref-problems/#opti.problems.single","text":"Single objective benchmark problems.","title":"single"},{"location":"ref-problems/#opti.problems.single.Ackley","text":"Ackley benchmark problem. Source code in opti/problems/single.py class Ackley ( Problem ): \"\"\"Ackley benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Ackley problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 32.768 , + 32.768 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : a = 20 b = 1 / 5 c = 2 * np . pi n = self . n_inputs x = self . get_X ( X ) part1 = - a * np . exp ( - b * np . sqrt (( 1 / n ) * np . sum ( x ** 2 , axis =- 1 ))) part2 = - np . exp (( 1 / n ) * np . sum ( np . cos ( c * x ), axis =- 1 )) y = part1 + part2 + a + np . exp ( 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Ackley"},{"location":"ref-problems/#opti.problems.single.Branin","text":"The Branin (Branin-Hoo) benchmark problem. f(x) = a(x2 - b x1^2 + cx1 - r)^2 + s(1 - t) cos(x1) + s a = 1, b = 5.1 / (4 pi^2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8pi) It has 3 global optima. Source code in opti/problems/single.py class Branin ( Problem ): \"\"\"The Branin (Branin-Hoo) benchmark problem. f(x) = a(x2 - b x1^2 + cx1 - r)^2 + s(1 - t) cos(x1) + s a = 1, b = 5.1 / (4 pi^2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8pi) It has 3 global optima. \"\"\" def __init__ ( self ): super () . __init__ ( name = \"Branin function\" , inputs = [ Continuous ( \"x1\" , [ - 5 , 10 ]), Continuous ( \"x2\" , [ 0 , 15 ])], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 , x2 = self . get_X ( X ) . T y = ( ( x2 - 5.1 / ( 4 * np . pi ** 2 ) * x1 ** 2 + 5 / np . pi * x1 - 6 ) ** 2 + 10 * ( 1 - 1 / ( 8 * np . pi )) * np . cos ( x1 ) + 10 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : return pd . DataFrame ( [ [ - np . pi , 12.275 , 0.397887 ], [ np . pi , 2.275 , 0.397887 ], [ 9.42478 , 2.475 , 0.397887 ], ], columns = self . inputs . names + self . outputs . names , )","title":"Branin"},{"location":"ref-problems/#opti.problems.single.Himmelblau","text":"Himmelblau benchmark problem Source code in opti/problems/single.py class Himmelblau ( Problem ): \"\"\"Himmelblau benchmark problem\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Himmelblau function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 6 , 6 ]) for i in range ( 2 )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x0 , x1 = self . get_X ( X ) . T y = ( x0 ** 2 + x1 - 11 ) ** 2 + ( x0 + x1 ** 2 - 7 ) ** 2 return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . array ( [ [ 3.0 , 2.0 ], [ - 2.805118 , 3.131312 ], [ - 3.779310 , - 3.283186 ], [ 3.584428 , - 1.848126 ], ] ) y = np . zeros ( 4 ) return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Himmelblau"},{"location":"ref-problems/#opti.problems.single.Michalewicz","text":"Michalewicz benchmark problem. The Michalewicz function has d! local minima, and it is multimodal. The parameter m (m=10 is used here) defines the steepness of they valleys and a larger m leads to a more difficult search. Source code in opti/problems/single.py class Michalewicz ( Problem ): \"\"\"Michalewicz benchmark problem. The Michalewicz function has d! local minima, and it is multimodal. The parameter m (m=10 is used here) defines the steepness of they valleys and a larger m leads to a more difficult search. \"\"\" def __init__ ( self , n_inputs : int = 2 ): super () . __init__ ( name = \"Michalewicz function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , np . pi ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) m = 10 i = np . arange ( 1 , self . n_inputs + 1 ) y = - np . sum ( np . sin ( x ) * np . sin ( i * x ** 2 / np . pi ) ** ( 2 * m ), axis = 1 ) return pd . DataFrame ({ \"y\" : y }, index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = pd . DataFrame ([[ 2.2 , 1.57 ]], columns = self . inputs . names ) return pd . concat ([ x , self . f ( x )], axis = 1 )","title":"Michalewicz"},{"location":"ref-problems/#opti.problems.single.Rastrigin","text":"Rastrigin benchmark problem. Source code in opti/problems/single.py class Rastrigin ( Problem ): \"\"\"Rastrigin benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Rastrigin function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 5 , 5 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) a = 10 y = a * self . n_inputs + np . sum ( x ** 2 - a * np . cos ( 2 * np . pi * x ), axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Rastrigin"},{"location":"ref-problems/#opti.problems.single.Rosenbrock","text":"Rosenbrock benchmark problem. Source code in opti/problems/single.py class Rosenbrock ( Problem ): \"\"\"Rosenbrock benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Rosenbrock function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 2.048 , 2.048 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) . T y = np . sum ( 100 * ( x [ 1 :] - x [: - 1 ] ** 2 ) ** 2 + ( 1 - x [: - 1 ]) ** 2 , axis = 0 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . ones (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Rosenbrock"},{"location":"ref-problems/#opti.problems.single.Schwefel","text":"Schwefel benchmark problem Source code in opti/problems/single.py class Schwefel ( Problem ): \"\"\"Schwefel benchmark problem\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Schwefel function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 500 , 500 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) y = 418.9829 * self . n_inputs - np . sum ( x * np . sin ( np . abs ( x ) ** 0.5 ), axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . full (( 1 , self . n_inputs ), 420.9687 ) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Schwefel"},{"location":"ref-problems/#opti.problems.single.Sphere","text":"Sphere benchmark problem. Source code in opti/problems/single.py class Sphere ( Problem ): \"\"\"Sphere benchmark problem.\"\"\" def __init__ ( self , n_inputs = 10 ): super () . __init__ ( name = \"Sphere function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" , [ 0 , 2 ])], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = self . get_X ( X ) y = np . sum (( x - 0.5 ) ** 2 , axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . full (( 1 , self . n_inputs ), 0.5 ) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Sphere"},{"location":"ref-problems/#opti.problems.single.ThreeHumpCamel","text":"Three-hump camel benchmark problem. Source code in opti/problems/single.py class ThreeHumpCamel ( Problem ): \"\"\"Three-hump camel benchmark problem.\"\"\" def __init__ ( self ): super () . __init__ ( name = \"Three-hump camel function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 5 , 5 ]) for i in range ( 2 )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x1 , x2 = self . get_X ( X ) . T y = 2 * x1 ** 2 - 1.05 * x1 ** 4 + x1 ** 6 / 6 + x1 * x2 + x2 ** 2 return pd . DataFrame ( y , columns = [ \"y\" ], index = X . index ) def get_optima ( self ) -> pd . DataFrame : return pd . DataFrame ( np . zeros (( 1 , 3 )), columns = self . inputs . names + self . outputs . names )","title":"ThreeHumpCamel"},{"location":"ref-problems/#opti.problems.single.Zakharov","text":"Zakharov benchmark problem. Source code in opti/problems/single.py class Zakharov ( Problem ): \"\"\"Zakharov benchmark problem.\"\"\" def __init__ ( self , n_inputs = 2 ): super () . __init__ ( name = \"Zakharov function\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ - 10 , 10 ]) for i in range ( n_inputs )], outputs = [ Continuous ( \"y\" )], ) def f ( self , X : pd . DataFrame ): x = self . get_X ( X ) a = 0.5 * np . sum ( np . arange ( 1 , self . n_inputs + 1 ) * x , axis = 1 ) y = np . sum ( x ** 2 , axis = 1 ) + a ** 2 + a ** 4 return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = np . zeros (( 1 , self . n_inputs )) y = 0 return pd . DataFrame ( np . c_ [ x , y ], columns = self . inputs . names + self . outputs . names )","title":"Zakharov"},{"location":"ref-problems/#opti.problems.single.Zakharov_Categorical","text":"Zakharov problem with one categorical input Source code in opti/problems/single.py class Zakharov_Categorical ( Problem ): \"\"\"Zakharov problem with one categorical input\"\"\" def __init__ ( self , n_inputs = 3 ): base = Zakharov ( n_inputs ) super () . __init__ ( name = \"Zakharov function with one categorical input\" , inputs = [ Continuous ( f \"x { i } \" , [ - 10 , 10 ]) for i in range ( n_inputs - 1 )] + [ Categorical ( \"expon_switch\" , [ \"one\" , \"two\" ])], outputs = base . outputs , ) def f ( self , X : pd . DataFrame ): x_conti = X [ self . inputs . names [: - 1 ]] . values # just the continuous inputs a = 0.5 * np . sum ( np . arange ( 1 , self . n_inputs ) * x_conti , axis = 1 ) powers = np . repeat ( np . expand_dims ([ 2.0 , 2.0 , 4.0 ], 0 ), repeats = len ( X ), axis = 0 ) modify_powers = X [ self . inputs . names [ - 1 ]] == \"two\" powers [ modify_powers , :] += powers [ modify_powers , :] res = ( np . sum ( x_conti ** np . expand_dims ( powers [:, 0 ], 1 ), axis = 1 ) + a ** np . expand_dims ( powers [:, 1 ], 0 ) + a ** np . expand_dims ( powers [:, 2 ], 0 ) ) res_float_array = np . array ( res , dtype = np . float64 ) . ravel () y = res_float_array return pd . DataFrame ( y , columns = self . outputs . names , index = X . index ) def get_optima ( self ) -> pd . DataFrame : x = list ( np . zeros ( self . n_inputs - 1 )) + [ \"one\" ] y = [ 0 ] return pd . DataFrame ([ x + y ], columns = self . inputs . names + self . outputs . names )","title":"Zakharov_Categorical"},{"location":"ref-problems/#opti.problems.single.Zakharov_Constrained","text":"Zakharov problem with one linear constraint Source code in opti/problems/single.py class Zakharov_Constrained ( Problem ): \"\"\"Zakharov problem with one linear constraint\"\"\" def __init__ ( self , n_inputs = 5 ): base = Zakharov ( n_inputs ) self . base = base super () . __init__ ( name = \"Zakharov with one linear constraint\" , inputs = base . inputs , outputs = base . outputs , constraints = [ LinearInequality ( base . inputs . names , lhs = 1 , rhs = 10 )], f = base . f , ) def get_optima ( self ): return self . base . get_optima ()","title":"Zakharov_Constrained"},{"location":"ref-problems/#opti.problems.single.Zakharov_NChooseKConstraint","text":"Zakharov problem with an n-choose-k constraint Source code in opti/problems/single.py class Zakharov_NChooseKConstraint ( Problem ): \"\"\"Zakharov problem with an n-choose-k constraint\"\"\" def __init__ ( self , n_inputs = 5 , n_max_active = 3 ): base = Zakharov ( n_inputs ) self . base = base super () . __init__ ( name = \"Zakharov with n-choose-k constraint\" , inputs = base . inputs , outputs = base . outputs , constraints = [ NChooseK ( names = base . inputs . names , max_active = n_max_active )], f = base . f , ) def get_optima ( self ): return self . base . get_optima ()","title":"Zakharov_NChooseKConstraint"},{"location":"ref-problems/#opti.problems.univariate","text":"Simple 1D problems for assessing probabilistic surrogate models. Note: these problems should be output-noisified, e.g. import opti problem = opti.problems.noisify_problem_with_gaussian( opti.problems.Line1D(), sigma=0.1 )","title":"univariate"},{"location":"ref-problems/#opti.problems.univariate.Line1D","text":"A line. Source code in opti/problems/univariate.py class Line1D ( Problem ): \"\"\"A line.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y\" : X . eval ( \"0.1 * x + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Line1D"},{"location":"ref-problems/#opti.problems.univariate.Parabola1D","text":"A parabola. Source code in opti/problems/univariate.py class Parabola1D ( Problem ): \"\"\"A parabola.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"0.025 * (x - 5) ** 2 + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Parabola1D"},{"location":"ref-problems/#opti.problems.univariate.Sigmoid1D","text":"A smooth step at x=5. Source code in opti/problems/univariate.py class Sigmoid1D ( Problem ): \"\"\"A smooth step at x=5.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"1 / (1 + exp(-2 * (x - 5))) + 1\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Sigmoid1D"},{"location":"ref-problems/#opti.problems.univariate.Sinus1D","text":"A sinus-function with one full period over the domain. Source code in opti/problems/univariate.py class Sinus1D ( Problem ): \"\"\"A sinus-function with one full period over the domain.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ( { \"y\" : X . eval ( \"sin(x * 2 * 3.14159 / 10) / 2 + 2\" )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Continuous ( \"y\" , [ 0 , 3 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Sinus1D"},{"location":"ref-problems/#opti.problems.univariate.Step1D","text":"A discrete step at x=1.1. Source code in opti/problems/univariate.py class Step1D ( Problem ): \"\"\"A discrete step at x=1.1.\"\"\" def __init__ ( self ): def f ( X : pd . DataFrame ) -> pd . DataFrame : return pd . DataFrame ({ \"y\" : X . eval ( \"x > 1.1\" ) . astype ( float )}, index = X . index ) super () . __init__ ( inputs = [ Continuous ( \"x\" , [ 0 , 10 ])], outputs = [ Discrete ( \"y\" , [ 0 , 1 ])], f = f , data = pd . concat ([ _X , f ( _X )], axis = 1 ), )","title":"Step1D"},{"location":"ref-problems/#opti.problems.zdt","text":"ZDT benchmark problem suite. All problems are bi-objective, have D continuous inputs and are unconstrained. Zitzler, Deb, Thiele 2000 - Comparison of Multiobjective Evolutionary Algorithms: Empirical Results http://dx.doi.org/10.1162/106365600568202","title":"zdt"},{"location":"ref-problems/#opti.problems.zdt.ZDT1","text":"ZDT-1 benchmark problem. Source code in opti/problems/zdt.py class ZDT1 ( Problem ): \"\"\"ZDT-1 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-1 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 0.5 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . sqrt ( x )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT1"},{"location":"ref-problems/#opti.problems.zdt.ZDT2","text":"ZDT-2 benchmark problem. Source code in opti/problems/zdt.py class ZDT2 ( Problem ): \"\"\"ZDT-2 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-2 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . power ( x , 2 )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT2"},{"location":"ref-problems/#opti.problems.zdt.ZDT3","text":"ZDT-3 benchmark problem. Source code in opti/problems/zdt.py class ZDT3 ( Problem ): \"\"\"ZDT-3 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-3 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - np . inf , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names [ 1 :]] . to_numpy () g = 1 + 9 / ( self . n_inputs - 1 ) * np . sum ( x , axis = 1 ) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - ( y1 / g ) ** 0.5 - ( y1 / g ) * np . sin ( 10 * np . pi * y1 )) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): regions = [ [ 0 , 0.0830015349 ], [ 0.182228780 , 0.2577623634 ], [ 0.4093136748 , 0.4538821041 ], [ 0.6183967944 , 0.6525117038 ], [ 0.8233317983 , 0.8518328654 ], ] pf = [] for r in regions : x1 = np . linspace ( r [ 0 ], r [ 1 ], int ( points / len ( regions ))) x2 = 1 - np . sqrt ( x1 ) - x1 * np . sin ( 10 * np . pi * x1 ) pf . append ( np . stack ([ x1 , x2 ], axis = 1 )) y = np . concatenate ( pf , axis = 0 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT3"},{"location":"ref-problems/#opti.problems.zdt.ZDT4","text":"ZDT-4 benchmark problem. Source code in opti/problems/zdt.py class ZDT4 ( Problem ): \"\"\"ZDT-4 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 10 ): super () . __init__ ( name = \"ZDT-4 problem\" , inputs = [ Continuous ( \"x1\" , [ 0 , 1 ])] + [ Continuous ( f \"x { i + 1 } \" , [ - 5 , 5 ]) for i in range ( 1 , n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ 0 , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . to_numpy () g = 1 + 10 * ( self . n_inputs - 1 ) for i in range ( 1 , self . n_inputs ): g += x [:, i ] ** 2 - 10 * np . cos ( 4.0 * np . pi * x [:, i ]) y1 = X [ \"x1\" ] . to_numpy () y2 = g * ( 1 - np . sqrt ( y1 / g )) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0 , 1 , points ) y = np . stack ([ x , 1 - np . sqrt ( x )], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT4"},{"location":"ref-problems/#opti.problems.zdt.ZDT6","text":"ZDT-6 benchmark problem. Source code in opti/problems/zdt.py class ZDT6 ( Problem ): \"\"\"ZDT-6 benchmark problem.\"\"\" def __init__ ( self , n_inputs = 30 ): super () . __init__ ( name = \"ZDT-6 problem\" , inputs = [ Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( n_inputs )], outputs = [ Continuous ( f \"y { i + 1 } \" , [ - np . inf , np . inf ]) for i in range ( 2 )], ) def f ( self , X : pd . DataFrame ) -> pd . DataFrame : x = X [ self . inputs . names ] . to_numpy () n = self . n_inputs g = 1 + 9 * ( np . sum ( x [:, 1 :], axis = 1 ) / ( n - 1 )) ** 0.25 y1 = 1 - np . exp ( - 4 * x [:, 0 ]) * ( np . sin ( 6 * np . pi * x [:, 0 ])) ** 6 y2 = g * ( 1 - ( y1 / g ) ** 2 ) return pd . DataFrame ({ \"y1\" : y1 , \"y2\" : y2 }, index = X . index ) def get_optima ( self , points = 100 ): x = np . linspace ( 0.2807753191 , 1 , points ) y = np . stack ([ x , 1 - x ** 2 ], axis = 1 ) return pd . DataFrame ( y , columns = self . outputs . names )","title":"ZDT6"},{"location":"ref-sampling/","text":"Sampling base apply_nchoosek ( samples , constraint ) Apply an n-choose-k constraint in-place Source code in opti/sampling/base.py def apply_nchoosek ( samples : pd . DataFrame , constraint : NChooseK ): \"\"\"Apply an n-choose-k constraint in-place\"\"\" n_zeros = len ( constraint . names ) - constraint . max_active for i in samples . index : s = np . random . choice ( constraint . names , size = n_zeros , replace = False ) samples . loc [ i , s ] = 0 constrained_sampling ( n_samples , parameters , constraints ) Uniform sampling from a constrained space. Source code in opti/sampling/base.py def constrained_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints ) -> pd . DataFrame : \"\"\"Uniform sampling from a constrained space.\"\"\" nchoosek_constraints , other_constraints = split_nchoosek ( constraints ) try : samples = rejection_sampling ( n_samples , parameters , other_constraints ) except Exception : samples = polytope_sampling ( n_samples , parameters , other_constraints ) if len ( nchoosek_constraints ) == 0 : return samples for c in nchoosek_constraints : apply_nchoosek ( samples , c ) # check if other constraints are still satisfied if not constraints . satisfied ( samples ) . all (): raise Exception ( \"Applying the n-choose-k constraint(s) violated another constraint.\" ) return samples rejection_sampling ( n_samples , parameters , constraints , max_iters = 1000 ) Uniformly distributed samples from a constrained space via rejection sampling. Source code in opti/sampling/base.py def rejection_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , max_iters : int = 1000 , ) -> pd . DataFrame : \"\"\"Uniformly distributed samples from a constrained space via rejection sampling.\"\"\" if constraints is None : return parameters . sample ( n_samples ) # check for equality constraints in combination with continuous parameters for c in constraints : if isinstance ( c , ( LinearEquality , NonlinearEquality )): for p in parameters : if isinstance ( p , Continuous ): raise Exception ( \"Rejection sampling doesn't work for equality constraints over continuous variables.\" ) n_iters = 0 n_found = 0 points_found = [] while n_found < n_samples : n_iters += 1 if n_iters > max_iters : raise Exception ( \"Maximum iterations exceeded in rejection sampling\" ) points = parameters . sample ( 10000 ) valid = constraints . satisfied ( points ) n_found += np . sum ( valid ) points_found . append ( points [ valid ]) return pd . concat ( points_found , ignore_index = True ) . iloc [: n_samples ] sobol_sampling ( n_samples , parameters ) Super-uniform sampling from an unconstrained space using a Sobol sequence. Source code in opti/sampling/base.py def sobol_sampling ( n_samples : int , parameters : Parameters ) -> pd . DataFrame : \"\"\"Super-uniform sampling from an unconstrained space using a Sobol sequence.\"\"\" d = len ( parameters ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( d ) . random ( n_samples ) res = [] for i , p in enumerate ( parameters ): if isinstance ( p , Continuous ): x = p . from_unit_range ( X [:, i ]) else : bins = np . linspace ( 0 , 1 , len ( p . domain ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( p . domain )[ idx ] res . append ( pd . Series ( x , name = p . name )) return pd . concat ( res , axis = 1 ) split_nchoosek ( constraints ) Split constraints in n-choose-k constraint and all other constraints. Source code in opti/sampling/base.py def split_nchoosek ( constraints : Optional [ Constraints ], ) -> Tuple [ Constraints , Constraints ]: \"\"\"Split constraints in n-choose-k constraint and all other constraints.\"\"\" if constraints is None : return Constraints ([]), Constraints ([]) nchoosek_constraints = Constraints ( [ c for c in constraints if isinstance ( c , NChooseK )] ) other_constraints = Constraints ( [ c for c in constraints if not isinstance ( c , NChooseK )] ) return nchoosek_constraints , other_constraints polytope This module provides functions to uniformly sample points subject to a system of linear inequality constraints, :math: Ax <= b (convex polytope), and linear equality constraints, :math: Ax = b (affine projection). A comparison of MCMC algorithms to generate uniform samples over a convex polytope is given in [Chen2018] . Here, we use the Hit & Run algorithm described in [Smith1984] . The R-package hitandrun _ provides similar functionality to this module. References .. [Chen2018] Chen Y., Dwivedi, R., Wainwright, M., Yu B. (2018) Fast MCMC Sampling Algorithms on Polytopes. JMLR, 19(55):1\u221286 https://arxiv.org/abs/1710.08165 .. [Smith1984] Smith, R. (1984). Efficient Monte Carlo Procedures for Generating Points Uniformly Distributed Over Bounded Regions. Operations Research, 32(6), 1296-1308. www.jstor.org/stable/170949 .. _ hitandrun : https://cran.r-project.org/web/packages/hitandrun/index.html polytope_sampling ( n_samples , parameters , constraints , thin = 100 ) Hit-and-run method to sample uniformly under linear constraints. Parameters: Name Type Description Default n_samples int Number of samples. required parameters opti.Parameters Parameter space. required constraints opti.Constraints Constraints on the parameters. required thin int Thinning factor of the generated samples. 100 Returns: Type Description array, shape=(n_samples, dimension) Randomly sampled points. Source code in opti/sampling/polytope.py def polytope_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , thin : int = 100 ) -> pd . DataFrame : \"\"\"Hit-and-run method to sample uniformly under linear constraints. Args: n_samples (int): Number of samples. parameters (opti.Parameters): Parameter space. constraints (opti.Constraints): Constraints on the parameters. thin (int, optional): Thinning factor of the generated samples. Returns: array, shape=(n_samples, dimension): Randomly sampled points. \"\"\" for c in constraints : if not isinstance ( c , ( LinearEquality , LinearInequality )): raise Exception ( \"Polytope sampling only works for linear constraints.\" ) At , bt , N , xp = _get_AbNx ( parameters , constraints ) # hit & run sampling x0 = _chebyshev_center ( At , bt ) sampler = _hitandrun ( At , bt , x0 ) X = np . empty (( n_samples , At . shape [ 1 ])) for i in range ( n_samples ): for _ in range ( thin - 1 ): next ( sampler ) X [ i ] = next ( sampler ) # project back X = X @ N . T + xp return pd . DataFrame ( columns = parameters . names , data = X ) simplex grid ( dimension , levels ) Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Parameters: Name Type Description Default dimension int Number of variables. required levels int Number of levels for each variable. required Returns: Type Description array Regularily spaced points on the unit simplex. Examples: >>> simplex_grid ( 3 , 3 ) array ([ [ 0. , 0. , 1. ], [ 0. , 0.5 , 0.5 ], [ 0. , 1. , 0. ], [ 0.5 , 0. , 0.5 ], [ 0.5 , 0.5 , 0. ], [ 1. , 0. , 0. ] ]) References Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. Source code in opti/sampling/simplex.py def grid ( dimension : int , levels : int ) -> np . ndarray : \"\"\"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Args: dimension (int): Number of variables. levels (int): Number of levels for each variable. Returns: array: Regularily spaced points on the unit simplex. Examples: >>> simplex_grid(3, 3) array([ [0. , 0. , 1. ], [0. , 0.5, 0.5], [0. , 1. , 0. ], [0.5, 0. , 0.5], [0.5, 0.5, 0. ], [1. , 0. , 0. ] ]) References: Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. \"\"\" m = dimension n = levels - 1 L = int ( comb ( dimension - 1 + levels - 1 , dimension - 1 , exact = True )) x = np . zeros ( m , dtype = int ) x [ - 1 ] = n out = np . empty (( L , m ), dtype = int ) out [ 0 ] = x h = m for i in range ( 1 , L ): h -= 1 val = x [ h ] x [ h ] = 0 x [ h - 1 ] += 1 x [ - 1 ] = val - 1 if val != 1 : h = m out [ i ] = x return out / n sample ( dimension , n_samples = 1 ) Sample uniformly from the unit simplex. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/simplex.py def sample ( dimension : int , n_samples : int = 1 ) -> np . ndarray : \"\"\"Sample uniformly from the unit simplex. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" s = np . random . standard_exponential (( n_samples , dimension )) return ( s . T / s . sum ( axis = 1 )) . T sphere sample ( dimension , n_samples = 1 , positive = False ) Sample uniformly from the unit hypersphere. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 positive bool Sample from the non-negative unit-sphere. False Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/sphere.py def sample ( dimension : int , n_samples : int = 1 , positive : bool = False ) -> np . ndarray : \"\"\"Sample uniformly from the unit hypersphere. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. positive (bool): Sample from the non-negative unit-sphere. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" x = np . random . normal ( 0 , 1 , size = ( n_samples , dimension )) x = x / np . sum ( x ** 2 , axis = 1 , keepdims = True ) ** 0.5 if positive : x *= - 2 * ( x < 0 ) + 1 return x","title":"Sampling"},{"location":"ref-sampling/#sampling","text":"","title":"Sampling"},{"location":"ref-sampling/#opti.sampling.base","text":"","title":"base"},{"location":"ref-sampling/#opti.sampling.base.apply_nchoosek","text":"Apply an n-choose-k constraint in-place Source code in opti/sampling/base.py def apply_nchoosek ( samples : pd . DataFrame , constraint : NChooseK ): \"\"\"Apply an n-choose-k constraint in-place\"\"\" n_zeros = len ( constraint . names ) - constraint . max_active for i in samples . index : s = np . random . choice ( constraint . names , size = n_zeros , replace = False ) samples . loc [ i , s ] = 0","title":"apply_nchoosek()"},{"location":"ref-sampling/#opti.sampling.base.constrained_sampling","text":"Uniform sampling from a constrained space. Source code in opti/sampling/base.py def constrained_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints ) -> pd . DataFrame : \"\"\"Uniform sampling from a constrained space.\"\"\" nchoosek_constraints , other_constraints = split_nchoosek ( constraints ) try : samples = rejection_sampling ( n_samples , parameters , other_constraints ) except Exception : samples = polytope_sampling ( n_samples , parameters , other_constraints ) if len ( nchoosek_constraints ) == 0 : return samples for c in nchoosek_constraints : apply_nchoosek ( samples , c ) # check if other constraints are still satisfied if not constraints . satisfied ( samples ) . all (): raise Exception ( \"Applying the n-choose-k constraint(s) violated another constraint.\" ) return samples","title":"constrained_sampling()"},{"location":"ref-sampling/#opti.sampling.base.rejection_sampling","text":"Uniformly distributed samples from a constrained space via rejection sampling. Source code in opti/sampling/base.py def rejection_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , max_iters : int = 1000 , ) -> pd . DataFrame : \"\"\"Uniformly distributed samples from a constrained space via rejection sampling.\"\"\" if constraints is None : return parameters . sample ( n_samples ) # check for equality constraints in combination with continuous parameters for c in constraints : if isinstance ( c , ( LinearEquality , NonlinearEquality )): for p in parameters : if isinstance ( p , Continuous ): raise Exception ( \"Rejection sampling doesn't work for equality constraints over continuous variables.\" ) n_iters = 0 n_found = 0 points_found = [] while n_found < n_samples : n_iters += 1 if n_iters > max_iters : raise Exception ( \"Maximum iterations exceeded in rejection sampling\" ) points = parameters . sample ( 10000 ) valid = constraints . satisfied ( points ) n_found += np . sum ( valid ) points_found . append ( points [ valid ]) return pd . concat ( points_found , ignore_index = True ) . iloc [: n_samples ]","title":"rejection_sampling()"},{"location":"ref-sampling/#opti.sampling.base.sobol_sampling","text":"Super-uniform sampling from an unconstrained space using a Sobol sequence. Source code in opti/sampling/base.py def sobol_sampling ( n_samples : int , parameters : Parameters ) -> pd . DataFrame : \"\"\"Super-uniform sampling from an unconstrained space using a Sobol sequence.\"\"\" d = len ( parameters ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( d ) . random ( n_samples ) res = [] for i , p in enumerate ( parameters ): if isinstance ( p , Continuous ): x = p . from_unit_range ( X [:, i ]) else : bins = np . linspace ( 0 , 1 , len ( p . domain ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( p . domain )[ idx ] res . append ( pd . Series ( x , name = p . name )) return pd . concat ( res , axis = 1 )","title":"sobol_sampling()"},{"location":"ref-sampling/#opti.sampling.base.split_nchoosek","text":"Split constraints in n-choose-k constraint and all other constraints. Source code in opti/sampling/base.py def split_nchoosek ( constraints : Optional [ Constraints ], ) -> Tuple [ Constraints , Constraints ]: \"\"\"Split constraints in n-choose-k constraint and all other constraints.\"\"\" if constraints is None : return Constraints ([]), Constraints ([]) nchoosek_constraints = Constraints ( [ c for c in constraints if isinstance ( c , NChooseK )] ) other_constraints = Constraints ( [ c for c in constraints if not isinstance ( c , NChooseK )] ) return nchoosek_constraints , other_constraints","title":"split_nchoosek()"},{"location":"ref-sampling/#opti.sampling.polytope","text":"This module provides functions to uniformly sample points subject to a system of linear inequality constraints, :math: Ax <= b (convex polytope), and linear equality constraints, :math: Ax = b (affine projection). A comparison of MCMC algorithms to generate uniform samples over a convex polytope is given in [Chen2018] . Here, we use the Hit & Run algorithm described in [Smith1984] . The R-package hitandrun _ provides similar functionality to this module.","title":"polytope"},{"location":"ref-sampling/#opti.sampling.polytope--references","text":".. [Chen2018] Chen Y., Dwivedi, R., Wainwright, M., Yu B. (2018) Fast MCMC Sampling Algorithms on Polytopes. JMLR, 19(55):1\u221286 https://arxiv.org/abs/1710.08165 .. [Smith1984] Smith, R. (1984). Efficient Monte Carlo Procedures for Generating Points Uniformly Distributed Over Bounded Regions. Operations Research, 32(6), 1296-1308. www.jstor.org/stable/170949 .. _ hitandrun : https://cran.r-project.org/web/packages/hitandrun/index.html","title":"References"},{"location":"ref-sampling/#opti.sampling.polytope.polytope_sampling","text":"Hit-and-run method to sample uniformly under linear constraints. Parameters: Name Type Description Default n_samples int Number of samples. required parameters opti.Parameters Parameter space. required constraints opti.Constraints Constraints on the parameters. required thin int Thinning factor of the generated samples. 100 Returns: Type Description array, shape=(n_samples, dimension) Randomly sampled points. Source code in opti/sampling/polytope.py def polytope_sampling ( n_samples : int , parameters : Parameters , constraints : Constraints , thin : int = 100 ) -> pd . DataFrame : \"\"\"Hit-and-run method to sample uniformly under linear constraints. Args: n_samples (int): Number of samples. parameters (opti.Parameters): Parameter space. constraints (opti.Constraints): Constraints on the parameters. thin (int, optional): Thinning factor of the generated samples. Returns: array, shape=(n_samples, dimension): Randomly sampled points. \"\"\" for c in constraints : if not isinstance ( c , ( LinearEquality , LinearInequality )): raise Exception ( \"Polytope sampling only works for linear constraints.\" ) At , bt , N , xp = _get_AbNx ( parameters , constraints ) # hit & run sampling x0 = _chebyshev_center ( At , bt ) sampler = _hitandrun ( At , bt , x0 ) X = np . empty (( n_samples , At . shape [ 1 ])) for i in range ( n_samples ): for _ in range ( thin - 1 ): next ( sampler ) X [ i ] = next ( sampler ) # project back X = X @ N . T + xp return pd . DataFrame ( columns = parameters . names , data = X )","title":"polytope_sampling()"},{"location":"ref-sampling/#opti.sampling.simplex","text":"","title":"simplex"},{"location":"ref-sampling/#opti.sampling.simplex.grid","text":"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Parameters: Name Type Description Default dimension int Number of variables. required levels int Number of levels for each variable. required Returns: Type Description array Regularily spaced points on the unit simplex. Examples: >>> simplex_grid ( 3 , 3 ) array ([ [ 0. , 0. , 1. ], [ 0. , 0.5 , 0.5 ], [ 0. , 1. , 0. ], [ 0.5 , 0. , 0.5 ], [ 0.5 , 0.5 , 0. ], [ 1. , 0. , 0. ] ]) References Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. Source code in opti/sampling/simplex.py def grid ( dimension : int , levels : int ) -> np . ndarray : \"\"\"Construct a regular grid on the unit simplex. The number of grid points is L = (n+m-1)!/(n!*(m-1)!) where m is the dimension and n+1 is the number of levels. Args: dimension (int): Number of variables. levels (int): Number of levels for each variable. Returns: array: Regularily spaced points on the unit simplex. Examples: >>> simplex_grid(3, 3) array([ [0. , 0. , 1. ], [0. , 0.5, 0.5], [0. , 1. , 0. ], [0.5, 0. , 0.5], [0.5, 0.5, 0. ], [1. , 0. , 0. ] ]) References: Nijenhuis and Wilf, Combinatorial Algorithms, Chapter 5, Academic Press, 1978. \"\"\" m = dimension n = levels - 1 L = int ( comb ( dimension - 1 + levels - 1 , dimension - 1 , exact = True )) x = np . zeros ( m , dtype = int ) x [ - 1 ] = n out = np . empty (( L , m ), dtype = int ) out [ 0 ] = x h = m for i in range ( 1 , L ): h -= 1 val = x [ h ] x [ h ] = 0 x [ h - 1 ] += 1 x [ - 1 ] = val - 1 if val != 1 : h = m out [ i ] = x return out / n","title":"grid()"},{"location":"ref-sampling/#opti.sampling.simplex.sample","text":"Sample uniformly from the unit simplex. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/simplex.py def sample ( dimension : int , n_samples : int = 1 ) -> np . ndarray : \"\"\"Sample uniformly from the unit simplex. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" s = np . random . standard_exponential (( n_samples , dimension )) return ( s . T / s . sum ( axis = 1 )) . T","title":"sample()"},{"location":"ref-sampling/#opti.sampling.sphere","text":"","title":"sphere"},{"location":"ref-sampling/#opti.sampling.sphere.sample","text":"Sample uniformly from the unit hypersphere. Parameters: Name Type Description Default dimension int Number of dimensions. required n_samples int Number of samples to draw. 1 positive bool Sample from the non-negative unit-sphere. False Returns: Type Description array, shape=(n_samples, dimesnion) Random samples from the unit simplex. Source code in opti/sampling/sphere.py def sample ( dimension : int , n_samples : int = 1 , positive : bool = False ) -> np . ndarray : \"\"\"Sample uniformly from the unit hypersphere. Args: dimension (int): Number of dimensions. n_samples (int): Number of samples to draw. positive (bool): Sample from the non-negative unit-sphere. Returns: array, shape=(n_samples, dimesnion): Random samples from the unit simplex. \"\"\" x = np . random . normal ( 0 , 1 , size = ( n_samples , dimension )) x = x / np . sum ( x ** 2 , axis = 1 , keepdims = True ) ** 0.5 if positive : x *= - 2 * ( x < 0 ) + 1 return x","title":"sample()"},{"location":"ref-tools/","text":"Tools modde MipFile File reader for MODDE investigation files (.mip) Source code in opti/tools/modde.py class MipFile : \"\"\"File reader for MODDE investigation files (.mip)\"\"\" def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data () def _get_experimental_data ( self ): \"\"\"Parse the experimental data. Returns: pd.DataFrame: dataframe of experimental data \"\"\" part = [ p for p in self . parts if p . startswith ( \"ExpNo\" )][ 0 ] return pd . read_csv ( io . StringIO ( part ), delimiter = \" \\t \" , index_col = \"ExpNo\" ) def _get_design_settings ( self ): \"\"\"Parse the design settings. Returns: dict of dicts: dictionary with 'Factors', 'Responses', 'Options' as well as the individual variables. \"\"\" part = [ p for p in self . parts if p . startswith ( \"[Status]\" )][ 0 ] settings = {} for line in part . split ( \" \\n \" ): if len ( line ) == 0 : continue if line . startswith ( \"[\" ): thing = line . strip ( \"[]\" ) settings [ thing ] = {} else : key , value = line . split ( \"=\" ) settings [ thing ][ key ] = value return settings __init__ ( self , filename ) special Read in a MODDE file. Parameters: Name Type Description Default filename str or path path to MODDE file required Source code in opti/tools/modde.py def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data () read_modde ( filepath ) Read a problem specification from a MODDE file. Parameters: Name Type Description Default filepath path-like path to MODDE .mip file required Returns: Type Description opti.Problem problem specification Source code in opti/tools/modde.py def read_modde ( filepath ): \"\"\"Read a problem specification from a MODDE file. Args: filepath (path-like): path to MODDE .mip file Returns: opti.Problem: problem specification \"\"\" mip = MipFile ( filepath ) inputs = [] outputs = [] constraints = [] formulation_parameters = [] # build input space for name , props in mip . factors . items (): if props [ \"Use\" ] == \"Uncontrolled\" : print ( f \"Uncontrolled factors not supported. Skipping { name } \" ) continue if props [ \"Type\" ] == \"Formulation\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) formulation_parameters . append ( name ) elif props [ \"Type\" ] == \"Quantitative\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Multilevel\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Discrete ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Qualitative\" : domain = props [ \"Settings\" ] . split ( \",\" ) inputs . append ( opti . Categorical ( name = name , domain = domain )) inputs = opti . Parameters ( inputs ) # build formulation constraint constraints . append ( opti . constraint . LinearEquality ( names = formulation_parameters , lhs = np . ones ( len ( formulation_parameters )), rhs = 1 , ) ) # build output space for name , props in mip . responses . items (): # check if data available that allows to infer the domain vmin = mip . data [ name ] . min () vmax = mip . data [ name ] . max () if np . isfinite ( vmin ) or np . isfinite ( vmax ) and vmin < vmax : domain = [ vmin , vmax ] else : domain = [ 0 , 1 ] dim = opti . Continuous ( name = name , domain = domain ) outputs . append ( dim ) outputs = opti . Parameters ( outputs ) # data data = mip . data . drop ( columns = [ \"ExpName\" , \"InOut\" ]) return opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , data = data ) noisify noisify_problem ( problem , noisifiers ) Creates a new problem that is based on the given one plus noise on the outputs. Parameters: Name Type Description Default problem Problem given problem where we will add noise required noisifiers List[Callable] list of functions that add noise to the outputs required Returns: new problem with noise on the output Source code in opti/tools/noisify.py def noisify_problem ( problem : Problem , noisifiers : List [ Callable ], ) -> Problem : \"\"\"Creates a new problem that is based on the given one plus noise on the outputs. Args: problem: given problem where we will add noise noisifiers: list of functions that add noise to the outputs Returns: new problem with noise on the output \"\"\" def noisy_f ( X ): return _add_noise_to_data ( problem . f ( X ), noisifiers , problem . outputs ) if problem . data is not None : data = problem . get_data () X = data [ problem . inputs . names ] Yn = _add_noise_to_data ( data [ problem . outputs . names ], noisifiers , problem . outputs ) data = pd . concat ([ X , Yn ], axis = 1 ) else : data = None return Problem ( inputs = problem . inputs , outputs = problem . outputs , objectives = problem . objectives , constraints = problem . constraints , output_constraints = problem . output_constraints , f = noisy_f , data = data , name = problem . name , ) noisify_problem_with_gaussian ( problem , mu = 0 , sigma = 0.05 ) Given an instance of a problem, this returns the problem with additive Gaussian noise Parameters: Name Type Description Default problem Problem problem instance where we add the noise required mu float mean of the Gaussian noise to be added 0 sigma float standard deviation of the Gaussian noise to be added 0.05 Returns: input problem with additive Gaussian noise Source code in opti/tools/noisify.py def noisify_problem_with_gaussian ( problem : Problem , mu : float = 0 , sigma : float = 0.05 ): \"\"\" Given an instance of a problem, this returns the problem with additive Gaussian noise Args: problem: problem instance where we add the noise mu: mean of the Gaussian noise to be added sigma: standard deviation of the Gaussian noise to be added Returns: input problem with additive Gaussian noise \"\"\" def noisify ( y ): rv = norm ( loc = mu , scale = sigma ) return y + rv . rvs ( len ( y )) return noisify_problem ( problem , noisifiers = [ noisify ] * len ( problem . outputs )) reduce AffineTransform Source code in opti/tools/reduce.py class AffineTransform : def __init__ ( self , equalities ): self . equalities = equalities def augment_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Restore eliminated parameters in a dataframe.\"\"\" data = data . copy () for name_lhs , names_rhs , coeffs in self . equalities : data [ name_lhs ] = coeffs [ - 1 ] for i , name in enumerate ( names_rhs ): data [ name_lhs ] += coeffs [ i ] * data [ name ] return data def drop_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Drop eliminated parameters from a DataFrame.\"\"\" drop = [] for name_lhs , _ , _ in self . equalities : if name_lhs in data . columns : drop . append ( name_lhs ) return data . drop ( columns = drop ) augment_data ( self , data ) Restore eliminated parameters in a dataframe. Source code in opti/tools/reduce.py def augment_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Restore eliminated parameters in a dataframe.\"\"\" data = data . copy () for name_lhs , names_rhs , coeffs in self . equalities : data [ name_lhs ] = coeffs [ - 1 ] for i , name in enumerate ( names_rhs ): data [ name_lhs ] += coeffs [ i ] * data [ name ] return data drop_data ( self , data ) Drop eliminated parameters from a DataFrame. Source code in opti/tools/reduce.py def drop_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Drop eliminated parameters from a DataFrame.\"\"\" drop = [] for name_lhs , _ , _ in self . equalities : if name_lhs in data . columns : drop . append ( name_lhs ) return data . drop ( columns = drop ) check_existence_of_solution ( A_aug ) Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem. Source code in opti/tools/reduce.py def check_existence_of_solution ( A_aug ): \"\"\"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.\"\"\" A = A_aug [:, : - 1 ] b = A_aug [:, - 1 ] len_inputs = np . shape ( A )[ 1 ] # catch special cases rk_A_aug = np . linalg . matrix_rank ( A_aug ) rk_A = np . linalg . matrix_rank ( A ) if rk_A == rk_A_aug : if rk_A < len_inputs : return # all good else : x = np . linalg . solve ( A , b ) raise Exception ( f \"There is a unique solution x for the linear equality constraints: x= { x } \" ) elif rk_A < rk_A_aug : raise Exception ( \"There is no solution fulfilling the linear equality constraints.\" ) check_problem_for_reduction ( problem ) Check if the reduction can be applied or if a trivial case is present. Source code in opti/tools/reduce.py def check_problem_for_reduction ( problem : Problem ) -> bool : \"\"\"Check if the reduction can be applied or if a trivial case is present.\"\"\" # are there any constraints? if problem . constraints is None : return False # are there any linear equality constraints? linear_equalities , _ = find_linear_equalities ( problem . constraints ) if len ( linear_equalities ) == 0 : return False # are there continuous inputs continuous_inputs , _ = find_continuous_inputs ( problem . inputs ) if len ( continuous_inputs ) == 0 : return False # check that equality constraints only contain continuous inputs for c in linear_equalities : for name in c . names : if name not in continuous_inputs . names : raise Exception ( f \"Linear equality constraint { c } contains a non-continuous parameter. Problem reduction is not supported.\" ) return True find_continuous_inputs ( inputs ) Separate parameters into continuous and all other parameters. Source code in opti/tools/reduce.py def find_continuous_inputs ( inputs : Parameters ) -> Tuple [ Parameters , Parameters ]: \"\"\"Separate parameters into continuous and all other parameters.\"\"\" continous_inputs = [ p for p in inputs if isinstance ( p , Continuous )] other_inputs = [ p for p in inputs if not isinstance ( p , Continuous )] return Parameters ( continous_inputs ), Parameters ( other_inputs ) find_linear_equalities ( constraints ) Separate constraints into linear equalities and all other constraints. Source code in opti/tools/reduce.py def find_linear_equalities ( constraints : Constraints ) -> Tuple [ Constraints , Constraints ]: \"\"\"Separate constraints into linear equalities and all other constraints.\"\"\" linear_equalities = [ c for c in constraints if isinstance ( c , LinearEquality )] other_constraints = [ c for c in constraints if not isinstance ( c , LinearEquality )] return Constraints ( linear_equalities ), Constraints ( other_constraints ) reduce_problem ( problem ) Reduce a problem with linear constraints to a subproblem where linear equality constraints are eliminated. Parameters: Name Type Description Default problem Problem problem to be reduced required Returns: Type Description Tuple[opti.problem.Problem, opti.tools.reduce.AffineTransform] (problem, trafo). Problem is the reduced problem where linear equality constraints have been eliminated. trafo is the according transformation. Source code in opti/tools/reduce.py def reduce_problem ( problem : Problem ) -> Tuple [ Problem , AffineTransform ]: \"\"\"Reduce a problem with linear constraints to a subproblem where linear equality constraints are eliminated. Args: problem (Problem): problem to be reduced Returns: (problem, trafo). Problem is the reduced problem where linear equality constraints have been eliminated. trafo is the according transformation. \"\"\" # check if the problem can be reduced if not check_problem_for_reduction ( problem ): return problem , AffineTransform ([]) # find linear equality constraints linear_equalities , other_constraints = find_linear_equalities ( problem . constraints ) # only consider continuous inputs continuous_inputs , other_inputs = find_continuous_inputs ( problem . inputs ) # assemble Matrix A from equality constraints N = len ( linear_equalities ) M = len ( continuous_inputs ) + 1 names = np . concatenate (( continuous_inputs . names , [ \"rhs\" ])) A_aug = pd . DataFrame ( data = np . zeros ( shape = ( N , M )), columns = names ) for i in range ( len ( linear_equalities )): c = linear_equalities [ i ] A_aug . loc [ i , c . names ] = c . lhs A_aug . loc [ i , \"rhs\" ] = c . rhs A_aug = A_aug . values # catch special cases check_existence_of_solution ( A_aug ) # bring A_aug to reduced row-echelon form A_aug_rref , pivots = rref ( A_aug ) pivots = np . array ( pivots ) A_aug_rref = np . array ( A_aug_rref ) . astype ( np . float64 ) # formulate box bounds as linear inequality constraints in matrix form B = np . zeros ( shape = ( 2 * ( M - 1 ), M )) B [: M - 1 , : M - 1 ] = np . eye ( M - 1 ) B [ M - 1 :, : M - 1 ] = - np . eye ( M - 1 ) B [: M - 1 , - 1 ] = continuous_inputs . bounds . loc [ \"max\" ] . copy () B [ M - 1 :, - 1 ] = - continuous_inputs . bounds . loc [ \"min\" ] . copy () # eliminate columns with pivot element for i in range ( len ( pivots )): p = pivots [ i ] B [ p , :] -= A_aug_rref [ i , :] B [ p + M - 1 , :] += A_aug_rref [ i , :] # build up reduced problem _inputs = list ( other_inputs . parameters . values ()) for i in range ( len ( continuous_inputs )): # add all inputs that were not eliminated if i not in pivots : _inputs . append ( continuous_inputs [ names [ i ]]) _inputs = Parameters ( _inputs ) _constraints = other_constraints . constraints for i in pivots : # reduce equation system of upper bounds ind = np . where ( B [ i , : - 1 ] != 0 )[ 0 ] if len ( ind ) > 0 and B [ i , - 1 ] < np . inf : c = LinearInequality ( names = list ( names [ ind ]), lhs = B [ i , ind ], rhs = B [ i , - 1 ]) _constraints . append ( c ) else : if B [ i , - 1 ] < - 1e-16 : raise Exception ( \"There is no solution that fulfills the constraints.\" ) # reduce equation system of lower bounds ind = np . where ( B [ i + M - 1 , : - 1 ] != 0 )[ 0 ] if len ( ind ) > 0 and B [ i + M - 1 , - 1 ] < np . inf : c = LinearInequality ( names = list ( names [ ind ]), lhs = B [ i + M - 1 , ind ], rhs = B [ i + M - 1 , - 1 ] ) _constraints . append ( c ) else : if B [ i + M - 1 , - 1 ] < - 1e-16 : raise Exception ( \"There is no solution that fulfills the constraints.\" ) _constraints = Constraints ( _constraints ) # assemble equalities _equalities = [] for i in range ( len ( pivots )): name_lhs = names [ pivots [ i ]] names_rhs = [] coeffs = [] for j in range ( len ( names ) - 1 ): if A_aug_rref [ i , j ] != 0 and j != pivots [ i ]: coeffs . append ( - A_aug_rref [ i , j ]) names_rhs . append ( names [ j ]) coeffs . append ( A_aug_rref [ i , - 1 ]) _equalities . append ([ name_lhs , names_rhs , coeffs ]) _data = problem . data trafo = AffineTransform ( _equalities ) _models = problem . models if _models is not None : warnings . warn ( \"Models are currently not adapted in reduce_problem.\" ) if hasattr ( problem , \"f\" ) and problem . f is not None : def _f ( X : pd . DataFrame ) -> pd . DataFrame : return problem . f ( trafo . augment_data ( X )) else : _f = None _problem = Problem ( inputs = _inputs , outputs = deepcopy ( problem . outputs ), objectives = deepcopy ( problem . objectives ), constraints = deepcopy ( _constraints ), f = _f , models = _models , data = _data , optima = deepcopy ( problem . optima ), name = deepcopy ( problem . name ), ) # remove remaining dependencies of eliminated inputs from the problem _problem = remove_eliminated_inputs ( _problem , trafo ) return _problem , trafo remove_eliminated_inputs ( problem , transform ) Eliminates remaining occurences of eliminated inputs in linear constraints. Source code in opti/tools/reduce.py def remove_eliminated_inputs ( problem : Problem , transform : AffineTransform ) -> Problem : \"\"\"Eliminates remaining occurences of eliminated inputs in linear constraints.\"\"\" inputs_names = problem . inputs . names M = len ( inputs_names ) # write the equalities for the backtransformation into one matrix inputs_dict = { inputs_names [ i ]: i for i in range ( M )} # build up dict from problem.equalities e.g. {\"xi1\": [coeff(xj1), ..., coeff(xjn)], ... \"xik\":...} coeffs_dict = {} for i , e in enumerate ( transform . equalities ): coeffs = np . zeros ( M + 1 ) for j , name in enumerate ( e [ 1 ]): coeffs [ inputs_dict [ name ]] = e [ 2 ][ j ] coeffs [ - 1 ] = e [ 2 ][ - 1 ] coeffs_dict [ e [ 0 ]] = coeffs constraints = [] for c in problem . constraints : # Nonlinear constraints supported if not isinstance ( c , ( LinearEquality , LinearInequality )): raise Exception ( \"Elimination of variables is only supported for LinearEquality and LinearInequality constraints.\" ) # no changes, if the constraint does not contain eliminated inputs elif all ( name in inputs_names for name in c . names ): constraints . append ( c ) # remove inputs from the constraint that were eliminated from the inputs before else : _names = np . array ( inputs_names ) _rhs = c . rhs # create new lhs and rhs from the old one and knowledge from problem._equalities _lhs = np . zeros ( M ) for j , name in enumerate ( c . names ): if name in inputs_names : _lhs [ inputs_dict [ name ]] += c . lhs [ j ] else : _lhs += c . lhs [ j ] * coeffs_dict [ name ][: - 1 ] _rhs -= c . lhs [ j ] * coeffs_dict [ name ][ - 1 ] _names = _names [ np . abs ( _lhs ) > 1e-16 ] _lhs = _lhs [ np . abs ( _lhs ) > 1e-16 ] # create new Constraints if isinstance ( c , LinearEquality ): _c = LinearEquality ( _names , _lhs , _rhs ) else : _c = LinearInequality ( _names , _lhs , _rhs ) # check if constraint is always fulfilled/not fulfilled if len ( _c . names ) == 0 and _c . rhs >= 0 : pass elif len ( _c . names ) == 0 and _c . rhs < 0 : raise Exception ( \"Linear constraints cannot be fulfilled.\" ) elif np . isinf ( _c . rhs ): pass else : constraints . append ( _c ) problem . constraints = Constraints ( constraints ) return problem rref ( A , tol = 1e-08 ) Computes the reduced row echelon form of a Matrix Parameters: Name Type Description Default A ndarray 2d array representing a matrix. required tol float tolerance for rounding to 0 1e-08 Returns: Type Description Tuple[numpy.ndarray, List[int]] (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref Source code in opti/tools/reduce.py def rref ( A : np . ndarray , tol = 1e-8 ) -> Tuple [ np . ndarray , List [ int ]]: \"\"\"Computes the reduced row echelon form of a Matrix Args: A (ndarray): 2d array representing a matrix. tol (float): tolerance for rounding to 0 Returns: (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref \"\"\" A = np . array ( A , dtype = np . float64 ) n , m = np . shape ( A ) col = 0 row = 0 pivots = [] for col in range ( m ): # does a pivot element exist? if all ( np . abs ( A [ row :, col ]) < tol ): pass # if yes: start elimination else : pivots . append ( col ) max_row = np . argmax ( np . abs ( A [ row :, col ])) + row # switch to most stable row A [[ row , max_row ], :] = A [[ max_row , row ], :] # normalize row A [ row , :] /= A [ row , col ] # eliminate other elements from column for r in range ( n ): if r != row : A [ r , :] -= A [ r , col ] / A [ row , col ] * A [ row , :] row += 1 prec = int ( - np . log10 ( tol )) return np . round ( A , prec ), pivots sanitize sanitize_problem ( problem ) This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives. More precisely, the resulting problem has the following properties: - Inputs are named input_0 , input_1 , .... Outputs are named analogously. - The data is scaled per feature to [0, 1] . - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of f are dropped if there are any. Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints. Parameters: Name Type Description Default problem Problem to be sanitized required Exceptions: Type Description TypeError in case there are unsupported constraints, data is None, or there are output constraints Returns: Type Description Problem Problem instance with sanitized labels and normalized data Source code in opti/tools/sanitize.py def sanitize_problem ( problem : Problem ) -> Problem : \"\"\" This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives. More precisely, the resulting problem has the following properties: - Inputs are named `input_0`, `input_1`, .... Outputs are named analogously. - The data is scaled per feature to `[0, 1]`. - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of `f` are dropped if there are any. Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints. Args: problem: to be sanitized Raises: TypeError: in case there are unsupported constraints, data is None, or there are output constraints Returns: Problem instance with sanitized labels and normalized data \"\"\" if problem . data is None : raise TypeError ( \"we cannot sanitize a problem without data\" ) if problem . output_constraints is not None : raise TypeError ( \"output constraints are currently not supported\" ) if getattr ( problem , \"f\" , None ) is not None : warnings . warn ( \"f is not sanitized but dropped\" ) if problem . models is not None : warnings . warn ( \"models are not sanitized but dropped\" ) inputs = _sanitize_params ( problem . inputs , \"input\" ) input_name_map = { pi . name : i . name for pi , i in zip ( problem . inputs , inputs )} normalized_in_data , xmin , \u0394x = _normalize_parameters_data ( problem . data , problem . inputs ) outputs = _sanitize_params ( problem . outputs , \"output\" ) output_name_map = { pi . name : i . name for pi , i in zip ( problem . outputs , outputs )} normalized_out_data , ymin , \u0394y = _normalize_parameters_data ( problem . data , problem . outputs ) normalized_in_data . columns = inputs . names normalized_out_data . columns = outputs . names normalized_data = pd . concat ([ normalized_in_data , normalized_out_data ], axis = 1 ) normalized_data . reset_index ( inplace = True , drop = True ) objectives = deepcopy ( problem . objectives ) for obj in objectives : sanitized_name = output_name_map [ obj . name ] i = outputs . names . index ( sanitized_name ) obj . name = sanitized_name obj . parameter = sanitized_name obj . target = ( obj . target - ymin [ i ]) / \u0394y [ i ] if hasattr ( obj , \"tolerance\" ): obj . tolerance /= \u0394y [ i ] constraints = deepcopy ( problem . constraints ) if constraints is not None : for c in constraints : c . names = [ input_name_map [ n ] for n in c . names ] if isinstance ( c , ( LinearEquality , LinearInequality )): c . lhs = ( c . lhs + xmin ) * \u0394x if c . rhs > 1e-5 : c . lhs = c . lhs / c . rhs c . rhs = 1.0 elif isinstance ( c , NChooseK ): pass else : raise TypeError ( \"sanitizer only supports linear and n-choose-k constraints\" ) normalized_problem = Problem ( inputs = inputs , outputs = outputs , objectives = objectives , constraints = constraints , data = normalized_data , ) return normalized_problem","title":"Tools"},{"location":"ref-tools/#tools","text":"","title":"Tools"},{"location":"ref-tools/#opti.tools.modde","text":"","title":"modde"},{"location":"ref-tools/#opti.tools.modde.MipFile","text":"File reader for MODDE investigation files (.mip) Source code in opti/tools/modde.py class MipFile : \"\"\"File reader for MODDE investigation files (.mip)\"\"\" def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data () def _get_experimental_data ( self ): \"\"\"Parse the experimental data. Returns: pd.DataFrame: dataframe of experimental data \"\"\" part = [ p for p in self . parts if p . startswith ( \"ExpNo\" )][ 0 ] return pd . read_csv ( io . StringIO ( part ), delimiter = \" \\t \" , index_col = \"ExpNo\" ) def _get_design_settings ( self ): \"\"\"Parse the design settings. Returns: dict of dicts: dictionary with 'Factors', 'Responses', 'Options' as well as the individual variables. \"\"\" part = [ p for p in self . parts if p . startswith ( \"[Status]\" )][ 0 ] settings = {} for line in part . split ( \" \\n \" ): if len ( line ) == 0 : continue if line . startswith ( \"[\" ): thing = line . strip ( \"[]\" ) settings [ thing ] = {} else : key , value = line . split ( \"=\" ) settings [ thing ][ key ] = value return settings","title":"MipFile"},{"location":"ref-tools/#opti.tools.modde.MipFile.__init__","text":"Read in a MODDE file. Parameters: Name Type Description Default filename str or path path to MODDE file required Source code in opti/tools/modde.py def __init__ ( self , filename ): \"\"\"Read in a MODDE file. Args: filename (str or path): path to MODDE file \"\"\" s = b \"\" . join ( open ( filename , \"rb\" ) . readlines ()) # Split into blocks of 1024 bytes num_blocks = len ( s ) // 1024 assert len ( s ) % 1024 == 0 blocks = [ s [ i * 1024 : ( i + 1 ) * 1024 ] for i in range ( num_blocks )] # Split blocks into header (first 21 bytes) and body headers = [ b [ 0 : 21 ] for b in blocks ] data = [ b [ 21 :] for b in blocks ] # Parse the block headers to create a mapping {block_index: next_block_index} block_order = {} for i , header in enumerate ( headers ): i_next = np . where ([ h . startswith ( header [ 15 : 19 ]) for h in headers ])[ 0 ] block_order [ i ] = i_next [ 0 ] if len ( i_next ) == 1 else None # Join all blocks that belong together and decode to UTF-8 self . parts = [] while len ( block_order ) > 0 : i = next ( iter ( block_order )) # get first key in ordered dict s = b \"\" while i is not None : s += data [ i ] i = block_order . pop ( i ) s = re . sub ( b \" \\r | \\x00 \" , b \"\" , s ) . decode ( \"utf-8\" , errors = \"ignore\" ) self . parts . append ( s ) self . settings = self . _get_design_settings () self . factors = { k : self . settings [ k ] for k in self . settings [ \"Factors\" ]} self . responses = { k : self . settings [ k ] for k in self . settings [ \"Responses\" ]} self . data = self . _get_experimental_data ()","title":"__init__()"},{"location":"ref-tools/#opti.tools.modde.read_modde","text":"Read a problem specification from a MODDE file. Parameters: Name Type Description Default filepath path-like path to MODDE .mip file required Returns: Type Description opti.Problem problem specification Source code in opti/tools/modde.py def read_modde ( filepath ): \"\"\"Read a problem specification from a MODDE file. Args: filepath (path-like): path to MODDE .mip file Returns: opti.Problem: problem specification \"\"\" mip = MipFile ( filepath ) inputs = [] outputs = [] constraints = [] formulation_parameters = [] # build input space for name , props in mip . factors . items (): if props [ \"Use\" ] == \"Uncontrolled\" : print ( f \"Uncontrolled factors not supported. Skipping { name } \" ) continue if props [ \"Type\" ] == \"Formulation\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) formulation_parameters . append ( name ) elif props [ \"Type\" ] == \"Quantitative\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Continuous ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Multilevel\" : domain = [ float ( s ) for s in props [ \"Settings\" ] . split ( \",\" )] inputs . append ( opti . Discrete ( name = name , domain = domain )) elif props [ \"Type\" ] == \"Qualitative\" : domain = props [ \"Settings\" ] . split ( \",\" ) inputs . append ( opti . Categorical ( name = name , domain = domain )) inputs = opti . Parameters ( inputs ) # build formulation constraint constraints . append ( opti . constraint . LinearEquality ( names = formulation_parameters , lhs = np . ones ( len ( formulation_parameters )), rhs = 1 , ) ) # build output space for name , props in mip . responses . items (): # check if data available that allows to infer the domain vmin = mip . data [ name ] . min () vmax = mip . data [ name ] . max () if np . isfinite ( vmin ) or np . isfinite ( vmax ) and vmin < vmax : domain = [ vmin , vmax ] else : domain = [ 0 , 1 ] dim = opti . Continuous ( name = name , domain = domain ) outputs . append ( dim ) outputs = opti . Parameters ( outputs ) # data data = mip . data . drop ( columns = [ \"ExpName\" , \"InOut\" ]) return opti . Problem ( inputs = inputs , outputs = outputs , constraints = constraints , data = data )","title":"read_modde()"},{"location":"ref-tools/#opti.tools.noisify","text":"","title":"noisify"},{"location":"ref-tools/#opti.tools.noisify.noisify_problem","text":"Creates a new problem that is based on the given one plus noise on the outputs. Parameters: Name Type Description Default problem Problem given problem where we will add noise required noisifiers List[Callable] list of functions that add noise to the outputs required Returns: new problem with noise on the output Source code in opti/tools/noisify.py def noisify_problem ( problem : Problem , noisifiers : List [ Callable ], ) -> Problem : \"\"\"Creates a new problem that is based on the given one plus noise on the outputs. Args: problem: given problem where we will add noise noisifiers: list of functions that add noise to the outputs Returns: new problem with noise on the output \"\"\" def noisy_f ( X ): return _add_noise_to_data ( problem . f ( X ), noisifiers , problem . outputs ) if problem . data is not None : data = problem . get_data () X = data [ problem . inputs . names ] Yn = _add_noise_to_data ( data [ problem . outputs . names ], noisifiers , problem . outputs ) data = pd . concat ([ X , Yn ], axis = 1 ) else : data = None return Problem ( inputs = problem . inputs , outputs = problem . outputs , objectives = problem . objectives , constraints = problem . constraints , output_constraints = problem . output_constraints , f = noisy_f , data = data , name = problem . name , )","title":"noisify_problem()"},{"location":"ref-tools/#opti.tools.noisify.noisify_problem_with_gaussian","text":"Given an instance of a problem, this returns the problem with additive Gaussian noise Parameters: Name Type Description Default problem Problem problem instance where we add the noise required mu float mean of the Gaussian noise to be added 0 sigma float standard deviation of the Gaussian noise to be added 0.05 Returns: input problem with additive Gaussian noise Source code in opti/tools/noisify.py def noisify_problem_with_gaussian ( problem : Problem , mu : float = 0 , sigma : float = 0.05 ): \"\"\" Given an instance of a problem, this returns the problem with additive Gaussian noise Args: problem: problem instance where we add the noise mu: mean of the Gaussian noise to be added sigma: standard deviation of the Gaussian noise to be added Returns: input problem with additive Gaussian noise \"\"\" def noisify ( y ): rv = norm ( loc = mu , scale = sigma ) return y + rv . rvs ( len ( y )) return noisify_problem ( problem , noisifiers = [ noisify ] * len ( problem . outputs ))","title":"noisify_problem_with_gaussian()"},{"location":"ref-tools/#opti.tools.reduce","text":"","title":"reduce"},{"location":"ref-tools/#opti.tools.reduce.AffineTransform","text":"Source code in opti/tools/reduce.py class AffineTransform : def __init__ ( self , equalities ): self . equalities = equalities def augment_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Restore eliminated parameters in a dataframe.\"\"\" data = data . copy () for name_lhs , names_rhs , coeffs in self . equalities : data [ name_lhs ] = coeffs [ - 1 ] for i , name in enumerate ( names_rhs ): data [ name_lhs ] += coeffs [ i ] * data [ name ] return data def drop_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Drop eliminated parameters from a DataFrame.\"\"\" drop = [] for name_lhs , _ , _ in self . equalities : if name_lhs in data . columns : drop . append ( name_lhs ) return data . drop ( columns = drop )","title":"AffineTransform"},{"location":"ref-tools/#opti.tools.reduce.AffineTransform.augment_data","text":"Restore eliminated parameters in a dataframe. Source code in opti/tools/reduce.py def augment_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Restore eliminated parameters in a dataframe.\"\"\" data = data . copy () for name_lhs , names_rhs , coeffs in self . equalities : data [ name_lhs ] = coeffs [ - 1 ] for i , name in enumerate ( names_rhs ): data [ name_lhs ] += coeffs [ i ] * data [ name ] return data","title":"augment_data()"},{"location":"ref-tools/#opti.tools.reduce.AffineTransform.drop_data","text":"Drop eliminated parameters from a DataFrame. Source code in opti/tools/reduce.py def drop_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Drop eliminated parameters from a DataFrame.\"\"\" drop = [] for name_lhs , _ , _ in self . equalities : if name_lhs in data . columns : drop . append ( name_lhs ) return data . drop ( columns = drop )","title":"drop_data()"},{"location":"ref-tools/#opti.tools.reduce.check_existence_of_solution","text":"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem. Source code in opti/tools/reduce.py def check_existence_of_solution ( A_aug ): \"\"\"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.\"\"\" A = A_aug [:, : - 1 ] b = A_aug [:, - 1 ] len_inputs = np . shape ( A )[ 1 ] # catch special cases rk_A_aug = np . linalg . matrix_rank ( A_aug ) rk_A = np . linalg . matrix_rank ( A ) if rk_A == rk_A_aug : if rk_A < len_inputs : return # all good else : x = np . linalg . solve ( A , b ) raise Exception ( f \"There is a unique solution x for the linear equality constraints: x= { x } \" ) elif rk_A < rk_A_aug : raise Exception ( \"There is no solution fulfilling the linear equality constraints.\" )","title":"check_existence_of_solution()"},{"location":"ref-tools/#opti.tools.reduce.check_problem_for_reduction","text":"Check if the reduction can be applied or if a trivial case is present. Source code in opti/tools/reduce.py def check_problem_for_reduction ( problem : Problem ) -> bool : \"\"\"Check if the reduction can be applied or if a trivial case is present.\"\"\" # are there any constraints? if problem . constraints is None : return False # are there any linear equality constraints? linear_equalities , _ = find_linear_equalities ( problem . constraints ) if len ( linear_equalities ) == 0 : return False # are there continuous inputs continuous_inputs , _ = find_continuous_inputs ( problem . inputs ) if len ( continuous_inputs ) == 0 : return False # check that equality constraints only contain continuous inputs for c in linear_equalities : for name in c . names : if name not in continuous_inputs . names : raise Exception ( f \"Linear equality constraint { c } contains a non-continuous parameter. Problem reduction is not supported.\" ) return True","title":"check_problem_for_reduction()"},{"location":"ref-tools/#opti.tools.reduce.find_continuous_inputs","text":"Separate parameters into continuous and all other parameters. Source code in opti/tools/reduce.py def find_continuous_inputs ( inputs : Parameters ) -> Tuple [ Parameters , Parameters ]: \"\"\"Separate parameters into continuous and all other parameters.\"\"\" continous_inputs = [ p for p in inputs if isinstance ( p , Continuous )] other_inputs = [ p for p in inputs if not isinstance ( p , Continuous )] return Parameters ( continous_inputs ), Parameters ( other_inputs )","title":"find_continuous_inputs()"},{"location":"ref-tools/#opti.tools.reduce.find_linear_equalities","text":"Separate constraints into linear equalities and all other constraints. Source code in opti/tools/reduce.py def find_linear_equalities ( constraints : Constraints ) -> Tuple [ Constraints , Constraints ]: \"\"\"Separate constraints into linear equalities and all other constraints.\"\"\" linear_equalities = [ c for c in constraints if isinstance ( c , LinearEquality )] other_constraints = [ c for c in constraints if not isinstance ( c , LinearEquality )] return Constraints ( linear_equalities ), Constraints ( other_constraints )","title":"find_linear_equalities()"},{"location":"ref-tools/#opti.tools.reduce.reduce_problem","text":"Reduce a problem with linear constraints to a subproblem where linear equality constraints are eliminated. Parameters: Name Type Description Default problem Problem problem to be reduced required Returns: Type Description Tuple[opti.problem.Problem, opti.tools.reduce.AffineTransform] (problem, trafo). Problem is the reduced problem where linear equality constraints have been eliminated. trafo is the according transformation. Source code in opti/tools/reduce.py def reduce_problem ( problem : Problem ) -> Tuple [ Problem , AffineTransform ]: \"\"\"Reduce a problem with linear constraints to a subproblem where linear equality constraints are eliminated. Args: problem (Problem): problem to be reduced Returns: (problem, trafo). Problem is the reduced problem where linear equality constraints have been eliminated. trafo is the according transformation. \"\"\" # check if the problem can be reduced if not check_problem_for_reduction ( problem ): return problem , AffineTransform ([]) # find linear equality constraints linear_equalities , other_constraints = find_linear_equalities ( problem . constraints ) # only consider continuous inputs continuous_inputs , other_inputs = find_continuous_inputs ( problem . inputs ) # assemble Matrix A from equality constraints N = len ( linear_equalities ) M = len ( continuous_inputs ) + 1 names = np . concatenate (( continuous_inputs . names , [ \"rhs\" ])) A_aug = pd . DataFrame ( data = np . zeros ( shape = ( N , M )), columns = names ) for i in range ( len ( linear_equalities )): c = linear_equalities [ i ] A_aug . loc [ i , c . names ] = c . lhs A_aug . loc [ i , \"rhs\" ] = c . rhs A_aug = A_aug . values # catch special cases check_existence_of_solution ( A_aug ) # bring A_aug to reduced row-echelon form A_aug_rref , pivots = rref ( A_aug ) pivots = np . array ( pivots ) A_aug_rref = np . array ( A_aug_rref ) . astype ( np . float64 ) # formulate box bounds as linear inequality constraints in matrix form B = np . zeros ( shape = ( 2 * ( M - 1 ), M )) B [: M - 1 , : M - 1 ] = np . eye ( M - 1 ) B [ M - 1 :, : M - 1 ] = - np . eye ( M - 1 ) B [: M - 1 , - 1 ] = continuous_inputs . bounds . loc [ \"max\" ] . copy () B [ M - 1 :, - 1 ] = - continuous_inputs . bounds . loc [ \"min\" ] . copy () # eliminate columns with pivot element for i in range ( len ( pivots )): p = pivots [ i ] B [ p , :] -= A_aug_rref [ i , :] B [ p + M - 1 , :] += A_aug_rref [ i , :] # build up reduced problem _inputs = list ( other_inputs . parameters . values ()) for i in range ( len ( continuous_inputs )): # add all inputs that were not eliminated if i not in pivots : _inputs . append ( continuous_inputs [ names [ i ]]) _inputs = Parameters ( _inputs ) _constraints = other_constraints . constraints for i in pivots : # reduce equation system of upper bounds ind = np . where ( B [ i , : - 1 ] != 0 )[ 0 ] if len ( ind ) > 0 and B [ i , - 1 ] < np . inf : c = LinearInequality ( names = list ( names [ ind ]), lhs = B [ i , ind ], rhs = B [ i , - 1 ]) _constraints . append ( c ) else : if B [ i , - 1 ] < - 1e-16 : raise Exception ( \"There is no solution that fulfills the constraints.\" ) # reduce equation system of lower bounds ind = np . where ( B [ i + M - 1 , : - 1 ] != 0 )[ 0 ] if len ( ind ) > 0 and B [ i + M - 1 , - 1 ] < np . inf : c = LinearInequality ( names = list ( names [ ind ]), lhs = B [ i + M - 1 , ind ], rhs = B [ i + M - 1 , - 1 ] ) _constraints . append ( c ) else : if B [ i + M - 1 , - 1 ] < - 1e-16 : raise Exception ( \"There is no solution that fulfills the constraints.\" ) _constraints = Constraints ( _constraints ) # assemble equalities _equalities = [] for i in range ( len ( pivots )): name_lhs = names [ pivots [ i ]] names_rhs = [] coeffs = [] for j in range ( len ( names ) - 1 ): if A_aug_rref [ i , j ] != 0 and j != pivots [ i ]: coeffs . append ( - A_aug_rref [ i , j ]) names_rhs . append ( names [ j ]) coeffs . append ( A_aug_rref [ i , - 1 ]) _equalities . append ([ name_lhs , names_rhs , coeffs ]) _data = problem . data trafo = AffineTransform ( _equalities ) _models = problem . models if _models is not None : warnings . warn ( \"Models are currently not adapted in reduce_problem.\" ) if hasattr ( problem , \"f\" ) and problem . f is not None : def _f ( X : pd . DataFrame ) -> pd . DataFrame : return problem . f ( trafo . augment_data ( X )) else : _f = None _problem = Problem ( inputs = _inputs , outputs = deepcopy ( problem . outputs ), objectives = deepcopy ( problem . objectives ), constraints = deepcopy ( _constraints ), f = _f , models = _models , data = _data , optima = deepcopy ( problem . optima ), name = deepcopy ( problem . name ), ) # remove remaining dependencies of eliminated inputs from the problem _problem = remove_eliminated_inputs ( _problem , trafo ) return _problem , trafo","title":"reduce_problem()"},{"location":"ref-tools/#opti.tools.reduce.remove_eliminated_inputs","text":"Eliminates remaining occurences of eliminated inputs in linear constraints. Source code in opti/tools/reduce.py def remove_eliminated_inputs ( problem : Problem , transform : AffineTransform ) -> Problem : \"\"\"Eliminates remaining occurences of eliminated inputs in linear constraints.\"\"\" inputs_names = problem . inputs . names M = len ( inputs_names ) # write the equalities for the backtransformation into one matrix inputs_dict = { inputs_names [ i ]: i for i in range ( M )} # build up dict from problem.equalities e.g. {\"xi1\": [coeff(xj1), ..., coeff(xjn)], ... \"xik\":...} coeffs_dict = {} for i , e in enumerate ( transform . equalities ): coeffs = np . zeros ( M + 1 ) for j , name in enumerate ( e [ 1 ]): coeffs [ inputs_dict [ name ]] = e [ 2 ][ j ] coeffs [ - 1 ] = e [ 2 ][ - 1 ] coeffs_dict [ e [ 0 ]] = coeffs constraints = [] for c in problem . constraints : # Nonlinear constraints supported if not isinstance ( c , ( LinearEquality , LinearInequality )): raise Exception ( \"Elimination of variables is only supported for LinearEquality and LinearInequality constraints.\" ) # no changes, if the constraint does not contain eliminated inputs elif all ( name in inputs_names for name in c . names ): constraints . append ( c ) # remove inputs from the constraint that were eliminated from the inputs before else : _names = np . array ( inputs_names ) _rhs = c . rhs # create new lhs and rhs from the old one and knowledge from problem._equalities _lhs = np . zeros ( M ) for j , name in enumerate ( c . names ): if name in inputs_names : _lhs [ inputs_dict [ name ]] += c . lhs [ j ] else : _lhs += c . lhs [ j ] * coeffs_dict [ name ][: - 1 ] _rhs -= c . lhs [ j ] * coeffs_dict [ name ][ - 1 ] _names = _names [ np . abs ( _lhs ) > 1e-16 ] _lhs = _lhs [ np . abs ( _lhs ) > 1e-16 ] # create new Constraints if isinstance ( c , LinearEquality ): _c = LinearEquality ( _names , _lhs , _rhs ) else : _c = LinearInequality ( _names , _lhs , _rhs ) # check if constraint is always fulfilled/not fulfilled if len ( _c . names ) == 0 and _c . rhs >= 0 : pass elif len ( _c . names ) == 0 and _c . rhs < 0 : raise Exception ( \"Linear constraints cannot be fulfilled.\" ) elif np . isinf ( _c . rhs ): pass else : constraints . append ( _c ) problem . constraints = Constraints ( constraints ) return problem","title":"remove_eliminated_inputs()"},{"location":"ref-tools/#opti.tools.reduce.rref","text":"Computes the reduced row echelon form of a Matrix Parameters: Name Type Description Default A ndarray 2d array representing a matrix. required tol float tolerance for rounding to 0 1e-08 Returns: Type Description Tuple[numpy.ndarray, List[int]] (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref Source code in opti/tools/reduce.py def rref ( A : np . ndarray , tol = 1e-8 ) -> Tuple [ np . ndarray , List [ int ]]: \"\"\"Computes the reduced row echelon form of a Matrix Args: A (ndarray): 2d array representing a matrix. tol (float): tolerance for rounding to 0 Returns: (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref \"\"\" A = np . array ( A , dtype = np . float64 ) n , m = np . shape ( A ) col = 0 row = 0 pivots = [] for col in range ( m ): # does a pivot element exist? if all ( np . abs ( A [ row :, col ]) < tol ): pass # if yes: start elimination else : pivots . append ( col ) max_row = np . argmax ( np . abs ( A [ row :, col ])) + row # switch to most stable row A [[ row , max_row ], :] = A [[ max_row , row ], :] # normalize row A [ row , :] /= A [ row , col ] # eliminate other elements from column for r in range ( n ): if r != row : A [ r , :] -= A [ r , col ] / A [ row , col ] * A [ row , :] row += 1 prec = int ( - np . log10 ( tol )) return np . round ( A , prec ), pivots","title":"rref()"},{"location":"ref-tools/#opti.tools.sanitize","text":"","title":"sanitize"},{"location":"ref-tools/#opti.tools.sanitize.sanitize_problem","text":"This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives. More precisely, the resulting problem has the following properties: - Inputs are named input_0 , input_1 , .... Outputs are named analogously. - The data is scaled per feature to [0, 1] . - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of f are dropped if there are any. Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints. Parameters: Name Type Description Default problem Problem to be sanitized required Exceptions: Type Description TypeError in case there are unsupported constraints, data is None, or there are output constraints Returns: Type Description Problem Problem instance with sanitized labels and normalized data Source code in opti/tools/sanitize.py def sanitize_problem ( problem : Problem ) -> Problem : \"\"\" This creates a transformation of the problem with sanitized data. Thereby, we try to preserve relationships between inputs, outputs, and objectives. More precisely, the resulting problem has the following properties: - Inputs are named `input_0`, `input_1`, .... Outputs are named analogously. - The data is scaled per feature to `[0, 1]`. - Coefficients of linear constraints are adapted to the data scaling. - Models and evaluatons in terms of `f` are dropped if there are any. Currently unsuported are problems with - discrete or categorical variables, - nonlinear (in)equality input constraints, or - output constraints. Args: problem: to be sanitized Raises: TypeError: in case there are unsupported constraints, data is None, or there are output constraints Returns: Problem instance with sanitized labels and normalized data \"\"\" if problem . data is None : raise TypeError ( \"we cannot sanitize a problem without data\" ) if problem . output_constraints is not None : raise TypeError ( \"output constraints are currently not supported\" ) if getattr ( problem , \"f\" , None ) is not None : warnings . warn ( \"f is not sanitized but dropped\" ) if problem . models is not None : warnings . warn ( \"models are not sanitized but dropped\" ) inputs = _sanitize_params ( problem . inputs , \"input\" ) input_name_map = { pi . name : i . name for pi , i in zip ( problem . inputs , inputs )} normalized_in_data , xmin , \u0394x = _normalize_parameters_data ( problem . data , problem . inputs ) outputs = _sanitize_params ( problem . outputs , \"output\" ) output_name_map = { pi . name : i . name for pi , i in zip ( problem . outputs , outputs )} normalized_out_data , ymin , \u0394y = _normalize_parameters_data ( problem . data , problem . outputs ) normalized_in_data . columns = inputs . names normalized_out_data . columns = outputs . names normalized_data = pd . concat ([ normalized_in_data , normalized_out_data ], axis = 1 ) normalized_data . reset_index ( inplace = True , drop = True ) objectives = deepcopy ( problem . objectives ) for obj in objectives : sanitized_name = output_name_map [ obj . name ] i = outputs . names . index ( sanitized_name ) obj . name = sanitized_name obj . parameter = sanitized_name obj . target = ( obj . target - ymin [ i ]) / \u0394y [ i ] if hasattr ( obj , \"tolerance\" ): obj . tolerance /= \u0394y [ i ] constraints = deepcopy ( problem . constraints ) if constraints is not None : for c in constraints : c . names = [ input_name_map [ n ] for n in c . names ] if isinstance ( c , ( LinearEquality , LinearInequality )): c . lhs = ( c . lhs + xmin ) * \u0394x if c . rhs > 1e-5 : c . lhs = c . lhs / c . rhs c . rhs = 1.0 elif isinstance ( c , NChooseK ): pass else : raise TypeError ( \"sanitizer only supports linear and n-choose-k constraints\" ) normalized_problem = Problem ( inputs = inputs , outputs = outputs , objectives = objectives , constraints = constraints , data = normalized_data , ) return normalized_problem","title":"sanitize_problem()"},{"location":"datasets/alkox/","text":"Alkox This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx). The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective. problem = opti . problems . Alkox () Reference F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI . Obtained from Olympus .","title":"Alkox"},{"location":"datasets/alkox/#alkox","text":"This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx). The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective. problem = opti . problems . Alkox () Reference F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI . Obtained from Olympus .","title":"Alkox"},{"location":"datasets/baumgartner-aniline/","text":"Aniline cross-coupling (Baumgartner 2019) Dataset on optimizing Pd-catalyzed C\u2013N coupling reactions promoted by organic bases using aniline as starting material. problem = opti . problems . BaumgartnerAniline () Reference Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI . Data obtained from Summit .","title":"Aniline cross-coupling (Baumgartner 2019)"},{"location":"datasets/baumgartner-aniline/#aniline-cross-coupling-baumgartner-2019","text":"Dataset on optimizing Pd-catalyzed C\u2013N coupling reactions promoted by organic bases using aniline as starting material. problem = opti . problems . BaumgartnerAniline () Reference Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI . Data obtained from Summit .","title":"Aniline cross-coupling (Baumgartner 2019)"},{"location":"datasets/baumgartner-benzamide/","text":"Benzamide cross-coupling (Baumgartner 2019) Dataset on optimizing Pd-catalyzed C\u2013N coupling reactions promoted by organic bases using benzamide as starting material. problem = opti . problems . BaumgartnerBenzamide () Reference Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI . Data obtained from Summit .","title":"Benzamide cross-coupling (Baumgartner 2019)"},{"location":"datasets/baumgartner-benzamide/#benzamide-cross-coupling-baumgartner-2019","text":"Dataset on optimizing Pd-catalyzed C\u2013N coupling reactions promoted by organic bases using benzamide as starting material. problem = opti . problems . BaumgartnerBenzamide () Reference Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI . Data obtained from Summit .","title":"Benzamide cross-coupling (Baumgartner 2019)"},{"location":"datasets/benzylation/","text":"Benzylation This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction. Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective. problem = opti . problems . Benzylation () Reference A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI . Obtained from Olympus .","title":"Benzylation"},{"location":"datasets/benzylation/#benzylation","text":"This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction. Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective. problem = opti . problems . Benzylation () Reference A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI . Obtained from Olympus .","title":"Benzylation"},{"location":"datasets/cake/","text":"Cake Fictional dataset for cake recipe optimization with mixed objectives. problem = opti . problems . Cake ()","title":"Cake"},{"location":"datasets/cake/#cake","text":"Fictional dataset for cake recipe optimization with mixed objectives. problem = opti . problems . Cake ()","title":"Cake"},{"location":"datasets/fullerenes/","text":"Fullerenes This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes. Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product. Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective. problem = opti . problems . Fullerenes () Reference B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798. DOI . Obtained from Olympus .","title":"Fullerenes"},{"location":"datasets/fullerenes/#fullerenes","text":"This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes. Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product. Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective. problem = opti . problems . Fullerenes () Reference B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798. DOI . Obtained from Olympus .","title":"Fullerenes"},{"location":"datasets/hplc/","text":"HPLC This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective. problem = opti . problems . HPLC () Reference L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) DOI . Obtained from Olympus .","title":"HPLC"},{"location":"datasets/hplc/#hplc","text":"This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective. problem = opti . problems . HPLC () Reference L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) DOI . Obtained from Olympus .","title":"HPLC"},{"location":"datasets/photodegradation/","text":"Photodegradation This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective. problem = opti . problems . Photodegradation () Reference S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. DOI . Obtained from Olympus .","title":"Photodegradation"},{"location":"datasets/photodegradation/#photodegradation","text":"This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light. Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective. problem = opti . problems . Photodegradation () Reference S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. DOI . Obtained from Olympus .","title":"Photodegradation"},{"location":"datasets/reizmann-suzuki/","text":"Suzuki (Reizmann 2016) Each case was has a different set of substrates but the same possible catalysts. problem = opti . problems . ReizmannSuzuki () Reference Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry & engineering, 1(6), 658-666 DOI . Data obtained from Summit .","title":"Suzuki (Reizmann 2016)"},{"location":"datasets/reizmann-suzuki/#suzuki-reizmann-2016","text":"Each case was has a different set of substrates but the same possible catalysts. problem = opti . problems . ReizmannSuzuki () Reference Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry & engineering, 1(6), 658-666 DOI . Data obtained from Summit .","title":"Suzuki (Reizmann 2016)"},{"location":"datasets/snar/","text":"SnAr This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism. Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective. problem = opti . problems . SnAr () Reference A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI . Obtained from Olympus .","title":"SnAr"},{"location":"datasets/snar/#snar","text":"This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism. Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective. problem = opti . problems . SnAr () Reference A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI . Obtained from Olympus .","title":"SnAr"},{"location":"datasets/suzuki/","text":"Suzuki This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate. Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective. problem = opti . problems . Suzuki () Reference F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI . Obtained from Olympus .","title":"Suzuki"},{"location":"datasets/suzuki/#suzuki","text":"This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate. Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective. problem = opti . problems . Suzuki () Reference F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI . Obtained from Olympus .","title":"Suzuki"},{"location":"examples/problem_reduction/","text":"Problem reduction When describing physical experiments there are often linear equality constraints to be considered. For example in a formulation all ingredients of a mixture add up to 1. problem = opti . Problem ( inputs = [ opti . Continuous ( \"x1\" , [ 0.1 , 1 ]), opti . Continuous ( \"x2\" , [ 0 , 0.8 ]), opti . Continuous ( \"x3\" , [ 0.3 , 0.9 ]), ], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ([ \"x1\" , \"x2\" , \"x3\" ], rhs = 1 )] ) In statistical modeling linear equalities lead to multicollinearities, which makes the coefficients of linear models sensitive to noise. For modeling tasks this collinearity can be addressed by e.g. dropping one input parameter for each corresponding equality constraint. For sampling and optimization tasks this becomes a bit trickier as the parameter bounds and inequality constraints need to be adapted as well. Consider in the initial example we drop \\(x_3\\) together with the linear equality. To ensure that solutions ( \\(x_1\\) , \\(x_2\\) ) still satisfy the box bounds and constraints, we need to add the following two inequality constraints: $$ \\begin{align} x_3 \\geq 0.3 \\Longleftrightarrow x_1 + x_2 \\leq 0.7 \\newline x_3 \\leq 0.9 \\Longleftrightarrow x_1 + x_2 \\geq 0.1 \\end{align} $$ The function reduce_problem automates this tedious task. Given a problem containing any number of linear inequalities and at least one equality constraint, it returns an equivalent problem where the linear equalities are removed by eliminating a corresponding number of inputs. reduced_problem , transform = opti . tools . reduce_problem ( problem ) print ( reduced_problem ) >>> Problem ( inputs = Parameters ([ Continuous ( 'x2' , domain = [ 0.0 , 0.8 ]), Continuous ( 'x3' , domain = [ 0.1 , 1.0 ]) ]), outputs = Parameters ([ Continuous ( 'y' )]), objectives = Objectives ([ Minimize ( 'y' )]), constraints = Constraints ([ LinearInequality ( names = [ 'x2' , 'x3' ], lhs = [ - 1.0 , - 1.0 ], rhs =- 0.1 ), LinearInequality ( names = [ 'x2' , 'x3' ], lhs = [ 1.0 , 1.0 ], rhs = 0.7 ) ]) ) The transformer object allows to transfrom data to and from the reduced space. X = problem . sample_inputs ( 10 ) Xr = transform . drop_data ( X ) X2 = transform . augment_data ( Xr ) assert np . allclose ( X , X2 [ X . columns ]) Equality constraints are not well supported in sampling (any form of acceptance-rejection sampling will not work) and optimization methods. For example population-based optimization approaches such as evolutionary algorithms only approximately support linear equalities via penalties or a conversion to two-sided inequalites. By reducing the problem, such optimization tasks become significantly easier to solve. Finally, let's consider a more involved example involving two mixtures, A and B, as well as an additional discrete and categorical variable, and an extra inequality constraint for some of the components of mixture A. We also set up a function y = f(X) to evaluate the system. def f ( X ): y = X [[ \"A1\" , \"A2\" , \"A3\" , \"A4\" ]] @ [ 1 , - 2 , 3 , 2 ] y += X [[ \"B1\" , \"B2\" , \"B3\" ]] @ [ 0.1 , 0.4 , 0.3 ] y += X [ \"Temperature\" ] / 30 y += X [ \"Process\" ] == \"process 2\" return pd . DataFrame ({ \"y\" : y }) problem = opti . Problem ( inputs = [ opti . Continuous ( \"A1\" , [ 0 , 0.9 ]), opti . Continuous ( \"A2\" , [ 0 , 0.8 ]), opti . Continuous ( \"A3\" , [ 0 , 0.9 ]), opti . Continuous ( \"A4\" , [ 0 , 0.9 ]), opti . Continuous ( \"B1\" , [ 0.3 , 0.9 ]), opti . Continuous ( \"B2\" , [ 0 , 0.8 ]), opti . Continuous ( \"B3\" , [ 0.1 , 1 ]), opti . Discrete ( \"Temperature\" , [ 20 , 25 , 30 ]), opti . Categorical ( \"Process\" , [ \"process 1\" , \"process 2\" , \"process 3\" ]) ], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ([ \"A1\" , \"A2\" , \"A3\" , \"A4\" ], rhs = 1 ), opti . LinearEquality ([ \"B1\" , \"B2\" , \"B3\" ], rhs = 1 ), opti . LinearInequality ([ \"A1\" , \"A2\" ], lhs = [ 1 , 2 ], rhs = 0.8 ), ], f = f ) Reducing the problem works despite the discrete and categorical inputs as these don't appear in the linear equalities. We end up 7 out of 9 initial inputs and 5 inequality constraints, which are only referring to the remaining inputs. reduced_problem , transform = opti . tools . reduce_problem ( problem ) print ( reduced_problem ) >>> Problem ( inputs = Parameters ([ Discrete ( 'Temperature' , domain = [ 20.0 , 25.0 , 30.0 ]), Categorical ( 'Process' , domain = [ 'process 1' , 'process 2' , 'process 3' ]), Continuous ( 'A2' , domain = [ 0.0 , 0.8 ]), Continuous ( 'A3' , domain = [ 0.0 , 0.9 ]), Continuous ( 'A4' , domain = [ 0.0 , 0.9 ]), Continuous ( 'B2' , domain = [ 0.0 , 0.8 ]), Continuous ( 'B3' , domain = [ 0.1 , 1.0 ]) ]), outputs = Parameters ([ Continuous ( 'y' )]), objectives = Objectives ([ Minimize ( 'y' )]), constraints = Constraints ([ LinearInequality ( names = [ 'A2' 'A3' 'A4' ], lhs = [ 1.0 , - 1.0 , - 1.0 ], rhs =- 0.2 ), LinearInequality ( names = [ 'A2' , 'A3' , 'A4' ], lhs = [ - 1.0 , - 1.0 , - 1.0 ], rhs =- 0.1 ), LinearInequality ( names = [ 'A2' , 'A3' , 'A4' ], lhs = [ 1.0 , 1.0 , 1.0 ], rhs = 1.0 ), LinearInequality ( names = [ 'B2' , 'B3' ], lhs = [ - 1.0 , - 1.0 ], rhs =- 0.1 ), LinearInequality ( names = [ 'B2' , 'B3' ], lhs = [ 1.0 , 1.0 ], rhs = 0.7 ) ]) ) The function f(X) was automaticaly wrapped so the in the reduced problem it can be evaluated for points in the reduced space, with the same result. Xr = reduced_problem . sample_inputs ( 10 ) X = transform . augment_data ( Xr ) y1 = problem . f ( X ) y2 = reduced_problem . f ( Xr ) assert np . allclose ( y1 , y2 )","title":"Problem reduction"},{"location":"examples/problem_reduction/#problem-reduction","text":"When describing physical experiments there are often linear equality constraints to be considered. For example in a formulation all ingredients of a mixture add up to 1. problem = opti . Problem ( inputs = [ opti . Continuous ( \"x1\" , [ 0.1 , 1 ]), opti . Continuous ( \"x2\" , [ 0 , 0.8 ]), opti . Continuous ( \"x3\" , [ 0.3 , 0.9 ]), ], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ([ \"x1\" , \"x2\" , \"x3\" ], rhs = 1 )] ) In statistical modeling linear equalities lead to multicollinearities, which makes the coefficients of linear models sensitive to noise. For modeling tasks this collinearity can be addressed by e.g. dropping one input parameter for each corresponding equality constraint. For sampling and optimization tasks this becomes a bit trickier as the parameter bounds and inequality constraints need to be adapted as well. Consider in the initial example we drop \\(x_3\\) together with the linear equality. To ensure that solutions ( \\(x_1\\) , \\(x_2\\) ) still satisfy the box bounds and constraints, we need to add the following two inequality constraints: $$ \\begin{align} x_3 \\geq 0.3 \\Longleftrightarrow x_1 + x_2 \\leq 0.7 \\newline x_3 \\leq 0.9 \\Longleftrightarrow x_1 + x_2 \\geq 0.1 \\end{align} $$ The function reduce_problem automates this tedious task. Given a problem containing any number of linear inequalities and at least one equality constraint, it returns an equivalent problem where the linear equalities are removed by eliminating a corresponding number of inputs. reduced_problem , transform = opti . tools . reduce_problem ( problem ) print ( reduced_problem ) >>> Problem ( inputs = Parameters ([ Continuous ( 'x2' , domain = [ 0.0 , 0.8 ]), Continuous ( 'x3' , domain = [ 0.1 , 1.0 ]) ]), outputs = Parameters ([ Continuous ( 'y' )]), objectives = Objectives ([ Minimize ( 'y' )]), constraints = Constraints ([ LinearInequality ( names = [ 'x2' , 'x3' ], lhs = [ - 1.0 , - 1.0 ], rhs =- 0.1 ), LinearInequality ( names = [ 'x2' , 'x3' ], lhs = [ 1.0 , 1.0 ], rhs = 0.7 ) ]) ) The transformer object allows to transfrom data to and from the reduced space. X = problem . sample_inputs ( 10 ) Xr = transform . drop_data ( X ) X2 = transform . augment_data ( Xr ) assert np . allclose ( X , X2 [ X . columns ]) Equality constraints are not well supported in sampling (any form of acceptance-rejection sampling will not work) and optimization methods. For example population-based optimization approaches such as evolutionary algorithms only approximately support linear equalities via penalties or a conversion to two-sided inequalites. By reducing the problem, such optimization tasks become significantly easier to solve. Finally, let's consider a more involved example involving two mixtures, A and B, as well as an additional discrete and categorical variable, and an extra inequality constraint for some of the components of mixture A. We also set up a function y = f(X) to evaluate the system. def f ( X ): y = X [[ \"A1\" , \"A2\" , \"A3\" , \"A4\" ]] @ [ 1 , - 2 , 3 , 2 ] y += X [[ \"B1\" , \"B2\" , \"B3\" ]] @ [ 0.1 , 0.4 , 0.3 ] y += X [ \"Temperature\" ] / 30 y += X [ \"Process\" ] == \"process 2\" return pd . DataFrame ({ \"y\" : y }) problem = opti . Problem ( inputs = [ opti . Continuous ( \"A1\" , [ 0 , 0.9 ]), opti . Continuous ( \"A2\" , [ 0 , 0.8 ]), opti . Continuous ( \"A3\" , [ 0 , 0.9 ]), opti . Continuous ( \"A4\" , [ 0 , 0.9 ]), opti . Continuous ( \"B1\" , [ 0.3 , 0.9 ]), opti . Continuous ( \"B2\" , [ 0 , 0.8 ]), opti . Continuous ( \"B3\" , [ 0.1 , 1 ]), opti . Discrete ( \"Temperature\" , [ 20 , 25 , 30 ]), opti . Categorical ( \"Process\" , [ \"process 1\" , \"process 2\" , \"process 3\" ]) ], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ([ \"A1\" , \"A2\" , \"A3\" , \"A4\" ], rhs = 1 ), opti . LinearEquality ([ \"B1\" , \"B2\" , \"B3\" ], rhs = 1 ), opti . LinearInequality ([ \"A1\" , \"A2\" ], lhs = [ 1 , 2 ], rhs = 0.8 ), ], f = f ) Reducing the problem works despite the discrete and categorical inputs as these don't appear in the linear equalities. We end up 7 out of 9 initial inputs and 5 inequality constraints, which are only referring to the remaining inputs. reduced_problem , transform = opti . tools . reduce_problem ( problem ) print ( reduced_problem ) >>> Problem ( inputs = Parameters ([ Discrete ( 'Temperature' , domain = [ 20.0 , 25.0 , 30.0 ]), Categorical ( 'Process' , domain = [ 'process 1' , 'process 2' , 'process 3' ]), Continuous ( 'A2' , domain = [ 0.0 , 0.8 ]), Continuous ( 'A3' , domain = [ 0.0 , 0.9 ]), Continuous ( 'A4' , domain = [ 0.0 , 0.9 ]), Continuous ( 'B2' , domain = [ 0.0 , 0.8 ]), Continuous ( 'B3' , domain = [ 0.1 , 1.0 ]) ]), outputs = Parameters ([ Continuous ( 'y' )]), objectives = Objectives ([ Minimize ( 'y' )]), constraints = Constraints ([ LinearInequality ( names = [ 'A2' 'A3' 'A4' ], lhs = [ 1.0 , - 1.0 , - 1.0 ], rhs =- 0.2 ), LinearInequality ( names = [ 'A2' , 'A3' , 'A4' ], lhs = [ - 1.0 , - 1.0 , - 1.0 ], rhs =- 0.1 ), LinearInequality ( names = [ 'A2' , 'A3' , 'A4' ], lhs = [ 1.0 , 1.0 , 1.0 ], rhs = 1.0 ), LinearInequality ( names = [ 'B2' , 'B3' ], lhs = [ - 1.0 , - 1.0 ], rhs =- 0.1 ), LinearInequality ( names = [ 'B2' , 'B3' ], lhs = [ 1.0 , 1.0 ], rhs = 0.7 ) ]) ) The function f(X) was automaticaly wrapped so the in the reduced problem it can be evaluated for points in the reduced space, with the same result. Xr = reduced_problem . sample_inputs ( 10 ) X = transform . augment_data ( Xr ) y1 = problem . f ( X ) y2 = reduced_problem . f ( Xr ) assert np . allclose ( y1 , y2 )","title":"Problem reduction"}]}