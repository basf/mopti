{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Warning</p> <p>Opti is deprecated. Consider using BoFire instead.</p> <p>The primary purpose of opti is to define tasks or problems in a number of closely related fields, including experimental design, multiobjective optimization and decision making and Bayesian optimization.</p> <p>Opti specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.</p>"},{"location":"#experimental-design","title":"Experimental design","text":"<p>In the context of experimental design opti allows to define a design space</p> \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] <p>where the design parameters may take values depending on their type and domain, e.g.</p> <ul> <li>continuous: \\(x_1 \\in [0, 1]\\)</li> <li>discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\)</li> <li>categorical: \\(x_3 \\in \\{A, B, C\\}\\)</li> </ul> <p>and a set of equations define additional experimental constraints, e.g.</p> <ul> <li>linear equality: \\(\\sum x_i = 1\\)</li> <li>linear inequality: \\(2 x_1 \\leq x_2\\)</li> <li>non-linear inequality: \\(\\sum x_i^2 \\leq 1\\)</li> <li>n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.</li> </ul>"},{"location":"#multiobjective-optimization","title":"Multiobjective optimization","text":"<p>In the context of multiobjective optimization opti allows to define a vector-valued optimization problem</p> \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] <p>where</p> <ul> <li>\\(x \\in \\mathbb{X}\\) is again the experimental design space</li> <li>\\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and</li> <li>\\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized.</li> </ul> <p>Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.</p>"},{"location":"#bayesian-optimization","title":"Bayesian optimization","text":"<p>In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\). An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation</p> \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] <p>and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.</p>"},{"location":"install/","title":"Install","text":"<p>Note: the package name is <code>mopti</code> while the import name is <code>opti</code>. <pre><code>pip install mopti\n</code></pre> or install the latest version with <pre><code>pip install git+https://github.com/basf/mopti.git\n</code></pre></p>"},{"location":"problem/","title":"Getting started","text":"<p>Opti problems consist of a definition of </p> <ul> <li>the input parameters \\(x \\in \\mathbb{X}\\), </li> <li>the output parameters \\(y \\in \\mathbb{Y}\\), </li> <li>the objectives \\(s(y)\\) (optional), </li> <li>the input constraints \\(g(x) \\leq 0\\) (optional), </li> <li>the output constraints \\(h(y)\\) (optional),</li> <li>a data set of previous function evaluations \\(\\{x, y\\}\\) (optional)</li> <li>and the function \\(f(x)\\) to be optimized (optional).</li> </ul>"},{"location":"problem/#parameters","title":"Parameters","text":"<p>Input and output spaces are defined using <code>Parameters</code> objects.  For example a mixed input space of three continuous, one discrete and one categorical parameter(s), along with an output space of continuous parameters can be defined as:</p> <p><pre><code>import opti\n\ninputs = opti.Parameters([\n    opti.Continuous(\"x1\", domain=[0, 1]),\n    opti.Continuous(\"x2\", domain=[0, 1]),\n    opti.Continuous(\"x3\", domain=[0, 1]),\n    opti.Discrete(\"x4\", domain=[1, 2, 5, 7.5]),\n    opti.Categorical(\"x5\", domain=[\"A\", \"B\", \"C\"])\n])\n\noutputs = opti.Parameters([\n    opti.Continuous(\"y1\", domain=[0, None]),\n    opti.Continuous(\"y2\", domain=[None, None]),\n    opti.Continuous(\"y3\", domain=[0, 100])\n])\n</code></pre> Note that for some of the outputs we didn't specify bounds as we may not know them.</p> <p>Individual parameters can be indexed by name. <pre><code>inputs[\"x5\"]\n&gt;&gt;&gt; Categorical(\"x5\", domain=[\"A\", \"B\", \"C\"])\n</code></pre> and all parameter names can retrieved with <pre><code>inputs.names\n&gt;&gt;&gt; [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"]\n</code></pre></p> <p>We can sample from individual parameters, parameter spaces or parameter spaces including constraints (more on that later) <pre><code>x5 = inputs[\"x1\"].sample(3)\nprint(x5.values)\n&gt;&gt;&gt; array([0.72405216, 0.14914942, 0.46051132])\n\nX = inputs.sample(5)\nprint(X)\n&gt;&gt;&gt;      x1        x2        x3   x4 x5\n0  0.760116  0.063584  0.518885  7.5  A\n1  0.807928  0.496213  0.885545  1.0  C\n2  0.351253  0.993993  0.340414  5.0  B\n3  0.385825  0.857306  0.355267  1.0  C\n4  0.191907  0.993494  0.384322  2.0  A\n</code></pre></p> <p>We can also check for each point in a dataframe, whether it is contained in the space. <pre><code>inputs.contains(X)\n&gt;&gt;&gt; array([ True,  True,  True,  True,  True])\n</code></pre></p> <p>In general all opti functions operate on dataframes and thus use the parameter name to identify corresponding column.  Hence, a dataframe may contain additional columns and columns may be in arbitrary order. The index of a dataframe is preserved, meaning that the returned dataframe will have the same indices as the original dataframe.</p>"},{"location":"problem/#constraints","title":"Constraints","text":"<p>Input constraints are defined separately from the input space. The following constraints are supported.</p> <p>Linear constraints (<code>LinearEquality</code> and <code>LinearInequality</code>) are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input parameters they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\). <pre><code># A mixture: x1 + x2 + x3 = 1\nconstr1 = opti.LinearEquality([\"x1\", \"x2\", \"x3\"], lhs=1, rhs=1)\n\n# x1 + 2 * x3 &lt; 0.8\nconstr2 = opti.LinearInequality([\"x1\", \"x3\"], lhs=[1, 2], rhs=0.8)\n</code></pre> Because of the product \\(a_i x_i\\), linear constraints cannot operate on categorical parameters.</p> <p>Nonlinear constraints (<code>NonlinearEquality</code> and <code>NonlinearInequality</code>) take any expression that can be evaluated by pandas.eval, including mathematical operators such as <code>sin</code>, <code>exp</code>, <code>log10</code> or exponentiation. <pre><code># The unit circle: x1**2 + x2**2 = 1\nconstr3 = opti.NonlinearEquality(\"x1**2 + x2**2 - 1\")\n</code></pre> Nonlinear constraints can also operate on categorical parameters and support conditional statements. <pre><code># Require x1 &lt; 0.5 if x5 == \"A\"\nconstr4 = opti.NonlinearInequality(\"(x1 - 0.5) * (x5 =='A')\")\n</code></pre></p> <p>Finally, there is a combinatorical constraint (<code>NChooseK</code>) to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe. <pre><code># Only 2 out of 3 parameters can be greater than zero\nconstr5 = opti.NChooseK([\"x1\", \"x2\", \"x3\"], max_active=2)\n</code></pre></p> <p>Constraints can be grouped in a container which acts as the union constraints. <pre><code>constraints = opti.Constraints([constr1, constr2, constr3, constr4, constr5])\n</code></pre></p> <p>We can check whether a point satisfies individual constraints or the list of constraints. <pre><code>constr2.satisfied(X).values\n&gt;&gt;&gt; array([False, False, True, True, True])\n</code></pre></p> <p>The distance to the constraint boundary can also be evaluated for use in numerical optimization methods, where values \\(\\leq 0\\) correspond to a satisified constraint. <pre><code>constr2(X).values\n&gt;&gt;&gt; array([ 0.479001  ,  0.89347371, -0.10833372, -0.05890873, -0.22377122])\n</code></pre></p> <p>Opti contains a number of methods to draw random samples from constrained spaces, see the sampling reference.</p>"},{"location":"problem/#objectives","title":"Objectives","text":"<p>In an optimization problem we need to define the target direction or target value individually for each output. This is done using objectives \\(s_m(y_m)\\) so that a mixed objective optimization becomes a minimization problem. <pre><code>objectives = opti.Objectives([\n    opti.Minimize(\"y1\"),\n    opti.Maximize(\"y2\"),\n    opti.CloseToTarget(\"y3\", target=7)\n])\n</code></pre></p> <p>We can compute objective values from output values to see the objective transformation applied. <pre><code>Y = pd.DataFrame({\n    \"y1\": [1, 2, 3],\n    \"y2\": [7, 4, 5],\n    \"y3\": [5, 6.9, 12]\n})\nobjectives(Y)\n&gt;&gt;&gt; minimize_y1  maximize_y2  closetotarget_y3\n0            1           -7              4.00\n1            2           -4              0.01\n2            3           -5             25.00\n</code></pre></p> <p>Objectives can also be used as output constraints.  This is different from an objective in that we want the constraint to be satisfied and not explore possible tradeoffs.</p>"},{"location":"problem/#problem","title":"Problem","text":"<p>Finally, a problem is the combination of inputs, outputs, objectives, constraints, output_constraints, (true) function and data.</p> <p><pre><code>problem = opti.Problem(\n    inputs=inputs,\n    outputs=outputs,\n    constraints=constraints,\n    objectives=objectives\n)\n</code></pre> Problems can be serialized to and from a dictionary <pre><code>config = problem.to_config()\nproblem = opti.Problem(**config)\n</code></pre> or to a json file <pre><code>problem.to_json(\"problem.json\")\nproblem = opti.read_json(\"problem.json\")\n</code></pre></p>"},{"location":"ref-constraint/","title":"Constraints","text":""},{"location":"ref-constraint/#opti.constraint.Constraint","title":"<code>Constraint</code>","text":"<p>Base class to define constraints on the input space, g(x) == 0 or g(x) &lt;= 0.</p> Source code in <code>opti/constraint.py</code> <pre><code>class Constraint:\n    \"\"\"Base class to define constraints on the input space, g(x) == 0 or g(x) &lt;= 0.\"\"\"\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Numerically evaluate the constraint g(x).\"\"\"\n        raise NotImplementedError\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Numerically evaluate the jacobian of the constraint J_g(x)\"\"\"\n        raise NotImplementedError\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.\"\"\"\n        raise NotImplementedError\n\n    def to_config(self) -&gt; Dict:\n        raise NotImplementedError\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraint.__call__","title":"<code>__call__(data)</code>","text":"<p>Numerically evaluate the constraint g(x).</p> Source code in <code>opti/constraint.py</code> <pre><code>def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Numerically evaluate the constraint g(x).\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraint.jacobian","title":"<code>jacobian(data)</code>","text":"<p>Numerically evaluate the jacobian of the constraint J_g(x)</p> Source code in <code>opti/constraint.py</code> <pre><code>def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Numerically evaluate the jacobian of the constraint J_g(x)\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraint.satisfied","title":"<code>satisfied(data)</code>","text":"<p>Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Check if a constraint is satisfied, i.e. g(x) == 0 for equalities and g(x) &lt;= for inequalities.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints","title":"<code>Constraints</code>","text":"<p>List of input constraints</p> Source code in <code>opti/constraint.py</code> <pre><code>class Constraints:\n    \"\"\"List of input constraints\"\"\"\n\n    def __init__(self, constraints: Sequence):\n        self.constraints = []\n        for c in constraints:\n            if not isinstance(c, Constraint):\n                if \"names\" in c and len(c[\"names\"]) == 0:\n                    continue  # skip empty constraints\n                c = make_constraint(**c)\n            self.constraints.append(c)\n\n    def __repr__(self):\n        return \"Constraints(\\n\" + pprint.pformat(self.constraints) + \"\\n)\"\n\n    def __iter__(self):\n        return iter(self.constraints)\n\n    def __len__(self):\n        return len(self.constraints)\n\n    def __getitem__(self, i):\n        return self.constraints[i]\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Numerically evaluate all constraints.\n\n        Args:\n            data: Data to evaluate the constraints on.\n\n        Returns:\n            Constraint evaluation g(x) for each of the constraints.\n        \"\"\"\n        return pd.concat([c(data) for c in self.constraints], axis=1)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; List:\n        \"\"\"Numerically evaluate all constraint gradients.\n\n        Args:\n            data: Data to evaluate the constraint gradients on.\n\n        Returns:\n            Jacobian evaluation J_g(x) for each of the constraints as a list of dataframes.\n        \"\"\"\n        return [c.jacobian(data) for c in self.constraints]\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Check if all constraints are satisfied.\n\n        Args:\n            data: Data to evaluate the constraints on.\n\n        Returns:\n            Series of booleans indicating if all constraints are satisfied.\n        \"\"\"\n        return pd.concat([c.satisfied(data) for c in self.constraints], axis=1).all(\n            axis=1\n        )\n\n    def to_config(self) -&gt; List[Dict]:\n        return [obj.to_config() for obj in self.constraints]\n\n    def get(self, types) -&gt; \"Constraints\":\n        \"\"\"Get all constraints of the given type(s).\"\"\"\n        return Constraints([c for c in self if isinstance(c, types)])\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints.__call__","title":"<code>__call__(data)</code>","text":"<p>Numerically evaluate all constraints.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to evaluate the constraints on.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Constraint evaluation g(x) for each of the constraints.</p> Source code in <code>opti/constraint.py</code> <pre><code>def __call__(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Numerically evaluate all constraints.\n\n    Args:\n        data: Data to evaluate the constraints on.\n\n    Returns:\n        Constraint evaluation g(x) for each of the constraints.\n    \"\"\"\n    return pd.concat([c(data) for c in self.constraints], axis=1)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints.get","title":"<code>get(types)</code>","text":"<p>Get all constraints of the given type(s).</p> Source code in <code>opti/constraint.py</code> <pre><code>def get(self, types) -&gt; \"Constraints\":\n    \"\"\"Get all constraints of the given type(s).\"\"\"\n    return Constraints([c for c in self if isinstance(c, types)])\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints.jacobian","title":"<code>jacobian(data)</code>","text":"<p>Numerically evaluate all constraint gradients.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to evaluate the constraint gradients on.</p> required <p>Returns:</p> Type Description <code>List</code> <p>Jacobian evaluation J_g(x) for each of the constraints as a list of dataframes.</p> Source code in <code>opti/constraint.py</code> <pre><code>def jacobian(self, data: pd.DataFrame) -&gt; List:\n    \"\"\"Numerically evaluate all constraint gradients.\n\n    Args:\n        data: Data to evaluate the constraint gradients on.\n\n    Returns:\n        Jacobian evaluation J_g(x) for each of the constraints as a list of dataframes.\n    \"\"\"\n    return [c.jacobian(data) for c in self.constraints]\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.Constraints.satisfied","title":"<code>satisfied(data)</code>","text":"<p>Check if all constraints are satisfied.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to evaluate the constraints on.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series of booleans indicating if all constraints are satisfied.</p> Source code in <code>opti/constraint.py</code> <pre><code>def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Check if all constraints are satisfied.\n\n    Args:\n        data: Data to evaluate the constraints on.\n\n    Returns:\n        Series of booleans indicating if all constraints are satisfied.\n    \"\"\"\n    return pd.concat([c.satisfied(data) for c in self.constraints], axis=1).all(\n        axis=1\n    )\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearEquality","title":"<code>LinearEquality</code>","text":"<p>               Bases: <code>Constraint</code></p> Source code in <code>opti/constraint.py</code> <pre><code>class LinearEquality(Constraint):\n    def __init__(\n        self,\n        names: List[str],\n        lhs: Union[float, List[float], np.ndarray] = 1,\n        rhs: float = 0,\n    ):\n        \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'.\n\n        Args:\n            names: Parameter names that the constraint works on.\n            lhs: Left-hand side / coefficients of the constraint.\n            rhs: Right-hand side of the constraint.\n\n        Examples:\n            A mixture constraint where A, B and C need to add up to 100 can be defined as\n            ```\n            LinearEquality([\"A\", \"B\", \"C\"], rhs=100)\n            ```\n            If the coefficients of A, B and C are not 1 they are passed explicitly.\n            ```\n            LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n            ```\n        \"\"\"\n        self.names = names\n        if np.isscalar(lhs):\n            self.lhs = lhs * np.ones(len(names))\n        else:\n            self.lhs = np.asarray(lhs)\n        if self.lhs.shape != (len(names),):\n            raise ValueError(\"Number of parameters and coefficients/lhs don't match.\")\n        self.rhs = rhs\n        self.is_equality = True\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        return (data[self.names] @ self.lhs - self.rhs) / np.linalg.norm(self.lhs)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            np.tile(self.lhs / np.linalg.norm(self.lhs), [data.shape[0], 1]),\n            columns=[\"dg/d\" + name for name in self.names],\n        )\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return pd.Series(np.isclose(self(data), 0), index=data.index)\n\n    def __repr__(self):\n        return (\n            f\"LinearEquality(names={self.names}, lhs={list(self.lhs)}, rhs={self.rhs})\"\n        )\n\n    def to_config(self) -&gt; Dict:\n        return dict(\n            type=\"linear-equality\",\n            names=self.names,\n            lhs=self.lhs.tolist(),\n            rhs=self.rhs,\n        )\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearEquality.__init__","title":"<code>__init__(names, lhs=1, rhs=0)</code>","text":"<p>Linear / affine inequality of the form 'lhs * x == rhs'.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>Parameter names that the constraint works on.</p> required <code>lhs</code> <code>Union[float, List[float], ndarray]</code> <p>Left-hand side / coefficients of the constraint.</p> <code>1</code> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint.</p> <code>0</code> <p>Examples:</p> <p>A mixture constraint where A, B and C need to add up to 100 can be defined as <pre><code>LinearEquality([\"A\", \"B\", \"C\"], rhs=100)\n</code></pre> If the coefficients of A, B and C are not 1 they are passed explicitly. <pre><code>LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(\n    self,\n    names: List[str],\n    lhs: Union[float, List[float], np.ndarray] = 1,\n    rhs: float = 0,\n):\n    \"\"\"Linear / affine inequality of the form 'lhs * x == rhs'.\n\n    Args:\n        names: Parameter names that the constraint works on.\n        lhs: Left-hand side / coefficients of the constraint.\n        rhs: Right-hand side of the constraint.\n\n    Examples:\n        A mixture constraint where A, B and C need to add up to 100 can be defined as\n        ```\n        LinearEquality([\"A\", \"B\", \"C\"], rhs=100)\n        ```\n        If the coefficients of A, B and C are not 1 they are passed explicitly.\n        ```\n        LinearEquality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n        ```\n    \"\"\"\n    self.names = names\n    if np.isscalar(lhs):\n        self.lhs = lhs * np.ones(len(names))\n    else:\n        self.lhs = np.asarray(lhs)\n    if self.lhs.shape != (len(names),):\n        raise ValueError(\"Number of parameters and coefficients/lhs don't match.\")\n    self.rhs = rhs\n    self.is_equality = True\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearInequality","title":"<code>LinearInequality</code>","text":"<p>               Bases: <code>Constraint</code></p> Source code in <code>opti/constraint.py</code> <pre><code>class LinearInequality(Constraint):\n    def __init__(\n        self,\n        names: List[str],\n        lhs: Union[float, List[float], np.ndarray] = 1,\n        rhs: float = 0,\n    ):\n        \"\"\"Linear / affine inequality of the form 'lhs * x &lt;= rhs'.\n\n        Args:\n            names: Parameter names that the constraint works on.\n            lhs: Left-hand side / coefficients of the constraint.\n            rhs: Right-hand side of the constraint.\n\n        Examples:\n            A mixture constraint where the values of A, B and C may not exceed 100 can be defined as\n            ```\n            LinearInequality([\"A\", \"B\", \"C\"], rhs=100)\n            ```\n            If the coefficients are not 1, they need to be passed explicitly.\n            ```\n            LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n            ```\n            Inequalities are alway of the form g(x) &lt;= 0. To define a the constraint g(x) &gt;=0 0, both `lhs` and `rhs` need to be multiplied by -1.\n            ```\n            LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100)\n            LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100)\n            ```\n        \"\"\"\n        self.names = names\n        if np.isscalar(lhs):\n            self.lhs = lhs * np.ones(len(names))\n        else:\n            self.lhs = np.asarray(lhs)\n        if self.lhs.shape != (len(names),):\n            raise ValueError(\"Number of parameters and coefficients/lhs don't match.\")\n        self.rhs = rhs\n        self.is_equality = False\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        return (data[self.names] @ self.lhs - self.rhs) / np.linalg.norm(self.lhs)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            np.tile(self.lhs / np.linalg.norm(self.lhs), [data.shape[0], 1]),\n            columns=[\"dg/d\" + name for name in self.names],\n        )\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return self(data) &lt;= 0\n\n    def __repr__(self):\n        return f\"LinearInequality(names={self.names}, lhs={list(self.lhs)}, rhs={self.rhs})\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(\n            type=\"linear-inequality\",\n            names=self.names,\n            lhs=self.lhs.tolist(),\n            rhs=self.rhs,\n        )\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.LinearInequality.__init__","title":"<code>__init__(names, lhs=1, rhs=0)</code>","text":"<p>Linear / affine inequality of the form 'lhs * x &lt;= rhs'.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>Parameter names that the constraint works on.</p> required <code>lhs</code> <code>Union[float, List[float], ndarray]</code> <p>Left-hand side / coefficients of the constraint.</p> <code>1</code> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint.</p> <code>0</code> <p>Examples:</p> <p>A mixture constraint where the values of A, B and C may not exceed 100 can be defined as <pre><code>LinearInequality([\"A\", \"B\", \"C\"], rhs=100)\n</code></pre> If the coefficients are not 1, they need to be passed explicitly. <pre><code>LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n</code></pre> Inequalities are alway of the form g(x) &lt;= 0. To define a the constraint g(x) &gt;=0 0, both <code>lhs</code> and <code>rhs</code> need to be multiplied by -1. <pre><code>LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100)\nLinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100)\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(\n    self,\n    names: List[str],\n    lhs: Union[float, List[float], np.ndarray] = 1,\n    rhs: float = 0,\n):\n    \"\"\"Linear / affine inequality of the form 'lhs * x &lt;= rhs'.\n\n    Args:\n        names: Parameter names that the constraint works on.\n        lhs: Left-hand side / coefficients of the constraint.\n        rhs: Right-hand side of the constraint.\n\n    Examples:\n        A mixture constraint where the values of A, B and C may not exceed 100 can be defined as\n        ```\n        LinearInequality([\"A\", \"B\", \"C\"], rhs=100)\n        ```\n        If the coefficients are not 1, they need to be passed explicitly.\n        ```\n        LinearInequality([\"A\", \"B\", \"C\"], lhs=[10, 2, 5], rhs=100)\n        ```\n        Inequalities are alway of the form g(x) &lt;= 0. To define a the constraint g(x) &gt;=0 0, both `lhs` and `rhs` need to be multiplied by -1.\n        ```\n        LinearInequality([\"A\", \"B\", \"C\"], lhs=-1, rhs=-100)\n        LinearInequality([\"A\", \"B\", \"C\"], lhs=[-10, -2, -5], rhs=-100)\n        ```\n    \"\"\"\n    self.names = names\n    if np.isscalar(lhs):\n        self.lhs = lhs * np.ones(len(names))\n    else:\n        self.lhs = np.asarray(lhs)\n    if self.lhs.shape != (len(names),):\n        raise ValueError(\"Number of parameters and coefficients/lhs don't match.\")\n    self.rhs = rhs\n    self.is_equality = False\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NChooseK","title":"<code>NChooseK</code>","text":"<p>               Bases: <code>Constraint</code></p> Source code in <code>opti/constraint.py</code> <pre><code>class NChooseK(Constraint):\n    def __init__(self, names: List[str], max_active: int):\n        \"\"\"Only k out of n values are allowed to take nonzero values.\n\n        Args:\n            names: Parameter names that the constraint works on.\n            max_active: Maximium number of non-zero parameter values.\n\n        Examples:\n            A choice of 2 or less from A, B, C, D or E can be defined as\n            ```\n            NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2)\n            ```\n        \"\"\"\n        self.names = names\n        self.max_active = max_active\n        self.is_equality = False\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        x = np.abs(data[self.names].values)\n        num_zeros = x.shape[1] - self.max_active\n        violation = np.apply_along_axis(\n            func1d=lambda r: sum(sorted(r)[:num_zeros]), axis=1, arr=x\n        )\n        return pd.Series(violation, index=data.index)\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return pd.Series(self(data) &lt;= 0, index=data.index)\n\n    def __repr__(self):\n        return f\"NChooseK(names={self.names}, max_active={self.max_active})\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(type=\"n-choose-k\", names=self.names, max_active=self.max_active)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NChooseK.__init__","title":"<code>__init__(names, max_active)</code>","text":"<p>Only k out of n values are allowed to take nonzero values.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>Parameter names that the constraint works on.</p> required <code>max_active</code> <code>int</code> <p>Maximium number of non-zero parameter values.</p> required <p>Examples:</p> <p>A choice of 2 or less from A, B, C, D or E can be defined as <pre><code>NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2)\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(self, names: List[str], max_active: int):\n    \"\"\"Only k out of n values are allowed to take nonzero values.\n\n    Args:\n        names: Parameter names that the constraint works on.\n        max_active: Maximium number of non-zero parameter values.\n\n    Examples:\n        A choice of 2 or less from A, B, C, D or E can be defined as\n        ```\n        NChooseK([\"A\", \"B\", \"C\", \"D\", \"E\"], max_active=2)\n        ```\n    \"\"\"\n    self.names = names\n    self.max_active = max_active\n    self.is_equality = False\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality","title":"<code>NonlinearEquality</code>","text":"<p>               Bases: <code>Constraint</code></p> Source code in <code>opti/constraint.py</code> <pre><code>class NonlinearEquality(Constraint):\n    def __init__(\n        self,\n        expression: str,\n        jacobian: Optional[str] = None,\n        names: Optional[List[str]] = None,\n    ):\n        \"\"\"Equality of the form 'expression == 0'.\n\n        Args:\n            expression: Mathematical expression that can be evaluated by `pandas.eval`.\n            jacobian: List of mathematical expressions that can be evaluated by `pandas.eval`.\n                The i-th expression should correspond to the partial derivative with respect to\n                the i-th variable. If `names` attribute is provided, the order of the variables should\n                correspond to the order of the variables in `names`. Optional.\n            names: List of variable names present in `expression`. Optional.\n\n        Examples:\n            You can pass any expression that can be evaluated by `pd.eval`.\n            To define x1**2 + x2**2 = 1, use\n            ```\n            NonlinearEquality(\"x1**2 + x2**2 - 1\")\n            ```\n            Standard mathematical operators are supported.\n            ```\n            NonlinearEquality(\"sin(A) / (exp(B) - 1)\")\n            ```\n            Parameter names with special characters or spaces need to be enclosed in backticks.\n            ```\n            NonlinearEquality(\"1 - `weight A` / `weight B`\")\n            ```\n        \"\"\"\n        self.expression = expression\n        self.is_equality = True\n        self.jacobian_expression = jacobian\n        self.names = names\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        return data.eval(self.expression)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\n        if self.jacobian_expression is not None:\n            res = data.eval(self.jacobian_expression)\n            for i, col in enumerate(res):\n                if not hasattr(col, \"__iter__\"):\n                    res[i] = pd.Series(np.repeat(col, data.shape[0]))\n\n            if self.names is not None:\n                return pd.DataFrame(\n                    res, index=[\"dg/d\" + name for name in self.names]\n                ).transpose()\n            else:\n                return pd.DataFrame(\n                    res, index=[f\"dg/dx{i}\" for i in range(data.shape[1])]\n                ).transpose()\n\n        return super().jacobian(data)\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return pd.Series(np.isclose(self(data), 0), index=data.index)\n\n    def __repr__(self):\n        return f\"NonlinearEquality('{self.expression}')\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(type=\"nonlinear-equality\", expression=self.expression)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearEquality.__init__","title":"<code>__init__(expression, jacobian=None, names=None)</code>","text":"<p>Equality of the form 'expression == 0'.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Mathematical expression that can be evaluated by <code>pandas.eval</code>.</p> required <code>jacobian</code> <code>Optional[str]</code> <p>List of mathematical expressions that can be evaluated by <code>pandas.eval</code>. The i-th expression should correspond to the partial derivative with respect to the i-th variable. If <code>names</code> attribute is provided, the order of the variables should correspond to the order of the variables in <code>names</code>. Optional.</p> <code>None</code> <code>names</code> <code>Optional[List[str]]</code> <p>List of variable names present in <code>expression</code>. Optional.</p> <code>None</code> <p>Examples:</p> <p>You can pass any expression that can be evaluated by <code>pd.eval</code>. To define x12 + x22 = 1, use <pre><code>NonlinearEquality(\"x1**2 + x2**2 - 1\")\n</code></pre> Standard mathematical operators are supported. <pre><code>NonlinearEquality(\"sin(A) / (exp(B) - 1)\")\n</code></pre> Parameter names with special characters or spaces need to be enclosed in backticks. <pre><code>NonlinearEquality(\"1 - `weight A` / `weight B`\")\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(\n    self,\n    expression: str,\n    jacobian: Optional[str] = None,\n    names: Optional[List[str]] = None,\n):\n    \"\"\"Equality of the form 'expression == 0'.\n\n    Args:\n        expression: Mathematical expression that can be evaluated by `pandas.eval`.\n        jacobian: List of mathematical expressions that can be evaluated by `pandas.eval`.\n            The i-th expression should correspond to the partial derivative with respect to\n            the i-th variable. If `names` attribute is provided, the order of the variables should\n            correspond to the order of the variables in `names`. Optional.\n        names: List of variable names present in `expression`. Optional.\n\n    Examples:\n        You can pass any expression that can be evaluated by `pd.eval`.\n        To define x1**2 + x2**2 = 1, use\n        ```\n        NonlinearEquality(\"x1**2 + x2**2 - 1\")\n        ```\n        Standard mathematical operators are supported.\n        ```\n        NonlinearEquality(\"sin(A) / (exp(B) - 1)\")\n        ```\n        Parameter names with special characters or spaces need to be enclosed in backticks.\n        ```\n        NonlinearEquality(\"1 - `weight A` / `weight B`\")\n        ```\n    \"\"\"\n    self.expression = expression\n    self.is_equality = True\n    self.jacobian_expression = jacobian\n    self.names = names\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality","title":"<code>NonlinearInequality</code>","text":"<p>               Bases: <code>Constraint</code></p> Source code in <code>opti/constraint.py</code> <pre><code>class NonlinearInequality(Constraint):\n    def __init__(\n        self,\n        expression: str,\n        jacobian: Optional[str] = None,\n        names: Optional[List[str]] = None,\n    ):\n        \"\"\"Inequality of the form 'expression &lt;= 0'.\n\n        Args:\n            expression: Mathematical expression that can be evaluated by `pandas.eval`.\n            jacobian: List of mathematical expressions that can be evaluated by `pandas.eval`.\n                The i-th expression should correspond to the partial derivative with respect to\n                the i-th variable. If `names` attribute is provided, the order of the variables should\n                correspond to the order of the variables in `names`. Optional.\n            names: List of variable names present in `expression`. Optional.\n\n        Examples:\n            You can pass any expression that can be evaluated by `pd.eval`.\n            To define x1**2 + x2**2 &lt; 1, use\n            ```\n            NonlinearInequality(\"x1**2 + x2**2 - 1\")\n            ```\n            Standard mathematical operators are supported.\n            ```\n            NonlinearInequality(\"sin(A) / (exp(B) - 1)\")\n            ```\n            Parameter names with special characters or spaces need to be enclosed in backticks.\n            ```\n            NonlinearInequality(\"1 - `weight A` / `weight B`\")\n            ```\n        \"\"\"\n        self.expression = expression\n        self.is_equality = False\n        self.jacobian_expression = jacobian\n        self.names = names\n\n    def __call__(self, data: pd.DataFrame) -&gt; pd.Series:\n        return data.eval(self.expression)\n\n    def jacobian(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\n        if self.jacobian_expression is not None:\n            res = data.eval(self.jacobian_expression)\n            for i, col in enumerate(res):\n                if not hasattr(col, \"__iter__\"):\n                    res[i] = pd.Series(np.repeat(col, data.shape[0]))\n\n            if self.names is not None:\n                return pd.DataFrame(\n                    res, index=[\"dg/d\" + name for name in self.names]\n                ).transpose()\n            else:\n                return pd.DataFrame(\n                    res, index=[f\"dg/dx{i}\" for i in range(data.shape[1])]\n                ).transpose()\n\n        return super().jacobian(data)\n\n    def satisfied(self, data: pd.DataFrame) -&gt; pd.Series:\n        return self(data) &lt;= 0\n\n    def __repr__(self):\n        return f\"NonlinearInequality('{self.expression}')\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(type=\"nonlinear-inequality\", expression=self.expression)\n</code></pre>"},{"location":"ref-constraint/#opti.constraint.NonlinearInequality.__init__","title":"<code>__init__(expression, jacobian=None, names=None)</code>","text":"<p>Inequality of the form 'expression &lt;= 0'.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Mathematical expression that can be evaluated by <code>pandas.eval</code>.</p> required <code>jacobian</code> <code>Optional[str]</code> <p>List of mathematical expressions that can be evaluated by <code>pandas.eval</code>. The i-th expression should correspond to the partial derivative with respect to the i-th variable. If <code>names</code> attribute is provided, the order of the variables should correspond to the order of the variables in <code>names</code>. Optional.</p> <code>None</code> <code>names</code> <code>Optional[List[str]]</code> <p>List of variable names present in <code>expression</code>. Optional.</p> <code>None</code> <p>Examples:</p> <p>You can pass any expression that can be evaluated by <code>pd.eval</code>. To define x12 + x22 &lt; 1, use <pre><code>NonlinearInequality(\"x1**2 + x2**2 - 1\")\n</code></pre> Standard mathematical operators are supported. <pre><code>NonlinearInequality(\"sin(A) / (exp(B) - 1)\")\n</code></pre> Parameter names with special characters or spaces need to be enclosed in backticks. <pre><code>NonlinearInequality(\"1 - `weight A` / `weight B`\")\n</code></pre></p> Source code in <code>opti/constraint.py</code> <pre><code>def __init__(\n    self,\n    expression: str,\n    jacobian: Optional[str] = None,\n    names: Optional[List[str]] = None,\n):\n    \"\"\"Inequality of the form 'expression &lt;= 0'.\n\n    Args:\n        expression: Mathematical expression that can be evaluated by `pandas.eval`.\n        jacobian: List of mathematical expressions that can be evaluated by `pandas.eval`.\n            The i-th expression should correspond to the partial derivative with respect to\n            the i-th variable. If `names` attribute is provided, the order of the variables should\n            correspond to the order of the variables in `names`. Optional.\n        names: List of variable names present in `expression`. Optional.\n\n    Examples:\n        You can pass any expression that can be evaluated by `pd.eval`.\n        To define x1**2 + x2**2 &lt; 1, use\n        ```\n        NonlinearInequality(\"x1**2 + x2**2 - 1\")\n        ```\n        Standard mathematical operators are supported.\n        ```\n        NonlinearInequality(\"sin(A) / (exp(B) - 1)\")\n        ```\n        Parameter names with special characters or spaces need to be enclosed in backticks.\n        ```\n        NonlinearInequality(\"1 - `weight A` / `weight B`\")\n        ```\n    \"\"\"\n    self.expression = expression\n    self.is_equality = False\n    self.jacobian_expression = jacobian\n    self.names = names\n</code></pre>"},{"location":"ref-metric/","title":"Metrics","text":""},{"location":"ref-metric/#opti.metric.crowding_distance","title":"<code>crowding_distance(A)</code>","text":"<p>Crowding distance indicator.</p> <p>The crowding distance is defined for each point in a Pareto front as the average side length of the cuboid formed by the neighbouring points.</p> Reference <p>Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array</code> <p>Set of points representing a Pareto front. Pareto-efficiency is assumed but not checked.</p> required <p>Returns:</p> Name Type Description <code>array</code> <p>Crowding distance indicator for each point in the front.</p> Source code in <code>opti/metric.py</code> <pre><code>def crowding_distance(A):\n    \"\"\"Crowding distance indicator.\n\n    The crowding distance is defined for each point in a Pareto front as the average\n    side length of the cuboid formed by the neighbouring points.\n\n    Reference:\n        [Kalyanmoy Deb (2000) A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II](https://link.springer.com/chapter/10.1007/3-540-45356-3_83)\n\n    Args:\n        A (2D-array): Set of points representing a Pareto front.\n            Pareto-efficiency is assumed but not checked.\n\n    Returns:\n        array: Crowding distance indicator for each point in the front.\n    \"\"\"\n    A = pareto_front(A)\n    N, m = A.shape\n\n    # no crowding distance for 2 points\n    if N &lt;= 2:\n        return np.full(N, np.inf)\n\n    # sort points along each objective\n    sort = np.argsort(A, axis=0)\n    A = A[sort, np.arange(m)]\n\n    # normalize all objectives\n    norm = np.max(A, axis=0) - np.min(A, axis=0)\n    A = A / norm\n    A[:, norm == 0] = 0  # handle min = max\n\n    # distance to previous and to next point along each objective\n    d = np.diff(A, axis=0)\n    inf = np.full((1, m), np.inf)\n    d0 = np.concatenate([inf, d])\n    d1 = np.concatenate([d, inf])\n\n    # TODO: handle cases with duplicate objective values leading to 0 distances\n\n    # cuboid side length = distance between previous and next point\n    unsort = np.argsort(sort, axis=0)\n    cuboid = d0[unsort, np.arange(m)] + d1[unsort, np.arange(m)]\n    return np.mean(cuboid, axis=1)\n</code></pre>"},{"location":"ref-metric/#opti.metric.generational_distance","title":"<code>generational_distance(A, R, p=1, clip=True)</code>","text":"<p>Generational distance indicator.</p> <p>The generational distance (GD) is defined as the average distance of points in the approximate front A to a reference front R. .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A where d(a, R) is the euclidean distance of point a to the reference front, N_A is the number of points in A. GD is a measure of convergence.</p> Reference <p>David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array</code> <p>Set of points representing an approximate Pareto front.</p> required <code>R</code> <code>2D-array</code> <p>Set of points representing a reference Pareto front.</p> required <code>p</code> <code>int</code> <p>Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average.</p> <code>1</code> <code>clip</code> <code>bool</code> <p>Flag for using the modfied generational distance, which prevents negative values for points that are non-dominated by the reference front.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Generational distance indicator.</p> Source code in <code>opti/metric.py</code> <pre><code>def generational_distance(\n    A: np.ndarray, R: np.ndarray, p: float = 1, clip: bool = True\n) -&gt; float:\n    r\"\"\"Generational distance indicator.\n\n    The generational distance (GD) is defined as the average distance of points in the\n    approximate front A to a reference front R.\n    .. math:: \\mathrm{GD}(A, R) = (\\sum\\limits_{a \\in A} d(a, R)^p)^{1/p} / N_A\n    where d(a, R) is the euclidean distance of point a to the reference front, N_A is\n    the number of points in A. GD is a measure of convergence.\n\n    Reference:\n        [David Van Veldhuizen+ (2000) Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art](http://dx.doi.org/10.1162/106365600568158)\n\n    Args:\n        A (2D-array): Set of points representing an approximate Pareto front.\n        R (2D-array): Set of points representing a reference Pareto front.\n        p (int, optional): Order of the p-norm for averaging over distances.\n            Defaults to 1, yielding the standard average.\n        clip (bool, optional): Flag for using the modfied generational distance, which\n            prevents negative values for points that are non-dominated by the reference\n            front.\n\n    Returns:\n        float: Generational distance indicator.\n    \"\"\"\n    A = pareto_front(A)\n    distances = A[:, np.newaxis] - R[np.newaxis]\n    if clip:\n        distances = distances.clip(0, None)\n    distances = np.linalg.norm(distances, axis=2).min(axis=1)\n    return np.linalg.norm(distances, p) / len(A)\n</code></pre>"},{"location":"ref-metric/#opti.metric.inverted_generational_distance","title":"<code>inverted_generational_distance(A, R, p=1)</code>","text":"<p>Inverted generational distance indicator.</p> <p>The inverted generational distance (IGD) is defined as the average distance of points in the reference front R to an approximate front A. IGD is a measure of convergence, spread and distribution of the approximate front.</p> Reference <p>CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array</code> <p>Set of points representing an approximate Pareto front.</p> required <code>R</code> <code>2D-array</code> <p>Set of points representing a reference Pareto front.</p> required <code>p</code> <code>int</code> <p>Order of the p-norm for averaging over distances. Defaults to 1, yielding the standard average.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Inverted generational distance indicator.</p> Source code in <code>opti/metric.py</code> <pre><code>def inverted_generational_distance(A: np.ndarray, R: np.ndarray, p: float = 1) -&gt; float:\n    \"\"\"Inverted generational distance indicator.\n\n    The inverted generational distance (IGD) is defined as the average distance of\n    points in the reference front R to an approximate front A.\n    IGD is a measure of convergence, spread and distribution of the approximate front.\n\n    Reference:\n        [CA. Coello Coello+ (2004) A study of the parallelization of a coevolutionary multi-objective evolutionary algorithm](https://doi.org/10.1007/978-3-540-24694-7_71)\n\n    Args:\n        A (2D-array): Set of points representing an approximate Pareto front.\n        R (2D-array): Set of points representing a reference Pareto front.\n        p (int, optional): Order of the p-norm for averaging over distances.\n            Defaults to 1, yielding the standard average.\n\n    Returns:\n        float: Inverted generational distance indicator.\n    \"\"\"\n    return generational_distance(R, A, p, clip=False)\n</code></pre>"},{"location":"ref-metric/#opti.metric.is_pareto_efficient","title":"<code>is_pareto_efficient(A)</code>","text":"<p>Find the Pareto-efficient points in a set of objective vectors.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array, shape=(samples, dimension</code> <p>Objective vectors.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>1D-array of bools: Boolean mask for the Pareto efficient points in A.</p> Source code in <code>opti/metric.py</code> <pre><code>def is_pareto_efficient(A: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Find the Pareto-efficient points in a set of objective vectors.\n\n    Args:\n        A (2D-array, shape=(samples, dimension)): Objective vectors.\n\n    Returns:\n        1D-array of bools: Boolean mask for the Pareto efficient points in A.\n    \"\"\"\n    efficient = np.ones(len(A), dtype=bool)\n    idx = np.arange(len(A))\n    for i, a in enumerate(A):\n        if not efficient[i]:\n            continue\n        # set all *other* efficent points to False, if they are not strictly better in at least one objective\n        efficient[efficient] = np.any(A[efficient] &lt; a, axis=1) | (i == idx[efficient])\n    return efficient\n</code></pre>"},{"location":"ref-metric/#opti.metric.pareto_front","title":"<code>pareto_front(A)</code>","text":"<p>Find the Pareto-efficient points in a set of objective vectors.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>2D-array, shape=(samples, dimension</code> <p>Objective vectors.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2D-array: Pareto efficient points in A.</p> Source code in <code>opti/metric.py</code> <pre><code>def pareto_front(A: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Find the Pareto-efficient points in a set of objective vectors.\n\n    Args:\n        A (2D-array, shape=(samples, dimension)): Objective vectors.\n\n    Returns:\n        2D-array: Pareto efficient points in A.\n    \"\"\"\n    return A[is_pareto_efficient(A)]\n</code></pre>"},{"location":"ref-model/","title":"Models","text":""},{"location":"ref-model/#opti.model.CustomModel","title":"<code>CustomModel</code>","text":"<p>               Bases: <code>Model</code></p> Source code in <code>opti/model.py</code> <pre><code>class CustomModel(Model):\n    def __init__(self, names: List[str], f: Callable):\n        \"\"\"Custom model for arbitrary functions.\n\n        Args:\n            names: names of the modeled outputs.\n            f: Callable that takes a DataFrame of inputs and returns a DataFrame of outputs.\n        \"\"\"\n        super().__init__(names)\n        self.f = f\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        return self.f(df)\n\n    def __repr__(self):\n        return f\"CustomModel({self.names}, f={self.f})\"\n</code></pre>"},{"location":"ref-model/#opti.model.CustomModel.__init__","title":"<code>__init__(names, f)</code>","text":"<p>Custom model for arbitrary functions.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>names of the modeled outputs.</p> required <code>f</code> <code>Callable</code> <p>Callable that takes a DataFrame of inputs and returns a DataFrame of outputs.</p> required Source code in <code>opti/model.py</code> <pre><code>def __init__(self, names: List[str], f: Callable):\n    \"\"\"Custom model for arbitrary functions.\n\n    Args:\n        names: names of the modeled outputs.\n        f: Callable that takes a DataFrame of inputs and returns a DataFrame of outputs.\n    \"\"\"\n    super().__init__(names)\n    self.f = f\n</code></pre>"},{"location":"ref-model/#opti.model.LinearModel","title":"<code>LinearModel</code>","text":"<p>               Bases: <code>Model</code></p> Source code in <code>opti/model.py</code> <pre><code>class LinearModel(Model):\n    def __init__(\n        self,\n        names: List[str],\n        coefficients: Dict[str, float],\n        offset: float = 0,\n    ):\n        \"\"\"Model to compute an output as a linear/affine function of the inputs, $y = ax + b$.\n\n        Args:\n            names: name of the modeled output.\n            coefficients: dictionary mapping input name to the corresponding coefficient a.\n            offset: the offset b.\n        \"\"\"\n        super().__init__(names)\n        if len(names) &gt; 1:\n            raise ValueError(\"LinearModel can only describe a single output.\")\n        self.coefficients = coefficients\n        self.offset = offset\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        coefficients_values = list(self.coefficients.values())\n        coefficients_names = list(self.coefficients.keys())\n        y = df[coefficients_names].to_numpy() @ coefficients_values + self.offset\n        return pd.DataFrame(y, columns=self.names)\n\n    def __repr__(self):\n        return f\"LinearModel({self.names}, coefficients={self.coefficients}, offset={self.offset})\"\n\n    def to_config(self) -&gt; Dict:\n        return dict(\n            type=\"linear-model\",\n            names=self.names,\n            coefficients=self.coefficients,\n            offset=self.offset,\n        )\n</code></pre>"},{"location":"ref-model/#opti.model.LinearModel.__init__","title":"<code>__init__(names, coefficients, offset=0)</code>","text":"<p>Model to compute an output as a linear/affine function of the inputs, \\(y = ax + b\\).</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>name of the modeled output.</p> required <code>coefficients</code> <code>Dict[str, float]</code> <p>dictionary mapping input name to the corresponding coefficient a.</p> required <code>offset</code> <code>float</code> <p>the offset b.</p> <code>0</code> Source code in <code>opti/model.py</code> <pre><code>def __init__(\n    self,\n    names: List[str],\n    coefficients: Dict[str, float],\n    offset: float = 0,\n):\n    \"\"\"Model to compute an output as a linear/affine function of the inputs, $y = ax + b$.\n\n    Args:\n        names: name of the modeled output.\n        coefficients: dictionary mapping input name to the corresponding coefficient a.\n        offset: the offset b.\n    \"\"\"\n    super().__init__(names)\n    if len(names) &gt; 1:\n        raise ValueError(\"LinearModel can only describe a single output.\")\n    self.coefficients = coefficients\n    self.offset = offset\n</code></pre>"},{"location":"ref-model/#opti.model.Model","title":"<code>Model</code>","text":"Source code in <code>opti/model.py</code> <pre><code>class Model:\n    def __init__(self, names: List[str]):\n        \"\"\"Base class for models of outputs as function of inputs.\n\n        Args:\n            names: names of the modeled outputs.\n        \"\"\"\n        for name in names:\n            if not isinstance(name, str):\n                TypeError(\"Model: names must be a list of strings.\")\n        self.names = list(names)\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Evaluate the objective values for a given DataFrame.\"\"\"\n        raise NotImplementedError\n\n    def to_config(self) -&gt; None:\n        \"\"\"Return a json-serializable dictionary of the objective.\"\"\"\n        pass  # non-serializable models should be ommited without raising an error\n</code></pre>"},{"location":"ref-model/#opti.model.Model.__call__","title":"<code>__call__(df)</code>","text":"<p>Evaluate the objective values for a given DataFrame.</p> Source code in <code>opti/model.py</code> <pre><code>def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Evaluate the objective values for a given DataFrame.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-model/#opti.model.Model.__init__","title":"<code>__init__(names)</code>","text":"<p>Base class for models of outputs as function of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>names of the modeled outputs.</p> required Source code in <code>opti/model.py</code> <pre><code>def __init__(self, names: List[str]):\n    \"\"\"Base class for models of outputs as function of inputs.\n\n    Args:\n        names: names of the modeled outputs.\n    \"\"\"\n    for name in names:\n        if not isinstance(name, str):\n            TypeError(\"Model: names must be a list of strings.\")\n    self.names = list(names)\n</code></pre>"},{"location":"ref-model/#opti.model.Model.to_config","title":"<code>to_config()</code>","text":"<p>Return a json-serializable dictionary of the objective.</p> Source code in <code>opti/model.py</code> <pre><code>def to_config(self) -&gt; None:\n    \"\"\"Return a json-serializable dictionary of the objective.\"\"\"\n    pass  # non-serializable models should be ommited without raising an error\n</code></pre>"},{"location":"ref-model/#opti.model.Models","title":"<code>Models</code>","text":"Source code in <code>opti/model.py</code> <pre><code>class Models:\n    def __init__(self, models: Union[List[Model], List[Dict]]):\n        \"\"\"Container for models.\n\n        Args:\n            models: list of models or model configurations.\n        \"\"\"\n        _models = []\n        for m in models:\n            if isinstance(m, Model):\n                _models.append(m)\n            else:\n                _models.append(make_model(**m))\n        self.models = _models\n\n    def __call__(self, y: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.concat([model(y) for model in self.models], axis=1)\n\n    def __repr__(self):\n        return \"Models(\\n\" + pprint.pformat(self.models) + \"\\n)\"\n\n    def __iter__(self):\n        return iter(self.models)\n\n    def __len__(self):\n        return len(self.models)\n\n    def __getitem__(self, i: int) -&gt; Model:\n        return self.models[i]\n\n    @property\n    def names(self):\n        names = []\n        for model in self.models:\n            names += model.names\n        return names\n\n    def to_config(self) -&gt; List[Dict]:\n        return [\n            model.to_config() for model in self.models if model.to_config() is not None\n        ]\n</code></pre>"},{"location":"ref-model/#opti.model.Models.__init__","title":"<code>__init__(models)</code>","text":"<p>Container for models.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Union[List[Model], List[Dict]]</code> <p>list of models or model configurations.</p> required Source code in <code>opti/model.py</code> <pre><code>def __init__(self, models: Union[List[Model], List[Dict]]):\n    \"\"\"Container for models.\n\n    Args:\n        models: list of models or model configurations.\n    \"\"\"\n    _models = []\n    for m in models:\n        if isinstance(m, Model):\n            _models.append(m)\n        else:\n            _models.append(make_model(**m))\n    self.models = _models\n</code></pre>"},{"location":"ref-model/#opti.model.make_model","title":"<code>make_model(type, **kwargs)</code>","text":"<p>Make a model object from a configuration dict.</p> Source code in <code>opti/model.py</code> <pre><code>def make_model(type, **kwargs):\n    \"\"\"Make a model object from a configuration dict.\"\"\"\n    t = type.lower()\n    if t == \"linear-model\":\n        return LinearModel(**kwargs)\n    raise ValueError(f\"Unknown model type: {t}.\")\n</code></pre>"},{"location":"ref-objective/","title":"Objectives","text":""},{"location":"ref-objective/#opti.objective.CloseToTarget","title":"<code>CloseToTarget</code>","text":"<p>               Bases: <code>Objective</code></p> Source code in <code>opti/objective.py</code> <pre><code>class CloseToTarget(Objective):\n    def __init__(\n        self,\n        name: str,\n        target: float = 0,\n        exponent: float = 1,\n        tolerance: float = 0,\n    ):\n        \"\"\"Objective for getting as close as possible to a given value.\n\n        s(y) = |y - target| ** exponent - tolerance ** exponent\n\n        Args:\n            name: output to optimize\n            target: target value\n            exponent: exponent of the difference\n            tolerance: only when used as output constraint. distance to target below which no further improvement is required\n        \"\"\"\n        super().__init__(name)\n        self.target = target\n        self.exponent = exponent\n        self.tolerance = tolerance\n\n    def __call__(self, y: pd.Series) -&gt; pd.Series:\n        return (\n            y - self.target\n        ).abs() ** self.exponent - self.tolerance**self.exponent\n\n    def __repr__(self):\n        return f\"CloseToTarget('{self.name}', target={self.target})\"\n\n    def to_config(self) -&gt; Dict:\n        config = dict(name=self.name, type=\"close-to-target\", target=self.target)\n        if self.exponent != 1:\n            config[\"exponent\"] = self.exponent\n        if self.tolerance != 0:\n            config[\"tolerance\"] = self.tolerance\n        return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.CloseToTarget.__init__","title":"<code>__init__(name, target=0, exponent=1, tolerance=0)</code>","text":"<p>Objective for getting as close as possible to a given value.</p> <p>s(y) = |y - target| ** exponent - tolerance ** exponent</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>output to optimize</p> required <code>target</code> <code>float</code> <p>target value</p> <code>0</code> <code>exponent</code> <code>float</code> <p>exponent of the difference</p> <code>1</code> <code>tolerance</code> <code>float</code> <p>only when used as output constraint. distance to target below which no further improvement is required</p> <code>0</code> Source code in <code>opti/objective.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    target: float = 0,\n    exponent: float = 1,\n    tolerance: float = 0,\n):\n    \"\"\"Objective for getting as close as possible to a given value.\n\n    s(y) = |y - target| ** exponent - tolerance ** exponent\n\n    Args:\n        name: output to optimize\n        target: target value\n        exponent: exponent of the difference\n        tolerance: only when used as output constraint. distance to target below which no further improvement is required\n    \"\"\"\n    super().__init__(name)\n    self.target = target\n    self.exponent = exponent\n    self.tolerance = tolerance\n</code></pre>"},{"location":"ref-objective/#opti.objective.Maximize","title":"<code>Maximize</code>","text":"<p>               Bases: <code>Objective</code></p> Source code in <code>opti/objective.py</code> <pre><code>class Maximize(Objective):\n    def __init__(self, name: str, target: float = 0):\n        \"\"\"Maximization objective\n\n        s(y) = target - y\n\n        Args:\n            name: output to maximize\n            target: only when used as output constraint. value above which no further improvement is required\n        \"\"\"\n        super().__init__(name)\n        self.target = target\n\n    def __call__(self, y: pd.Series) -&gt; pd.Series:\n        return self.target - y\n\n    def untransform(self, y: pd.Series) -&gt; pd.Series:\n        \"\"\"Undo the transformation from output to objective value\"\"\"\n        return self.target - y\n\n    def __repr__(self):\n        return f\"Maximize('{self.name}')\"\n\n    def to_config(self) -&gt; Dict:\n        config = dict(name=self.name, type=\"maximize\")\n        if self.target != 0:\n            config[\"target\"] = str(self.target)\n        return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.Maximize.__init__","title":"<code>__init__(name, target=0)</code>","text":"<p>Maximization objective</p> <p>s(y) = target - y</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>output to maximize</p> required <code>target</code> <code>float</code> <p>only when used as output constraint. value above which no further improvement is required</p> <code>0</code> Source code in <code>opti/objective.py</code> <pre><code>def __init__(self, name: str, target: float = 0):\n    \"\"\"Maximization objective\n\n    s(y) = target - y\n\n    Args:\n        name: output to maximize\n        target: only when used as output constraint. value above which no further improvement is required\n    \"\"\"\n    super().__init__(name)\n    self.target = target\n</code></pre>"},{"location":"ref-objective/#opti.objective.Maximize.untransform","title":"<code>untransform(y)</code>","text":"<p>Undo the transformation from output to objective value</p> Source code in <code>opti/objective.py</code> <pre><code>def untransform(self, y: pd.Series) -&gt; pd.Series:\n    \"\"\"Undo the transformation from output to objective value\"\"\"\n    return self.target - y\n</code></pre>"},{"location":"ref-objective/#opti.objective.Minimize","title":"<code>Minimize</code>","text":"<p>               Bases: <code>Objective</code></p> Source code in <code>opti/objective.py</code> <pre><code>class Minimize(Objective):\n    def __init__(self, name: str, target: float = 0):\n        \"\"\"Minimization objective\n\n        s(y) = y - target\n\n        Args:\n            name: output to minimize\n            target: only when used as output constraint. value below which no further improvement is required\n        \"\"\"\n        super().__init__(name)\n        self.target = target\n\n    def __call__(self, y: pd.Series) -&gt; pd.Series:\n        return y - self.target\n\n    def untransform(self, y: pd.Series) -&gt; pd.Series:\n        \"\"\"Undo the transformation from output to objective value\"\"\"\n        return y + self.target\n\n    def __repr__(self):\n        return f\"Minimize('{self.name}')\"\n\n    def to_config(self) -&gt; Dict:\n        config = dict(name=self.name, type=\"minimize\")\n        if self.target != 0:\n            config[\"target\"] = str(self.target)\n        return config\n</code></pre>"},{"location":"ref-objective/#opti.objective.Minimize.__init__","title":"<code>__init__(name, target=0)</code>","text":"<p>Minimization objective</p> <p>s(y) = y - target</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>output to minimize</p> required <code>target</code> <code>float</code> <p>only when used as output constraint. value below which no further improvement is required</p> <code>0</code> Source code in <code>opti/objective.py</code> <pre><code>def __init__(self, name: str, target: float = 0):\n    \"\"\"Minimization objective\n\n    s(y) = y - target\n\n    Args:\n        name: output to minimize\n        target: only when used as output constraint. value below which no further improvement is required\n    \"\"\"\n    super().__init__(name)\n    self.target = target\n</code></pre>"},{"location":"ref-objective/#opti.objective.Minimize.untransform","title":"<code>untransform(y)</code>","text":"<p>Undo the transformation from output to objective value</p> Source code in <code>opti/objective.py</code> <pre><code>def untransform(self, y: pd.Series) -&gt; pd.Series:\n    \"\"\"Undo the transformation from output to objective value\"\"\"\n    return y + self.target\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objective","title":"<code>Objective</code>","text":"Source code in <code>opti/objective.py</code> <pre><code>class Objective:\n    def __init__(self, name: str):\n        \"\"\"Base class for optimzation objectives.\"\"\"\n        self.name = name\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Evaluate the objective values for given output values.\"\"\"\n        raise NotImplementedError\n\n    def to_config(self) -&gt; Dict:\n        \"\"\"Return a json-serializable dictionary of the objective.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objective.__call__","title":"<code>__call__(df)</code>","text":"<p>Evaluate the objective values for given output values.</p> Source code in <code>opti/objective.py</code> <pre><code>def __call__(self, df: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Evaluate the objective values for given output values.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objective.__init__","title":"<code>__init__(name)</code>","text":"<p>Base class for optimzation objectives.</p> Source code in <code>opti/objective.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"Base class for optimzation objectives.\"\"\"\n    self.name = name\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objective.to_config","title":"<code>to_config()</code>","text":"<p>Return a json-serializable dictionary of the objective.</p> Source code in <code>opti/objective.py</code> <pre><code>def to_config(self) -&gt; Dict:\n    \"\"\"Return a json-serializable dictionary of the objective.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objectives","title":"<code>Objectives</code>","text":"<p>Container for optimization objectives.</p> <p>Objectives can be either used to quantify the optimility or as a constraint on the viability of output values (chance / feasibility constraint)</p> Source code in <code>opti/objective.py</code> <pre><code>class Objectives:\n    \"\"\"Container for optimization objectives.\n\n    Objectives can be either used to quantify the optimility or as a constraint on the\n    viability of output values (chance / feasibility constraint)\n    \"\"\"\n\n    def __init__(self, objectives: Union[List[Objective], List[Dict]]):\n        _objectives = []\n        for m in objectives:\n            if isinstance(m, Objective):\n                _objectives.append(m)\n            else:\n                _objectives.append(make_objective(**m))\n        self.objectives = _objectives\n\n    def __call__(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.concat([obj(df[obj.name]) for obj in self.objectives], axis=1)\n\n    def __repr__(self):\n        return \"Objectives(\\n\" + pprint.pformat(self.objectives) + \"\\n)\"\n\n    def __iter__(self):\n        return iter(self.objectives)\n\n    def __len__(self):\n        return len(self.objectives)\n\n    def __getitem__(self, i: int) -&gt; Objective:\n        return self.objectives[i]\n\n    @property\n    def names(self):\n        return [obj.name for obj in self]\n\n    def bounds(self, outputs: Parameters) -&gt; pd.DataFrame:\n        \"\"\"Compute the bounds in objective space based on the output space bounds.\n\n        The bounds can be interpreted as the ideal and nadir points.\n\n        Examples for continuous parameters:\n            min y for y in [0, 10] -&gt; ideal = 0, nadir = 10\n            max y for y in [0, 10] -&gt; ideal = -10, nadir = 0\n            min (y - 7)**2 for y in [0, 10] -&gt; ideal = 0, nadir = 7**2\n\n        Args:\n            outputs: Output parameters.\n        \"\"\"\n        Z = self(outputs.bounds)\n        bounds = pd.DataFrame(columns=self.names, dtype=float)\n        bounds.loc[\"min\"] = Z.min(axis=0)\n        bounds.loc[\"max\"] = Z.max(axis=0)\n\n        for name, obj in zip(self.names, self):\n            if isinstance(obj, CloseToTarget):\n                bounds.loc[\"min\", name] = 0\n\n        return bounds\n\n    def to_config(self) -&gt; List[Dict]:\n        return [obj.to_config() for obj in self.objectives]\n\n    def get(self, types) -&gt; \"Objectives\":\n        \"\"\"Get all parameters of the given type(s).\"\"\"\n        return Objectives([o for o in self if isinstance(o, types)])\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objectives.bounds","title":"<code>bounds(outputs)</code>","text":"<p>Compute the bounds in objective space based on the output space bounds.</p> <p>The bounds can be interpreted as the ideal and nadir points.</p> Examples for continuous parameters <p>min y for y in [0, 10] -&gt; ideal = 0, nadir = 10 max y for y in [0, 10] -&gt; ideal = -10, nadir = 0 min (y - 7)2 for y in [0, 10] -&gt; ideal = 0, nadir = 72</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Parameters</code> <p>Output parameters.</p> required Source code in <code>opti/objective.py</code> <pre><code>def bounds(self, outputs: Parameters) -&gt; pd.DataFrame:\n    \"\"\"Compute the bounds in objective space based on the output space bounds.\n\n    The bounds can be interpreted as the ideal and nadir points.\n\n    Examples for continuous parameters:\n        min y for y in [0, 10] -&gt; ideal = 0, nadir = 10\n        max y for y in [0, 10] -&gt; ideal = -10, nadir = 0\n        min (y - 7)**2 for y in [0, 10] -&gt; ideal = 0, nadir = 7**2\n\n    Args:\n        outputs: Output parameters.\n    \"\"\"\n    Z = self(outputs.bounds)\n    bounds = pd.DataFrame(columns=self.names, dtype=float)\n    bounds.loc[\"min\"] = Z.min(axis=0)\n    bounds.loc[\"max\"] = Z.max(axis=0)\n\n    for name, obj in zip(self.names, self):\n        if isinstance(obj, CloseToTarget):\n            bounds.loc[\"min\", name] = 0\n\n    return bounds\n</code></pre>"},{"location":"ref-objective/#opti.objective.Objectives.get","title":"<code>get(types)</code>","text":"<p>Get all parameters of the given type(s).</p> Source code in <code>opti/objective.py</code> <pre><code>def get(self, types) -&gt; \"Objectives\":\n    \"\"\"Get all parameters of the given type(s).\"\"\"\n    return Objectives([o for o in self if isinstance(o, types)])\n</code></pre>"},{"location":"ref-objective/#opti.objective.make_objective","title":"<code>make_objective(type, name, **kwargs)</code>","text":"<p>Make an objective from a configuration.</p> <pre><code>obj = make_objective(**config)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>objective type</p> required <code>name</code> <code>str</code> <p>output to optimize</p> required Source code in <code>opti/objective.py</code> <pre><code>def make_objective(type: str, name: str, **kwargs) -&gt; Objective:\n    \"\"\"Make an objective from a configuration.\n\n    ```\n    obj = make_objective(**config)\n    ```\n\n    Args:\n        type: objective type\n        name: output to optimize\n    \"\"\"\n    objective = {\n        \"minimize\": Minimize,\n        \"maximize\": Maximize,\n        \"close-to-target\": CloseToTarget,\n    }[type.lower()]\n    return objective(name, **kwargs)\n</code></pre>"},{"location":"ref-parameter/","title":"Parameters","text":""},{"location":"ref-parameter/#opti.parameter.Categorical","title":"<code>Categorical</code>","text":"<p>               Bases: <code>Parameter</code></p> <p>Variable with a categorical domain (nominal scale, values cannot be put into order).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the parameter</p> <code>domain</code> <code>list</code> <p>list possible values</p> Source code in <code>opti/parameter.py</code> <pre><code>class Categorical(Parameter):\n    \"\"\"Variable with a categorical domain (nominal scale, values cannot be put into order).\n\n    Attributes:\n        name (str): name of the parameter\n        domain (list): list possible values\n    \"\"\"\n\n    def __init__(self, name: str, domain: List[str], **kwargs):\n        if not isinstance(domain, list):\n            raise TypeError(f\"{name}: Domain must be of type list.\")\n        if len(domain) &lt; 2:\n            raise ValueError(f\"{name}: Domain must a least contain 2 values.\")\n        if len(set(domain)) != len(domain):\n            raise ValueError(f\"{name}: Domain contains duplicates.\")\n        super().__init__(name, domain, type=\"categorical\", **kwargs)\n\n    def __repr__(self):\n        return f\"Categorical('{self.name}', domain={self.domain})\"\n\n    @property\n    def bounds(self) -&gt; Tuple[float, float]:\n        \"\"\"Return the domain bounds.\"\"\"\n        return np.nan, np.nan\n\n    def contains(self, point):\n        \"\"\"Check if a point is in contained in the domain.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with boolean datatype.\n        \"\"\"\n        if not np.isscalar(point):\n            point = np.array(point)\n        return np.isin(point, self.domain)\n\n    def round(self, point):\n        \"\"\"Round a point to the closest contained values.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point`.\n        \"\"\"\n        if not np.all(self.contains(point)):\n            raise ValueError(\n                f\"{self.name}: Cannot round values for categorical parameter.\"\n            )\n        return point\n\n    def sample(self, n: int = 1) -&gt; pd.Series:\n        \"\"\"Draw random samples from the domain.\"\"\"\n        return pd.Series(name=self.name, data=np.random.choice(self.domain, n))\n\n    def to_onehot_encoding(self, points: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"Convert points to a one-hot encoding.\"\"\"\n        return pd.DataFrame(\n            {f\"{self.name}{_CAT_SEP}{c}\": points == c for c in self.domain}, dtype=float\n        )\n\n    def from_onehot_encoding(self, points: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Convert points back from one-hot encoding.\"\"\"\n        cat_cols = [f\"{self.name}{_CAT_SEP}{c}\" for c in self.domain]\n        if np.any([c not in cat_cols for c in points.columns]):\n            raise ValueError(\n                f\"{self.name}: Column names don't match categorical levels: {points.columns}, {cat_cols}.\"\n            )\n        s = points.idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n        s.name = self.name\n        return s\n\n    def to_dummy_encoding(self, points: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\"\n        return pd.DataFrame(\n            {f\"{self.name}{_CAT_SEP}{c}\": points == c for c in self.domain[1:]},\n            dtype=float,\n        )\n\n    def from_dummy_encoding(self, points: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Convert points back from dummy encoding.\"\"\"\n        cat_cols = [f\"{self.name}{_CAT_SEP}{c}\" for c in self.domain]\n        if np.any([c not in cat_cols[1:] for c in points.columns]):\n            raise ValueError(\n                f\"{self.name}: Column names don't match categorical levels: {points.columns}, {cat_cols}.\"\n            )\n        points = points.copy()\n        points[cat_cols[0]] = 1 - points[cat_cols[1:]].sum(axis=1)\n        s = points.idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n        s.name = self.name\n        return s\n\n    def to_label_encoding(self, points: pd.Series) -&gt; pd.Series:\n        \"\"\"Convert points to label-encoding.\"\"\"\n        enc = pd.Series(range(len(self.domain)), index=list(self.domain))\n        s = enc[points]\n        s.index = points.index\n        s.name = self.name\n        return s\n\n    def from_label_encoding(self, points: pd.Series) -&gt; pd.Series:\n        \"\"\"Convert points back from label-encoding.\"\"\"\n        enc = np.array(self.domain)\n        return pd.Series(enc[points], index=points.index)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Return the domain bounds.</p>"},{"location":"ref-parameter/#opti.parameter.Categorical.contains","title":"<code>contains(point)</code>","text":"<p>Check if a point is in contained in the domain.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>(float, ndarray, Series or Dataframe)</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with boolean datatype.</p> Source code in <code>opti/parameter.py</code> <pre><code>def contains(self, point):\n    \"\"\"Check if a point is in contained in the domain.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with boolean datatype.\n    \"\"\"\n    if not np.isscalar(point):\n        point = np.array(point)\n    return np.isin(point, self.domain)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.from_dummy_encoding","title":"<code>from_dummy_encoding(points)</code>","text":"<p>Convert points back from dummy encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_dummy_encoding(self, points: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Convert points back from dummy encoding.\"\"\"\n    cat_cols = [f\"{self.name}{_CAT_SEP}{c}\" for c in self.domain]\n    if np.any([c not in cat_cols[1:] for c in points.columns]):\n        raise ValueError(\n            f\"{self.name}: Column names don't match categorical levels: {points.columns}, {cat_cols}.\"\n        )\n    points = points.copy()\n    points[cat_cols[0]] = 1 - points[cat_cols[1:]].sum(axis=1)\n    s = points.idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n    s.name = self.name\n    return s\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.from_label_encoding","title":"<code>from_label_encoding(points)</code>","text":"<p>Convert points back from label-encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_label_encoding(self, points: pd.Series) -&gt; pd.Series:\n    \"\"\"Convert points back from label-encoding.\"\"\"\n    enc = np.array(self.domain)\n    return pd.Series(enc[points], index=points.index)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.from_onehot_encoding","title":"<code>from_onehot_encoding(points)</code>","text":"<p>Convert points back from one-hot encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_onehot_encoding(self, points: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Convert points back from one-hot encoding.\"\"\"\n    cat_cols = [f\"{self.name}{_CAT_SEP}{c}\" for c in self.domain]\n    if np.any([c not in cat_cols for c in points.columns]):\n        raise ValueError(\n            f\"{self.name}: Column names don't match categorical levels: {points.columns}, {cat_cols}.\"\n        )\n    s = points.idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n    s.name = self.name\n    return s\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.round","title":"<code>round(point)</code>","text":"<p>Round a point to the closest contained values.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>(float, ndarray, Series or Dataframe)</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code>.</p> Source code in <code>opti/parameter.py</code> <pre><code>def round(self, point):\n    \"\"\"Round a point to the closest contained values.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point`.\n    \"\"\"\n    if not np.all(self.contains(point)):\n        raise ValueError(\n            f\"{self.name}: Cannot round values for categorical parameter.\"\n        )\n    return point\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.sample","title":"<code>sample(n=1)</code>","text":"<p>Draw random samples from the domain.</p> Source code in <code>opti/parameter.py</code> <pre><code>def sample(self, n: int = 1) -&gt; pd.Series:\n    \"\"\"Draw random samples from the domain.\"\"\"\n    return pd.Series(name=self.name, data=np.random.choice(self.domain, n))\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.to_dummy_encoding","title":"<code>to_dummy_encoding(points)</code>","text":"<p>Convert points to a dummy-hot encoding, dropping the first categorical level.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_dummy_encoding(self, points: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"Convert points to a dummy-hot encoding, dropping the first categorical level.\"\"\"\n    return pd.DataFrame(\n        {f\"{self.name}{_CAT_SEP}{c}\": points == c for c in self.domain[1:]},\n        dtype=float,\n    )\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.to_label_encoding","title":"<code>to_label_encoding(points)</code>","text":"<p>Convert points to label-encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_label_encoding(self, points: pd.Series) -&gt; pd.Series:\n    \"\"\"Convert points to label-encoding.\"\"\"\n    enc = pd.Series(range(len(self.domain)), index=list(self.domain))\n    s = enc[points]\n    s.index = points.index\n    s.name = self.name\n    return s\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Categorical.to_onehot_encoding","title":"<code>to_onehot_encoding(points)</code>","text":"<p>Convert points to a one-hot encoding.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_onehot_encoding(self, points: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"Convert points to a one-hot encoding.\"\"\"\n    return pd.DataFrame(\n        {f\"{self.name}{_CAT_SEP}{c}\": points == c for c in self.domain}, dtype=float\n    )\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous","title":"<code>Continuous</code>","text":"<p>               Bases: <code>Parameter</code></p> <p>Variable that can take on any real value in the specified domain.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the parameter</p> <code>domain</code> <code>list</code> <p>[lower bound, upper bound]</p> Source code in <code>opti/parameter.py</code> <pre><code>class Continuous(Parameter):\n    \"\"\"Variable that can take on any real value in the specified domain.\n\n    Attributes:\n        name (str): name of the parameter\n        domain (list): [lower bound, upper bound]\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        domain: Optional[Sequence] = None,\n        **kwargs,\n    ):\n        if domain is None:\n            domain = [-np.inf, np.inf]\n        else:\n            if len(domain) != 2:\n                raise ValueError(\n                    f\"{name}: Domain must consist of two values [low, high].\"\n                )\n        # convert None to +/- inf and string to float\n        low = -np.inf if domain[0] is None else float(domain[0])\n        high = np.inf if domain[1] is None else float(domain[1])\n\n        if high &lt; low:\n            raise ValueError(\n                f\"{name}: Lower bound {low} must be less than upper bound {high}.\"\n            )\n        self.low = low\n        self.high = high\n        super().__init__(name=name, domain=[low, high], type=\"continuous\", **kwargs)\n\n    def __repr__(self):\n        if np.isfinite(self.low) or np.isfinite(self.high):\n            return f\"Continuous('{self.name}', domain={self.domain})\"\n        else:\n            return f\"Continuous('{self.name}')\"\n\n    @property\n    def bounds(self) -&gt; Tuple[float, float]:\n        \"\"\"Return the domain bounds.\"\"\"\n        return self.low, self.high\n\n    def contains(self, point):\n        \"\"\"Check if a point is in contained in the domain.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with boolean datatype.\n        \"\"\"\n        return (self.low &lt;= point) &amp; (point &lt;= self.high)\n\n    def round(self, point):\n        \"\"\"Round a point to the closest contained values.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with values clipped to parameter bounds.\n        \"\"\"\n        return np.clip(point, self.low, self.high)\n\n    def sample(self, n: int = 1) -&gt; pd.Series:\n        \"\"\"Draw random samples from the domain.\"\"\"\n        low = max(self.low, np.finfo(np.float32).min)\n        high = min(self.high, np.finfo(np.float32).max)\n        return pd.Series(name=self.name, data=np.random.uniform(low, high, n))\n\n    def to_config(self) -&gt; dict:\n        \"\"\"Return a json-serializable configuration dict.\"\"\"\n        conf = dict(name=self.name, type=self.type)\n        low = None if np.isinf(self.low) else float(self.low)\n        high = None if np.isinf(self.high) else float(self.high)\n        if low is not None or high is not None:\n            conf.update({\"domain\": [low, high]})\n        conf.update(self.extra_fields)\n        return conf\n\n    def to_unit_range(self, points):\n        \"\"\"Transform points to the unit range: [low, high] -&gt; [0, 1].\n\n        Points outside of the domain will transform to outside of [0, 1].\n        Nothing is done if low == high.\n        \"\"\"\n        if np.isclose(self.low, self.high):\n            return points\n        else:\n            return (points - self.low) / (self.high - self.low)\n\n    def from_unit_range(self, points):\n        \"\"\"Transform points from the unit range: [0, 1] -&gt; [low, high].\n\n        A rounding is applied to correct for numerical precision.\n        Nothing is done if low == high.\n        \"\"\"\n        if np.isclose(self.low, self.high):\n            return points\n        else:\n            return points * (self.high - self.low) + self.low\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Return the domain bounds.</p>"},{"location":"ref-parameter/#opti.parameter.Continuous.contains","title":"<code>contains(point)</code>","text":"<p>Check if a point is in contained in the domain.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>(float, ndarray, Series or Dataframe)</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with boolean datatype.</p> Source code in <code>opti/parameter.py</code> <pre><code>def contains(self, point):\n    \"\"\"Check if a point is in contained in the domain.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with boolean datatype.\n    \"\"\"\n    return (self.low &lt;= point) &amp; (point &lt;= self.high)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.from_unit_range","title":"<code>from_unit_range(points)</code>","text":"<p>Transform points from the unit range: [0, 1] -&gt; [low, high].</p> <p>A rounding is applied to correct for numerical precision. Nothing is done if low == high.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_unit_range(self, points):\n    \"\"\"Transform points from the unit range: [0, 1] -&gt; [low, high].\n\n    A rounding is applied to correct for numerical precision.\n    Nothing is done if low == high.\n    \"\"\"\n    if np.isclose(self.low, self.high):\n        return points\n    else:\n        return points * (self.high - self.low) + self.low\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.round","title":"<code>round(point)</code>","text":"<p>Round a point to the closest contained values.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>(float, ndarray, Series or Dataframe)</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with values clipped to parameter bounds.</p> Source code in <code>opti/parameter.py</code> <pre><code>def round(self, point):\n    \"\"\"Round a point to the closest contained values.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with values clipped to parameter bounds.\n    \"\"\"\n    return np.clip(point, self.low, self.high)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.sample","title":"<code>sample(n=1)</code>","text":"<p>Draw random samples from the domain.</p> Source code in <code>opti/parameter.py</code> <pre><code>def sample(self, n: int = 1) -&gt; pd.Series:\n    \"\"\"Draw random samples from the domain.\"\"\"\n    low = max(self.low, np.finfo(np.float32).min)\n    high = min(self.high, np.finfo(np.float32).max)\n    return pd.Series(name=self.name, data=np.random.uniform(low, high, n))\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.to_config","title":"<code>to_config()</code>","text":"<p>Return a json-serializable configuration dict.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_config(self) -&gt; dict:\n    \"\"\"Return a json-serializable configuration dict.\"\"\"\n    conf = dict(name=self.name, type=self.type)\n    low = None if np.isinf(self.low) else float(self.low)\n    high = None if np.isinf(self.high) else float(self.high)\n    if low is not None or high is not None:\n        conf.update({\"domain\": [low, high]})\n    conf.update(self.extra_fields)\n    return conf\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Continuous.to_unit_range","title":"<code>to_unit_range(points)</code>","text":"<p>Transform points to the unit range: [low, high] -&gt; [0, 1].</p> <p>Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_unit_range(self, points):\n    \"\"\"Transform points to the unit range: [low, high] -&gt; [0, 1].\n\n    Points outside of the domain will transform to outside of [0, 1].\n    Nothing is done if low == high.\n    \"\"\"\n    if np.isclose(self.low, self.high):\n        return points\n    else:\n        return (points - self.low) / (self.high - self.low)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete","title":"<code>Discrete</code>","text":"<p>               Bases: <code>Parameter</code></p> <p>Variable with a discrete domain (ordinal scale).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the parameter</p> <code>domain</code> <code>list</code> <p>list of possible numeric values</p> Source code in <code>opti/parameter.py</code> <pre><code>class Discrete(Parameter):\n    \"\"\"Variable with a discrete domain (ordinal scale).\n\n    Attributes:\n        name (str): name of the parameter\n        domain (list): list of possible numeric values\n    \"\"\"\n\n    def __init__(self, name: str, domain: Sequence, **kwargs):\n        if len(domain) &lt; 1:\n            raise ValueError(f\"{name}: Domain must contain at least one value.\")\n        try:\n            # convert to a sorted list of floats\n            domain = np.sort(np.array(domain).astype(float)).tolist()\n        except ValueError:\n            raise ValueError(f\"{name}: Domain contains non-numeric values.\")\n        if len(set(domain)) != len(domain):\n            raise ValueError(f\"{name}: Domain contains duplicates.\")\n        self.low = min(domain)\n        self.high = max(domain)\n        super().__init__(name, domain, type=\"discrete\", **kwargs)\n\n    def __repr__(self):\n        return f\"Discrete('{self.name}', domain={self.domain})\"\n\n    @property\n    def bounds(self) -&gt; Tuple[float, float]:\n        \"\"\"Return the domain bounds.\"\"\"\n        return self.low, self.high\n\n    def contains(self, point):\n        \"\"\"Check if a point is in contained in the domain.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with boolean datatype.\n        \"\"\"\n        if not np.isscalar(point):\n            point = np.array(point)\n        return np.isin(point, self.domain)\n\n    def round(self, point):\n        \"\"\"Round a point to the closest contained values.\n\n        Args:\n            point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n        Returns:\n            Object of the same type as `point` with values clipped to parameter bounds.\n        \"\"\"\n        if np.isscalar(point):\n            i = np.argmin(np.abs(np.array(self.domain) - point))\n            return self.domain[i]\n        closest = [np.argmin(np.abs(np.array(self.domain) - p)) for p in point]\n        rounded = np.array(self.domain)[closest]\n        if isinstance(point, np.ndarray):\n            return rounded\n        elif isinstance(point, pd.Series):\n            return pd.Series(name=self.name, data=rounded, index=point.index)\n        elif isinstance(point, pd.DataFrame):\n            return pd.DataFrame({self.name: rounded}, index=point.index)\n\n    def sample(self, n: int = 1) -&gt; pd.Series:\n        \"\"\"Draw random samples from the domain.\"\"\"\n        return pd.Series(name=self.name, data=np.random.choice(self.domain, n))\n\n    def to_unit_range(self, points):\n        \"\"\"Transform points to the unit range: [low, high] -&gt; [0, 1].\n\n        Points outside of the domain will transform to outside of [0, 1].\n        Nothing is done if low == high.\n        \"\"\"\n        if np.isclose(self.low, self.high):\n            return points\n        else:\n            return (points - self.low) / (self.high - self.low)\n\n    def from_unit_range(self, points):\n        \"\"\"Transform points from the unit range: [0, 1] -&gt; [low, high].\n\n        A rounding is applied to correct for numerical precision.\n        Nothing is done if low == high.\n        \"\"\"\n        if np.isclose(self.low, self.high):\n            return points\n        else:\n            points = points * (self.high - self.low) + self.low\n            return self.round(points)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Return the domain bounds.</p>"},{"location":"ref-parameter/#opti.parameter.Discrete.contains","title":"<code>contains(point)</code>","text":"<p>Check if a point is in contained in the domain.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>(float, ndarray, Series or Dataframe)</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with boolean datatype.</p> Source code in <code>opti/parameter.py</code> <pre><code>def contains(self, point):\n    \"\"\"Check if a point is in contained in the domain.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with boolean datatype.\n    \"\"\"\n    if not np.isscalar(point):\n        point = np.array(point)\n    return np.isin(point, self.domain)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.from_unit_range","title":"<code>from_unit_range(points)</code>","text":"<p>Transform points from the unit range: [0, 1] -&gt; [low, high].</p> <p>A rounding is applied to correct for numerical precision. Nothing is done if low == high.</p> Source code in <code>opti/parameter.py</code> <pre><code>def from_unit_range(self, points):\n    \"\"\"Transform points from the unit range: [0, 1] -&gt; [low, high].\n\n    A rounding is applied to correct for numerical precision.\n    Nothing is done if low == high.\n    \"\"\"\n    if np.isclose(self.low, self.high):\n        return points\n    else:\n        points = points * (self.high - self.low) + self.low\n        return self.round(points)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.round","title":"<code>round(point)</code>","text":"<p>Round a point to the closest contained values.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>(float, ndarray, Series or Dataframe)</code> <p>parameter value(s).</p> required <p>Returns:</p> Type Description <p>Object of the same type as <code>point</code> with values clipped to parameter bounds.</p> Source code in <code>opti/parameter.py</code> <pre><code>def round(self, point):\n    \"\"\"Round a point to the closest contained values.\n\n    Args:\n        point (float, np.ndarray, pd.Series or pd.Dataframe): parameter value(s).\n\n    Returns:\n        Object of the same type as `point` with values clipped to parameter bounds.\n    \"\"\"\n    if np.isscalar(point):\n        i = np.argmin(np.abs(np.array(self.domain) - point))\n        return self.domain[i]\n    closest = [np.argmin(np.abs(np.array(self.domain) - p)) for p in point]\n    rounded = np.array(self.domain)[closest]\n    if isinstance(point, np.ndarray):\n        return rounded\n    elif isinstance(point, pd.Series):\n        return pd.Series(name=self.name, data=rounded, index=point.index)\n    elif isinstance(point, pd.DataFrame):\n        return pd.DataFrame({self.name: rounded}, index=point.index)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.sample","title":"<code>sample(n=1)</code>","text":"<p>Draw random samples from the domain.</p> Source code in <code>opti/parameter.py</code> <pre><code>def sample(self, n: int = 1) -&gt; pd.Series:\n    \"\"\"Draw random samples from the domain.\"\"\"\n    return pd.Series(name=self.name, data=np.random.choice(self.domain, n))\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Discrete.to_unit_range","title":"<code>to_unit_range(points)</code>","text":"<p>Transform points to the unit range: [low, high] -&gt; [0, 1].</p> <p>Points outside of the domain will transform to outside of [0, 1]. Nothing is done if low == high.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_unit_range(self, points):\n    \"\"\"Transform points to the unit range: [low, high] -&gt; [0, 1].\n\n    Points outside of the domain will transform to outside of [0, 1].\n    Nothing is done if low == high.\n    \"\"\"\n    if np.isclose(self.low, self.high):\n        return points\n    else:\n        return (points - self.low) / (self.high - self.low)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameter","title":"<code>Parameter</code>","text":"<p>Parameter base class.</p> Source code in <code>opti/parameter.py</code> <pre><code>class Parameter:\n    \"\"\"Parameter base class.\"\"\"\n\n    def __init__(self, name: str, domain: Sequence, type: str = None, **kwargs):\n        self.name = name\n        self.domain = domain\n        self.type = type\n        self.extra_fields = kwargs\n\n    def to_config(self) -&gt; dict:\n        \"\"\"Return a json-serializable configuration dict.\"\"\"\n        conf = dict(name=self.name, type=self.type, domain=self.domain)\n        conf.update(self.extra_fields)\n        return conf\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameter.to_config","title":"<code>to_config()</code>","text":"<p>Return a json-serializable configuration dict.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_config(self) -&gt; dict:\n    \"\"\"Return a json-serializable configuration dict.\"\"\"\n    conf = dict(name=self.name, type=self.type, domain=self.domain)\n    conf.update(self.extra_fields)\n    return conf\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters","title":"<code>Parameters</code>","text":"<p>Set of parameters representing either the input or the output parameter space.</p> Source code in <code>opti/parameter.py</code> <pre><code>class Parameters:\n    \"\"\"Set of parameters representing either the input or the output parameter space.\"\"\"\n\n    def __init__(self, parameters):\n        \"\"\"\n        It can be constructed either from a list / tuple (of at least one) Parameter objects\n        ```\n        Parameters([\n            Continuous(name=\"foo\", domain=[1, 10]),\n            Discrete(name=\"bar\", domain=[1, 2, 3, 4]),\n            Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]),\n        ])\n        ```\n        or from a list / tuple of dicts\n        ```\n        Parameters([\n            {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]},\n            {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]},\n            {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]},\n            {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"},\n        ])\n        ```\n        In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses.\n        Parameters(conf).to_config() == conf\n        \"\"\"\n        if not isinstance(parameters, (list, tuple)):\n            raise TypeError(\"Parameters expects a list or tuple of parameters.\")\n\n        self.parameters = {}\n        for d in parameters:\n            if not isinstance(d, Parameter):\n                d = make_parameter(**d)\n            if d.name in self.parameters:\n                raise ValueError(f\"Duplicate parameter name {d.name}\")\n            self.parameters[d.name] = d\n\n    def __repr__(self):\n        return \"Parameters(\\n\" + pprint.pformat(list(self.parameters.values())) + \"\\n)\"\n\n    def __iter__(self):\n        return iter(self.parameters.values())\n\n    def __getitem__(self, name):\n        return self.parameters[name]\n\n    def __len__(self):\n        return len(self.parameters)\n\n    def __add__(self, other):\n        parameters = list(self.parameters.values()) + list(other.parameters.values())\n        return Parameters(parameters)\n\n    @property\n    def names(self):\n        return list(self.parameters.keys())\n\n    @property\n    def bounds(self) -&gt; pd.DataFrame:\n        \"\"\"Return the parameter bounds.\"\"\"\n        return pd.DataFrame({p.name: p.bounds for p in self}, index=[\"min\", \"max\"])\n\n    def contains(self, points: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Check if points are inside the domain of each parameter.\"\"\"\n        if isinstance(points, pd.DataFrame):\n            points = points[self.names]\n        b = np.stack([self[k].contains(v) for k, v in points.items()], axis=1)\n        return b.all(axis=1)\n\n    def round(self, points: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Round points to the closest contained values.\"\"\"\n        return pd.concat([self[k].round(v) for k, v in points.items()], axis=1)\n\n    def sample(self, n: int = 1) -&gt; pd.DataFrame:\n        \"\"\"Draw uniformly distributed random samples.\"\"\"\n        return pd.concat([param.sample(n) for param in self], axis=1)\n\n    def transform(\n        self,\n        points: pd.DataFrame,\n        continuous: str = \"none\",\n        discrete: str = \"none\",\n        categorical: str = \"onehot-encode\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Transfrom the given dataframe according to a set of transformation rules.\n\n        Args:\n            points (pd.DataFrame): Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns.\n            continuous (str, optional): Transform for continuous parameters. Options are\n                - \"none\" (default): keep values unchanged.\n                - \"normalize\": transforms the domain bounds to [0, 1]\n            discrete (str, optional): Transform for discrete parameters. Options are\n                - \"none\" (default): keep values unchanged.\n                - \"normalize\": transforms the domain bounds to [0, 1]\n            categorical (str, optional): Transform for categoricals. Options are\n                - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1]\n                - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1]\n                - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2]\n                - \"none\": keep values unchanged\n\n        Raises:\n            ValueError: Unknown transform.\n\n        Returns:\n            pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped.\n        \"\"\"\n        transformed = []\n        for p in self:\n            s = points[p.name]\n            if isinstance(p, Continuous):\n                if continuous == \"none\":\n                    transformed.append(s)\n                elif continuous == \"normalize\":\n                    transformed.append(p.to_unit_range(s))\n                else:\n                    raise ValueError(f\"Unknown continuous transform {continuous}.\")\n            if isinstance(p, Discrete):\n                if discrete == \"none\":\n                    transformed.append(s)\n                elif discrete == \"normalize\":\n                    transformed.append(p.to_unit_range(s))\n                else:\n                    raise ValueError(f\"Unknown discrete transform {discrete}.\")\n            if isinstance(p, Categorical):\n                if categorical == \"none\":\n                    transformed.append(s)\n                elif categorical == \"onehot-encode\":\n                    transformed.append(p.to_onehot_encoding(s))\n                elif categorical == \"dummy-encode\":\n                    transformed.append(p.to_dummy_encoding(s))\n                elif categorical == \"label-encode\":\n                    transformed.append(p.to_label_encoding(s))\n                else:\n                    raise ValueError(f\"Unknown categorical transform {categorical}.\")\n        return pd.concat(transformed, axis=1)\n\n    def to_config(self) -&gt; List[dict]:\n        \"\"\"Configuration of the parameter space.\"\"\"\n        return [param.to_config() for param in self.parameters.values()]\n\n    def get(self, types) -&gt; \"Parameters\":\n        \"\"\"Get all parameters of the given type(s).\"\"\"\n        return Parameters([p for p in self if isinstance(p, types)])\n\n    def to_df(self, x: np.ndarray, to_numeric=False) -&gt; pd.DataFrame:\n        \"\"\"Create a dataframe for a given numpy array of parameter values.\"\"\"\n        X = pd.DataFrame(np.atleast_2d(x), columns=self.names)\n        if to_numeric:\n            for n in self.get((Continuous, Discrete)).names:\n                X[n] = pd.to_numeric(X[n])\n        return X\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Return the parameter bounds.</p>"},{"location":"ref-parameter/#opti.parameter.Parameters.__init__","title":"<code>__init__(parameters)</code>","text":"<p>It can be constructed either from a list / tuple (of at least one) Parameter objects <pre><code>Parameters([\n    Continuous(name=\"foo\", domain=[1, 10]),\n    Discrete(name=\"bar\", domain=[1, 2, 3, 4]),\n    Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]),\n])\n</code></pre> or from a list / tuple of dicts <pre><code>Parameters([\n    {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]},\n    {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]},\n    {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]},\n    {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"},\n])\n</code></pre> In particular, Parameters().init and the to_config method of each Parameter type are inverses. Parameters(conf).to_config() == conf</p> Source code in <code>opti/parameter.py</code> <pre><code>def __init__(self, parameters):\n    \"\"\"\n    It can be constructed either from a list / tuple (of at least one) Parameter objects\n    ```\n    Parameters([\n        Continuous(name=\"foo\", domain=[1, 10]),\n        Discrete(name=\"bar\", domain=[1, 2, 3, 4]),\n        Categorical(name=\"baz\", domain=[\"A\", \"B\", 3]),\n    ])\n    ```\n    or from a list / tuple of dicts\n    ```\n    Parameters([\n        {\"name\": \"foo\", \"type\": \"continuous\", \"domain\": [1, 10]},\n        {\"name\": \"bar\", \"type\": \"discrete\", \"domain\": [1, 2, 3, 4]},\n        {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3]},\n        {\"name\": \"baz\", \"type\": \"categorical\", \"domain\": [\"A\", \"B\", 3], extra=\"info\"},\n    ])\n    ```\n    In particular, Parameters().__init__ and the to_config method of each Parameter type are inverses.\n    Parameters(conf).to_config() == conf\n    \"\"\"\n    if not isinstance(parameters, (list, tuple)):\n        raise TypeError(\"Parameters expects a list or tuple of parameters.\")\n\n    self.parameters = {}\n    for d in parameters:\n        if not isinstance(d, Parameter):\n            d = make_parameter(**d)\n        if d.name in self.parameters:\n            raise ValueError(f\"Duplicate parameter name {d.name}\")\n        self.parameters[d.name] = d\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.contains","title":"<code>contains(points)</code>","text":"<p>Check if points are inside the domain of each parameter.</p> Source code in <code>opti/parameter.py</code> <pre><code>def contains(self, points: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Check if points are inside the domain of each parameter.\"\"\"\n    if isinstance(points, pd.DataFrame):\n        points = points[self.names]\n    b = np.stack([self[k].contains(v) for k, v in points.items()], axis=1)\n    return b.all(axis=1)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.get","title":"<code>get(types)</code>","text":"<p>Get all parameters of the given type(s).</p> Source code in <code>opti/parameter.py</code> <pre><code>def get(self, types) -&gt; \"Parameters\":\n    \"\"\"Get all parameters of the given type(s).\"\"\"\n    return Parameters([p for p in self if isinstance(p, types)])\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.round","title":"<code>round(points)</code>","text":"<p>Round points to the closest contained values.</p> Source code in <code>opti/parameter.py</code> <pre><code>def round(self, points: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Round points to the closest contained values.\"\"\"\n    return pd.concat([self[k].round(v) for k, v in points.items()], axis=1)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.sample","title":"<code>sample(n=1)</code>","text":"<p>Draw uniformly distributed random samples.</p> Source code in <code>opti/parameter.py</code> <pre><code>def sample(self, n: int = 1) -&gt; pd.DataFrame:\n    \"\"\"Draw uniformly distributed random samples.\"\"\"\n    return pd.concat([param.sample(n) for param in self], axis=1)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.to_config","title":"<code>to_config()</code>","text":"<p>Configuration of the parameter space.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_config(self) -&gt; List[dict]:\n    \"\"\"Configuration of the parameter space.\"\"\"\n    return [param.to_config() for param in self.parameters.values()]\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.to_df","title":"<code>to_df(x, to_numeric=False)</code>","text":"<p>Create a dataframe for a given numpy array of parameter values.</p> Source code in <code>opti/parameter.py</code> <pre><code>def to_df(self, x: np.ndarray, to_numeric=False) -&gt; pd.DataFrame:\n    \"\"\"Create a dataframe for a given numpy array of parameter values.\"\"\"\n    X = pd.DataFrame(np.atleast_2d(x), columns=self.names)\n    if to_numeric:\n        for n in self.get((Continuous, Discrete)).names:\n            X[n] = pd.to_numeric(X[n])\n    return X\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.Parameters.transform","title":"<code>transform(points, continuous='none', discrete='none', categorical='onehot-encode')</code>","text":"<p>Transfrom the given dataframe according to a set of transformation rules.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>DataFrame</code> <p>Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns.</p> required <code>continuous</code> <code>str</code> <p>Transform for continuous parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1]</p> <code>'none'</code> <code>discrete</code> <code>str</code> <p>Transform for discrete parameters. Options are - \"none\" (default): keep values unchanged. - \"normalize\": transforms the domain bounds to [0, 1]</p> <code>'none'</code> <code>categorical</code> <code>str</code> <p>Transform for categoricals. Options are - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1] - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1] - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2] - \"none\": keep values unchanged</p> <code>'onehot-encode'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Unknown transform.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped.</p> Source code in <code>opti/parameter.py</code> <pre><code>def transform(\n    self,\n    points: pd.DataFrame,\n    continuous: str = \"none\",\n    discrete: str = \"none\",\n    categorical: str = \"onehot-encode\",\n) -&gt; pd.DataFrame:\n    \"\"\"Transfrom the given dataframe according to a set of transformation rules.\n\n    Args:\n        points (pd.DataFrame): Dataframe to transfrom. Must contain columns for each parameter and may contain additional columns.\n        continuous (str, optional): Transform for continuous parameters. Options are\n            - \"none\" (default): keep values unchanged.\n            - \"normalize\": transforms the domain bounds to [0, 1]\n        discrete (str, optional): Transform for discrete parameters. Options are\n            - \"none\" (default): keep values unchanged.\n            - \"normalize\": transforms the domain bounds to [0, 1]\n        categorical (str, optional): Transform for categoricals. Options are\n            - \"onehot-encode\" (default): A parameter with levels [A, B, C] transforms to 3 columns holding values [0, 1]\n            - \"dummy-encode\": a parameter with levels [A, B, C] transforms to 2 columns holding values [0, 1]\n            - \"label-encode\": a parameter with levels [A, B, C] transfroms to 1 columns with values [0, 1, 2]\n            - \"none\": keep values unchanged\n\n    Raises:\n        ValueError: Unknown transform.\n\n    Returns:\n        pd.DataFrame: Transformed points. Columns that don't correspond to parameters are dropped.\n    \"\"\"\n    transformed = []\n    for p in self:\n        s = points[p.name]\n        if isinstance(p, Continuous):\n            if continuous == \"none\":\n                transformed.append(s)\n            elif continuous == \"normalize\":\n                transformed.append(p.to_unit_range(s))\n            else:\n                raise ValueError(f\"Unknown continuous transform {continuous}.\")\n        if isinstance(p, Discrete):\n            if discrete == \"none\":\n                transformed.append(s)\n            elif discrete == \"normalize\":\n                transformed.append(p.to_unit_range(s))\n            else:\n                raise ValueError(f\"Unknown discrete transform {discrete}.\")\n        if isinstance(p, Categorical):\n            if categorical == \"none\":\n                transformed.append(s)\n            elif categorical == \"onehot-encode\":\n                transformed.append(p.to_onehot_encoding(s))\n            elif categorical == \"dummy-encode\":\n                transformed.append(p.to_dummy_encoding(s))\n            elif categorical == \"label-encode\":\n                transformed.append(p.to_label_encoding(s))\n            else:\n                raise ValueError(f\"Unknown categorical transform {categorical}.\")\n    return pd.concat(transformed, axis=1)\n</code></pre>"},{"location":"ref-parameter/#opti.parameter.make_parameter","title":"<code>make_parameter(name, type, domain=None, **kwargs)</code>","text":"<p>Make a parameter object from a configuration</p> <p>p = make_parameter(**config)</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>\"continuous\", \"discrete\" or \"categorical\"</p> required <code>name</code> <code>str</code> <p>Name of the parameter</p> required <code>domain</code> <code>list</code> <p>Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"]</p> <code>None</code> Source code in <code>opti/parameter.py</code> <pre><code>def make_parameter(\n    name: str,\n    type: str,\n    domain: Optional[Sequence] = None,\n    **kwargs,\n):\n    \"\"\"Make a parameter object from a configuration\n\n    p = make_parameter(**config)\n\n    Args:\n        type (str): \"continuous\", \"discrete\" or \"categorical\"\n        name (str): Name of the parameter\n        domain (list): Domain, e.g, [0, 1], [1, 2.5, 5] or [\"A\", \"B\", \"C\"]\n    \"\"\"\n    parameter = {\n        \"continuous\": Continuous,\n        \"discrete\": Discrete,\n        \"categorical\": Categorical,\n    }[type.lower()]\n    if domain is None and parameter is not Continuous:\n        raise ValueError(f\"Domain not given for parameter {name}.\")\n\n    return parameter(name=name, domain=domain, **kwargs)\n</code></pre>"},{"location":"ref-problem/","title":"Problem","text":""},{"location":"ref-problem/#opti.problem.Problem","title":"<code>Problem</code>","text":"Source code in <code>opti/problem.py</code> <pre><code>class Problem:\n    def __init__(\n        self,\n        inputs: ParametersLike,\n        outputs: ParametersLike,\n        objectives: Optional[ObjectivesLike] = None,\n        constraints: Optional[ConstraintsLike] = None,\n        output_constraints: Optional[ObjectivesLike] = None,\n        f: Optional[Callable] = None,\n        models: Optional[ModelsLike] = None,\n        data: Optional[DataFrameLike] = None,\n        optima: Optional[DataFrameLike] = None,\n        name: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"An optimization problem.\n\n        Args:\n            inputs: Input parameters.\n            outputs: Output parameters.\n            objectives: Optimization objectives. Defaults to minimization.\n            constraints: Constraints on the inputs.\n            output_constraints: Constraints on the outputs.\n            f: Function to evaluate the outputs for given inputs.\n                Must have the signature: f(x: pd.DataFrame) -&gt; pd.DataFrame\n            data: Experimental data.\n            optima: Pareto optima.\n            name: Name of the problem.\n        \"\"\"\n        self.name = name if name is not None else \"Problem\"\n        self.inputs = inputs if isinstance(inputs, Parameters) else Parameters(inputs)\n        self.outputs = (\n            outputs if isinstance(outputs, Parameters) else Parameters(outputs)\n        )\n\n        if objectives is None:\n            self.objectives = Objectives([Minimize(m) for m in self.outputs.names])\n        elif isinstance(objectives, Objectives):\n            self.objectives = objectives\n        else:\n            self.objectives = Objectives(objectives)\n\n        if isinstance(constraints, Constraints):\n            pass\n        elif not constraints:\n            constraints = None\n        else:\n            constraints = Constraints(constraints)\n            if len(constraints) == 0:  # no valid constraints\n                constraints = None\n        self.constraints = constraints\n\n        if isinstance(output_constraints, Objectives) or output_constraints is None:\n            self.output_constraints = output_constraints\n        else:\n            self.output_constraints = Objectives(output_constraints)\n\n        if isinstance(models, Models) or models is None:\n            self.models = models\n        else:\n            self.models = Models(models)\n\n        if f is not None:\n            self.f = f\n\n        if isinstance(data, dict):\n            data = pd.read_json(json.dumps(data), orient=\"split\")\n\n        if isinstance(optima, dict):\n            optima = pd.read_json(json.dumps(optima), orient=\"split\")\n\n        self.set_data(data)\n        self.set_optima(optima)\n        self.check_problem()\n        self.check_models()\n\n    @property\n    def n_inputs(self) -&gt; int:\n        return len(self.inputs)\n\n    @property\n    def n_outputs(self) -&gt; int:\n        return len(self.outputs)\n\n    @property\n    def n_objectives(self) -&gt; int:\n        return len(self.objectives)\n\n    @property\n    def n_constraints(self) -&gt; int:\n        return 0 if self.constraints is None else len(self.constraints)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        s = \"Problem(\\n\"\n        s += f\"name={self.name},\\n\"\n        s += f\"inputs={self.inputs},\\n\"\n        s += f\"outputs={self.outputs},\\n\"\n        s += f\"objectives={self.objectives},\\n\"\n        if self.output_constraints is not None:\n            s += f\"output_constraints={self.output_constraints},\\n\"\n        if self.constraints is not None:\n            s += f\"constraints={self.constraints},\\n\"\n        if self.models is not None:\n            s += f\"models={self.models},\\n\"\n        if self.data is not None:\n            s += f\"data=\\n{self.data.head()}\\n\"\n        if self.optima is not None:\n            s += f\"optima=\\n{self.optima.head()}\\n\"\n        return s + \")\"\n\n    @staticmethod\n    def from_config(config: dict) -&gt; \"Problem\":\n        \"\"\"Create a Problem instance from a configuration dict.\"\"\"\n        return Problem(**config)\n\n    def to_config(self) -&gt; dict:\n        \"\"\"Return json-serializable configuration dict.\"\"\"\n\n        config = {\n            \"name\": self.name,\n            \"inputs\": self.inputs.to_config(),\n            \"outputs\": self.outputs.to_config(),\n            \"objectives\": self.objectives.to_config(),\n        }\n        if self.output_constraints is not None:\n            config[\"output_constraints\"] = self.output_constraints.to_config()\n        if self.constraints is not None:\n            config[\"constraints\"] = self.constraints.to_config()\n        if self.models is not None:\n            config[\"models\"] = self.models.to_config()\n        if self.data is not None:\n            config[\"data\"] = self.data.replace({np.nan: None}).to_dict(\"split\")\n        if self.optima is not None:\n            config[\"optima\"] = self.optima.replace({np.nan: None}).to_dict(\"split\")\n        return config\n\n    @staticmethod\n    def from_json(fname: PathLike) -&gt; \"Problem\":\n        \"\"\"Read a problem from a JSON file.\"\"\"\n        with open(fname, \"rb\") as infile:\n            config = json.loads(infile.read())\n        return Problem(**config)\n\n    def to_json(self, fname: PathLike) -&gt; None:\n        \"\"\"Save a problem from a JSON file.\"\"\"\n        with open(fname, \"wb\") as outfile:\n            b = json.dumps(self.to_config(), ensure_ascii=False, separators=(\",\", \":\"))\n            outfile.write(b.encode(\"utf-8\"))\n\n    def check_problem(self) -&gt; None:\n        \"\"\"Check if input and output parameters are consistent.\"\"\"\n        # check for duplicate names\n        duplicates = set(self.inputs.names).intersection(self.outputs.names)\n        if duplicates:\n            raise ValueError(f\"Parameter name in both inputs and outputs: {duplicates}\")\n\n        # check if all objectives refer to an output\n        for obj in self.objectives:\n            if obj.name not in self.outputs.names:\n                raise ValueError(f\"Objective refers to unknown parameter: {obj.name}\")\n\n    def check_data(self, data: pd.DataFrame) -&gt; None:\n        \"\"\"Check if data is consistent with input and output parameters.\"\"\"\n        for p in self.inputs + self.outputs:\n            # data must contain all parameters\n            if p.name not in data.columns:\n                raise ValueError(\n                    f\"Parameter {p.name} is missing. Data must contain all parameters.\"\n                )\n\n            # data for continuous / discrete parameters must be numeric\n            if isinstance(p, (Continuous, Discrete)):\n                ok = is_numeric_dtype(data[p.name]) or data[p.name].isnull().all()\n                if not ok:\n                    raise ValueError(\n                        f\"Parameter {p.name} contains non-numeric values. Data for continuous / discrete parameters must be numeric.\"\n                    )\n\n            # categorical levels in data must be specified\n            elif isinstance(p, Categorical):\n                ok = p.contains(data[p.name]) | data[p.name].isna()\n                if not ok.all():\n                    unknowns = data[p.name][~ok].unique().tolist()\n                    raise ValueError(\n                        f\"Data for parameter {p.name} contains unknown values: {unknowns}. All categorical levels must be specified.\"\n                    )\n\n        # inputs must be complete\n        for p in self.inputs:\n            if data[p.name].isnull().any():\n                raise ValueError(\n                    f\"Input parameter {p.name} has missing data. Inputs must be complete.\"\n                )\n\n        # outputs must have at least one observation\n        for p in self.outputs:\n            if data[p.name].isnull().all():\n                raise ValueError(\n                    f\"Output parameter {p.name} has no data. Outputs must have at least one observation.\"\n                )\n\n    def check_models(self) -&gt; None:\n        \"\"\"Check if the models are well defined\"\"\"\n        if self.models is None:\n            return\n\n        for model in self.models:\n            # models need to refer to output parameters\n            for n in model.names:\n                if n not in self.outputs.names:\n                    raise ValueError(f\"Model {model} refers to unknown outputs\")\n\n            if isinstance(model, LinearModel):\n                if len(model.coefficients) != self.n_inputs:\n                    raise ValueError(f\"Model {model} has wrong number of coefficients.\")\n\n    def set_data(self, data: Optional[pd.DataFrame]) -&gt; None:\n        \"\"\"Set the data.\"\"\"\n        if data is not None:\n            for p in self.inputs:\n                # Categorical levels are required to be strings. Ensure that the corresponding data is as well.\n                if isinstance(p, Categorical):\n                    nulls = data[p.name].isna()\n                    data[p.name] = data[p.name].astype(str).mask(nulls, np.nan)\n\n            self.check_data(data)\n\n        self.data = data\n\n    def get_data(self) -&gt; pd.DataFrame:\n        \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\"\n        if self.data is None:\n            return pd.DataFrame(columns=self.inputs.names + self.outputs.names)\n        return self.data\n\n    def add_data(self, data: pd.DataFrame) -&gt; None:\n        \"\"\"Add a number of data points.\"\"\"\n        self.check_data(data)\n        self.data = pd.concat([self.data, data], axis=0)\n\n    def set_optima(self, optima: Optional[pd.DataFrame]) -&gt; None:\n        \"\"\"Set the optima / Pareto front.\"\"\"\n        if optima is not None:\n            self.check_data(optima)\n        self.optima = optima\n\n    def get_X(self, data: Optional[pd.DataFrame] = None) -&gt; np.ndarray:\n        \"\"\"Return the input values in `data` or `self.data`.\"\"\"\n        if data is not None:\n            return data[self.inputs.names].values\n        return self.get_data()[self.inputs.names].values\n\n    def get_Y(self, data: Optional[pd.DataFrame] = None) -&gt; np.ndarray:\n        \"\"\"Return the output values in `data` or `self.data`.\"\"\"\n        if data is not None:\n            return data[self.outputs.names].values\n        return self.get_data()[self.outputs.names].values\n\n    def get_XY(\n        self,\n        outputs: Optional[List[str]] = None,\n        data: Optional[pd.DataFrame] = None,\n        continuous: str = \"none\",\n        discrete: str = \"none\",\n        categorical: str = \"none\",\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Return the input and output values as numeric numpy arrays.\n\n        Rows with missing output values will be dropped.\n        Input values are assumed to be complete.\n        Categorical outputs are one-hot or label encoded.\n\n        Args:\n            outputs (optional): Subset of the outputs to consider.\n            data (optional): Dataframe to consider instead of problem.data\n        \"\"\"\n        if outputs is None:\n            outputs = self.outputs.names\n        if data is None:\n            data = self.get_data()\n        notna = data[outputs].notna().all(axis=1)\n        X = self.inputs.transform(\n            data, continuous=continuous, discrete=discrete, categorical=categorical\n        )[notna].values\n        Y = data[outputs][notna].values\n        return X, Y\n\n    def get_X_bounds(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Return the lower and upper data bounds.\"\"\"\n        X = self.get_X()\n        xlo = X.min(axis=0)\n        xhi = X.max(axis=0)\n        b = xlo == xhi\n        xhi[b] = xlo[b] + 1  # prevent division by zero when dividing by (xhi - xlo)\n        return xlo, xhi\n\n    def sample_inputs(self, n_samples=10) -&gt; pd.DataFrame:\n        \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\"\n        if self.constraints is None:\n            return sobol_sampling(n_samples, self.inputs)\n        return constrained_sampling(n_samples, self.inputs, self.constraints)\n\n    def create_initial_data(self, n_samples: int = 10) -&gt; None:\n        \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\"\n        if self.f is None:\n            raise NotImplementedError(\"problem.f is not implemented for the problem.\")\n        X = self.sample_inputs(n_samples)\n        Y = self.f(X)\n        self.data = pd.concat([X, Y], axis=1)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.__init__","title":"<code>__init__(inputs, outputs, objectives=None, constraints=None, output_constraints=None, f=None, models=None, data=None, optima=None, name=None, **kwargs)</code>","text":"<p>An optimization problem.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>ParametersLike</code> <p>Input parameters.</p> required <code>outputs</code> <code>ParametersLike</code> <p>Output parameters.</p> required <code>objectives</code> <code>Optional[ObjectivesLike]</code> <p>Optimization objectives. Defaults to minimization.</p> <code>None</code> <code>constraints</code> <code>Optional[ConstraintsLike]</code> <p>Constraints on the inputs.</p> <code>None</code> <code>output_constraints</code> <code>Optional[ObjectivesLike]</code> <p>Constraints on the outputs.</p> <code>None</code> <code>f</code> <code>Optional[Callable]</code> <p>Function to evaluate the outputs for given inputs. Must have the signature: f(x: pd.DataFrame) -&gt; pd.DataFrame</p> <code>None</code> <code>data</code> <code>Optional[DataFrameLike]</code> <p>Experimental data.</p> <code>None</code> <code>optima</code> <code>Optional[DataFrameLike]</code> <p>Pareto optima.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of the problem.</p> <code>None</code> Source code in <code>opti/problem.py</code> <pre><code>def __init__(\n    self,\n    inputs: ParametersLike,\n    outputs: ParametersLike,\n    objectives: Optional[ObjectivesLike] = None,\n    constraints: Optional[ConstraintsLike] = None,\n    output_constraints: Optional[ObjectivesLike] = None,\n    f: Optional[Callable] = None,\n    models: Optional[ModelsLike] = None,\n    data: Optional[DataFrameLike] = None,\n    optima: Optional[DataFrameLike] = None,\n    name: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"An optimization problem.\n\n    Args:\n        inputs: Input parameters.\n        outputs: Output parameters.\n        objectives: Optimization objectives. Defaults to minimization.\n        constraints: Constraints on the inputs.\n        output_constraints: Constraints on the outputs.\n        f: Function to evaluate the outputs for given inputs.\n            Must have the signature: f(x: pd.DataFrame) -&gt; pd.DataFrame\n        data: Experimental data.\n        optima: Pareto optima.\n        name: Name of the problem.\n    \"\"\"\n    self.name = name if name is not None else \"Problem\"\n    self.inputs = inputs if isinstance(inputs, Parameters) else Parameters(inputs)\n    self.outputs = (\n        outputs if isinstance(outputs, Parameters) else Parameters(outputs)\n    )\n\n    if objectives is None:\n        self.objectives = Objectives([Minimize(m) for m in self.outputs.names])\n    elif isinstance(objectives, Objectives):\n        self.objectives = objectives\n    else:\n        self.objectives = Objectives(objectives)\n\n    if isinstance(constraints, Constraints):\n        pass\n    elif not constraints:\n        constraints = None\n    else:\n        constraints = Constraints(constraints)\n        if len(constraints) == 0:  # no valid constraints\n            constraints = None\n    self.constraints = constraints\n\n    if isinstance(output_constraints, Objectives) or output_constraints is None:\n        self.output_constraints = output_constraints\n    else:\n        self.output_constraints = Objectives(output_constraints)\n\n    if isinstance(models, Models) or models is None:\n        self.models = models\n    else:\n        self.models = Models(models)\n\n    if f is not None:\n        self.f = f\n\n    if isinstance(data, dict):\n        data = pd.read_json(json.dumps(data), orient=\"split\")\n\n    if isinstance(optima, dict):\n        optima = pd.read_json(json.dumps(optima), orient=\"split\")\n\n    self.set_data(data)\n    self.set_optima(optima)\n    self.check_problem()\n    self.check_models()\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.add_data","title":"<code>add_data(data)</code>","text":"<p>Add a number of data points.</p> Source code in <code>opti/problem.py</code> <pre><code>def add_data(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Add a number of data points.\"\"\"\n    self.check_data(data)\n    self.data = pd.concat([self.data, data], axis=0)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.check_data","title":"<code>check_data(data)</code>","text":"<p>Check if data is consistent with input and output parameters.</p> Source code in <code>opti/problem.py</code> <pre><code>def check_data(self, data: pd.DataFrame) -&gt; None:\n    \"\"\"Check if data is consistent with input and output parameters.\"\"\"\n    for p in self.inputs + self.outputs:\n        # data must contain all parameters\n        if p.name not in data.columns:\n            raise ValueError(\n                f\"Parameter {p.name} is missing. Data must contain all parameters.\"\n            )\n\n        # data for continuous / discrete parameters must be numeric\n        if isinstance(p, (Continuous, Discrete)):\n            ok = is_numeric_dtype(data[p.name]) or data[p.name].isnull().all()\n            if not ok:\n                raise ValueError(\n                    f\"Parameter {p.name} contains non-numeric values. Data for continuous / discrete parameters must be numeric.\"\n                )\n\n        # categorical levels in data must be specified\n        elif isinstance(p, Categorical):\n            ok = p.contains(data[p.name]) | data[p.name].isna()\n            if not ok.all():\n                unknowns = data[p.name][~ok].unique().tolist()\n                raise ValueError(\n                    f\"Data for parameter {p.name} contains unknown values: {unknowns}. All categorical levels must be specified.\"\n                )\n\n    # inputs must be complete\n    for p in self.inputs:\n        if data[p.name].isnull().any():\n            raise ValueError(\n                f\"Input parameter {p.name} has missing data. Inputs must be complete.\"\n            )\n\n    # outputs must have at least one observation\n    for p in self.outputs:\n        if data[p.name].isnull().all():\n            raise ValueError(\n                f\"Output parameter {p.name} has no data. Outputs must have at least one observation.\"\n            )\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.check_models","title":"<code>check_models()</code>","text":"<p>Check if the models are well defined</p> Source code in <code>opti/problem.py</code> <pre><code>def check_models(self) -&gt; None:\n    \"\"\"Check if the models are well defined\"\"\"\n    if self.models is None:\n        return\n\n    for model in self.models:\n        # models need to refer to output parameters\n        for n in model.names:\n            if n not in self.outputs.names:\n                raise ValueError(f\"Model {model} refers to unknown outputs\")\n\n        if isinstance(model, LinearModel):\n            if len(model.coefficients) != self.n_inputs:\n                raise ValueError(f\"Model {model} has wrong number of coefficients.\")\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.check_problem","title":"<code>check_problem()</code>","text":"<p>Check if input and output parameters are consistent.</p> Source code in <code>opti/problem.py</code> <pre><code>def check_problem(self) -&gt; None:\n    \"\"\"Check if input and output parameters are consistent.\"\"\"\n    # check for duplicate names\n    duplicates = set(self.inputs.names).intersection(self.outputs.names)\n    if duplicates:\n        raise ValueError(f\"Parameter name in both inputs and outputs: {duplicates}\")\n\n    # check if all objectives refer to an output\n    for obj in self.objectives:\n        if obj.name not in self.outputs.names:\n            raise ValueError(f\"Objective refers to unknown parameter: {obj.name}\")\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.create_initial_data","title":"<code>create_initial_data(n_samples=10)</code>","text":"<p>Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.</p> Source code in <code>opti/problem.py</code> <pre><code>def create_initial_data(self, n_samples: int = 10) -&gt; None:\n    \"\"\"Create an initial data set for benchmark problems by sampling uniformly from the input space and evaluating f(x) at the sampled inputs.\"\"\"\n    if self.f is None:\n        raise NotImplementedError(\"problem.f is not implemented for the problem.\")\n    X = self.sample_inputs(n_samples)\n    Y = self.f(X)\n    self.data = pd.concat([X, Y], axis=1)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.from_config","title":"<code>from_config(config)</code>  <code>staticmethod</code>","text":"<p>Create a Problem instance from a configuration dict.</p> Source code in <code>opti/problem.py</code> <pre><code>@staticmethod\ndef from_config(config: dict) -&gt; \"Problem\":\n    \"\"\"Create a Problem instance from a configuration dict.\"\"\"\n    return Problem(**config)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.from_json","title":"<code>from_json(fname)</code>  <code>staticmethod</code>","text":"<p>Read a problem from a JSON file.</p> Source code in <code>opti/problem.py</code> <pre><code>@staticmethod\ndef from_json(fname: PathLike) -&gt; \"Problem\":\n    \"\"\"Read a problem from a JSON file.\"\"\"\n    with open(fname, \"rb\") as infile:\n        config = json.loads(infile.read())\n    return Problem(**config)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_X","title":"<code>get_X(data=None)</code>","text":"<p>Return the input values in <code>data</code> or <code>self.data</code>.</p> Source code in <code>opti/problem.py</code> <pre><code>def get_X(self, data: Optional[pd.DataFrame] = None) -&gt; np.ndarray:\n    \"\"\"Return the input values in `data` or `self.data`.\"\"\"\n    if data is not None:\n        return data[self.inputs.names].values\n    return self.get_data()[self.inputs.names].values\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_XY","title":"<code>get_XY(outputs=None, data=None, continuous='none', discrete='none', categorical='none')</code>","text":"<p>Return the input and output values as numeric numpy arrays.</p> <p>Rows with missing output values will be dropped. Input values are assumed to be complete. Categorical outputs are one-hot or label encoded.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>optional</code> <p>Subset of the outputs to consider.</p> <code>None</code> <code>data</code> <code>optional</code> <p>Dataframe to consider instead of problem.data</p> <code>None</code> Source code in <code>opti/problem.py</code> <pre><code>def get_XY(\n    self,\n    outputs: Optional[List[str]] = None,\n    data: Optional[pd.DataFrame] = None,\n    continuous: str = \"none\",\n    discrete: str = \"none\",\n    categorical: str = \"none\",\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return the input and output values as numeric numpy arrays.\n\n    Rows with missing output values will be dropped.\n    Input values are assumed to be complete.\n    Categorical outputs are one-hot or label encoded.\n\n    Args:\n        outputs (optional): Subset of the outputs to consider.\n        data (optional): Dataframe to consider instead of problem.data\n    \"\"\"\n    if outputs is None:\n        outputs = self.outputs.names\n    if data is None:\n        data = self.get_data()\n    notna = data[outputs].notna().all(axis=1)\n    X = self.inputs.transform(\n        data, continuous=continuous, discrete=discrete, categorical=categorical\n    )[notna].values\n    Y = data[outputs][notna].values\n    return X, Y\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_X_bounds","title":"<code>get_X_bounds()</code>","text":"<p>Return the lower and upper data bounds.</p> Source code in <code>opti/problem.py</code> <pre><code>def get_X_bounds(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return the lower and upper data bounds.\"\"\"\n    X = self.get_X()\n    xlo = X.min(axis=0)\n    xhi = X.max(axis=0)\n    b = xlo == xhi\n    xhi[b] = xlo[b] + 1  # prevent division by zero when dividing by (xhi - xlo)\n    return xlo, xhi\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_Y","title":"<code>get_Y(data=None)</code>","text":"<p>Return the output values in <code>data</code> or <code>self.data</code>.</p> Source code in <code>opti/problem.py</code> <pre><code>def get_Y(self, data: Optional[pd.DataFrame] = None) -&gt; np.ndarray:\n    \"\"\"Return the output values in `data` or `self.data`.\"\"\"\n    if data is not None:\n        return data[self.outputs.names].values\n    return self.get_data()[self.outputs.names].values\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.get_data","title":"<code>get_data()</code>","text":"<p>Return <code>self.data</code> if it exists or an empty dataframe.</p> Source code in <code>opti/problem.py</code> <pre><code>def get_data(self) -&gt; pd.DataFrame:\n    \"\"\"Return `self.data` if it exists or an empty dataframe.\"\"\"\n    if self.data is None:\n        return pd.DataFrame(columns=self.inputs.names + self.outputs.names)\n    return self.data\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.sample_inputs","title":"<code>sample_inputs(n_samples=10)</code>","text":"<p>Uniformly sample points from the input space subject to the constraints.</p> Source code in <code>opti/problem.py</code> <pre><code>def sample_inputs(self, n_samples=10) -&gt; pd.DataFrame:\n    \"\"\"Uniformly sample points from the input space subject to the constraints.\"\"\"\n    if self.constraints is None:\n        return sobol_sampling(n_samples, self.inputs)\n    return constrained_sampling(n_samples, self.inputs, self.constraints)\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.set_data","title":"<code>set_data(data)</code>","text":"<p>Set the data.</p> Source code in <code>opti/problem.py</code> <pre><code>def set_data(self, data: Optional[pd.DataFrame]) -&gt; None:\n    \"\"\"Set the data.\"\"\"\n    if data is not None:\n        for p in self.inputs:\n            # Categorical levels are required to be strings. Ensure that the corresponding data is as well.\n            if isinstance(p, Categorical):\n                nulls = data[p.name].isna()\n                data[p.name] = data[p.name].astype(str).mask(nulls, np.nan)\n\n        self.check_data(data)\n\n    self.data = data\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.set_optima","title":"<code>set_optima(optima)</code>","text":"<p>Set the optima / Pareto front.</p> Source code in <code>opti/problem.py</code> <pre><code>def set_optima(self, optima: Optional[pd.DataFrame]) -&gt; None:\n    \"\"\"Set the optima / Pareto front.\"\"\"\n    if optima is not None:\n        self.check_data(optima)\n    self.optima = optima\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.to_config","title":"<code>to_config()</code>","text":"<p>Return json-serializable configuration dict.</p> Source code in <code>opti/problem.py</code> <pre><code>def to_config(self) -&gt; dict:\n    \"\"\"Return json-serializable configuration dict.\"\"\"\n\n    config = {\n        \"name\": self.name,\n        \"inputs\": self.inputs.to_config(),\n        \"outputs\": self.outputs.to_config(),\n        \"objectives\": self.objectives.to_config(),\n    }\n    if self.output_constraints is not None:\n        config[\"output_constraints\"] = self.output_constraints.to_config()\n    if self.constraints is not None:\n        config[\"constraints\"] = self.constraints.to_config()\n    if self.models is not None:\n        config[\"models\"] = self.models.to_config()\n    if self.data is not None:\n        config[\"data\"] = self.data.replace({np.nan: None}).to_dict(\"split\")\n    if self.optima is not None:\n        config[\"optima\"] = self.optima.replace({np.nan: None}).to_dict(\"split\")\n    return config\n</code></pre>"},{"location":"ref-problem/#opti.problem.Problem.to_json","title":"<code>to_json(fname)</code>","text":"<p>Save a problem from a JSON file.</p> Source code in <code>opti/problem.py</code> <pre><code>def to_json(self, fname: PathLike) -&gt; None:\n    \"\"\"Save a problem from a JSON file.\"\"\"\n    with open(fname, \"wb\") as outfile:\n        b = json.dumps(self.to_config(), ensure_ascii=False, separators=(\",\", \":\"))\n        outfile.write(b.encode(\"utf-8\"))\n</code></pre>"},{"location":"ref-problem/#opti.problem.read_json","title":"<code>read_json(filepath)</code>","text":"<p>Read a problem specification from a JSON file.</p> Source code in <code>opti/problem.py</code> <pre><code>def read_json(filepath: PathLike) -&gt; Problem:\n    \"\"\"Read a problem specification from a JSON file.\"\"\"\n    return Problem.from_json(filepath)\n</code></pre>"},{"location":"ref-problems/","title":"Test Problems","text":""},{"location":"ref-sampling/","title":"Sampling","text":""},{"location":"ref-tools/","title":"Tools","text":""},{"location":"datasets/alkox/","title":"Alkox","text":"<p>This dataset reports the biocatalytic oxidation of benzyl alcohol by a copper radical oxidase (AlkOx).  The effects of enzyme loading, cocatalyst loading, and pH balance on both initial rate and total conversion were assayed. The dataset includes 104 samples with four parameters and one objective.</p> <pre><code>problem = opti.problems.Alkox()\n</code></pre> <p>Reference</p> <p>F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI. Obtained from Olympus.</p>"},{"location":"datasets/baumgartner-aniline/","title":"Aniline cross-coupling (Baumgartner 2019)","text":"<p>Dataset on optimizing Pd-catalyzed C\u2013N coupling reactions promoted by organic bases using aniline as starting material.</p> <pre><code>problem = opti.problems.BaumgartnerAniline()\n</code></pre> <p>Reference</p> <p>Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI. Data obtained from Summit.</p>"},{"location":"datasets/baumgartner-benzamide/","title":"Benzamide cross-coupling (Baumgartner 2019)","text":"<p>Dataset on optimizing Pd-catalyzed C\u2013N coupling reactions promoted by organic bases using benzamide as starting material.</p> <pre><code>problem = opti.problems.BaumgartnerBenzamide()\n</code></pre> <p>Reference</p> <p>Baumgartner et al. 2019 - Use of a Droplet Platform To Optimize Pd-Catalyzed C-N Coupling Reactions Promoted by Organic Bases DOI. Data obtained from Summit.</p>"},{"location":"datasets/benzylation/","title":"Benzylation","text":"<p>This dataset reports the yield of undesired product (impurity) in an N-benzylation reaction.  Four conditions of this reaction performed in a flow reactor can be controlled to minimize the yield of impurity. The dataset includes 73 samples with four parameters and one objective.</p> <pre><code>problem = opti.problems.Benzylation()\n</code></pre> <p>Reference</p> <p>A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI. Obtained from Olympus.</p>"},{"location":"datasets/cake/","title":"Cake","text":"<p>Fictional dataset for cake recipe optimization with mixed objectives.</p> <pre><code>problem = opti.problems.Cake()\n</code></pre>"},{"location":"datasets/fullerenes/","title":"Fullerenes","text":"<p>This dataset reports the production of o-xylenyl adducts of Buckminsterfullerenes.  Three process conditions (temperature, reaction time and ratio of sultine to C60) are varied to maximize the mole fraction of the desired product.  Experiments are executed on a three factor fully factorial grid with six levels per factor. The dataset includes 246 samples with three parameters and one objective.</p> <pre><code>problem = opti.problems.Fullerenes()\n</code></pre> <p>Reference</p> <p>B.E. Walker, J.H. Bannock, A.M. Nightingale, J.C. deMello. Tuning reaction products by constrained optimisation. React. Chem. Eng., (2017), 2, 785-798.  DOI. Obtained from Olympus.</p>"},{"location":"datasets/hplc/","title":"HPLC","text":"<p>This dataset reports the peak response of an automated high-performance liquid chromatography (HPLC) system for varying process parameters. The dataset includes 1,386 samples with six parameters and one objective.</p> <pre><code>problem = opti.problems.HPLC()\n</code></pre> <p>Reference</p> <p>L.M. Roch, F. H\u00e4se, C. Kreisbeck, T. Tamayo-Mendoza, L.P.E. Yunker, J.E. Hein, A. Aspuru-Guzik. ChemOS: an orchestration software to democratize autonomous discovery. (2018) DOI. Obtained from Olympus.</p>"},{"location":"datasets/photodegradation/","title":"Photodegradation","text":"<p>This dataset reports the degradation of polymer blends for organic solar cells under the exposure to light.  Individual data points encode the ratios of individual polymers in one blend, along with the measured photodegradation of this blend. The dataset includes 2,080 samples with five parameters and one objective.</p> <pre><code>problem = opti.problems.Photodegradation()\n</code></pre> <p>Reference</p> <p>S. Langner, F. H\u00e4se, J.D. Perea, T. Stubhan, J. Hauch, L.M. Roch, T. Heumueller, A. Aspuru-Guzik, C.J. Brabec. Beyond Ternary OPV: High-Throughput Experimentation and Self-Driving Laboratories Optimize Multicomponent Systems. Advanced Materials, 2020, 1907801. DOI. Obtained from Olympus.</p>"},{"location":"datasets/reizmann-suzuki/","title":"Suzuki (Reizmann 2016)","text":"<p>Each case was has a different set of substrates but the same possible catalysts. </p> <pre><code>problem = opti.problems.ReizmannSuzuki()\n</code></pre> <p>Reference</p> <p>Reizman et al. (2016) Suzuki-Miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry &amp; engineering, 1(6), 658-666 DOI. Data obtained from Summit.</p>"},{"location":"datasets/snar/","title":"SnAr","text":"<p>This dataset reports the e-factor for a nucleophilic aromatic substitution following the SnAr mechanism.  Individual data points encode four process parameters for a flow reactor to run the reaction, along with the measured e-factor (defined as the ratio of the mass waste to the mass of product). The dataset includes 67 samples with four parameters and one objective.</p> <pre><code>problem = opti.problems.SnAr()\n</code></pre> <p>Reference</p> <p>A.M. Schweidtmann, A.D. Clayton, N. Holmes, E. Bradford, R.A. Bourne, A.A. Lapkin. Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto front of multiple objectives. Chem. Eng. J. 352 (2018) 277-282. DOI. Obtained from Olympus.</p>"},{"location":"datasets/suzuki/","title":"Suzuki","text":"<p>This dataset reports palladium-catalyzed Suzuki cross-coupling between 2-bromophenyltetrazole and an electron-deficient aryl boronate.  Four reaction conditions can be controlled to maximise the reaction yield. The dataset includes 247 samples with four parameters and one objective.</p> <pre><code>problem = opti.problems.Suzuki()\n</code></pre> <p>Reference</p> <p>F. H\u00e4se, M. Aldeghi, R.J. Hickman, L.M. Roch, M. Christensen, E. Liles, J.E. Hein, A. Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. arXiv (2020), 2010.04153. DOI. Obtained from Olympus.</p>"},{"location":"examples/problem_reduction/","title":"Problem reduction","text":"<p>When describing physical experiments there are often linear equality constraints to be considered. For example in a formulation all ingredients of a mixture add up to 1. </p> <pre><code>problem = opti.Problem(\n    inputs=[\n        opti.Continuous(\"x1\", [0.1, 0.7]),\n        opti.Continuous(\"x2\", [0, 0.8]),\n        opti.Continuous(\"x3\", [0.3, 0.9]),\n    ],\n    outputs=[opti.Continuous(\"y\")],\n    constraints=[opti.LinearEquality([\"x1\", \"x2\", \"x3\"], rhs=1)]\n)\n</code></pre> <p>In statistical modeling linear equalities lead to multicollinearities, which makes the coefficients of linear models sensitive to noise. For modeling tasks this collinearity can be addressed by e.g. dropping one input parameter for each corresponding equality constraint.</p> <p>For sampling and optimization tasks this becomes a bit trickier as the parameter bounds and inequality constraints need to be adapted as well. Consider in the initial example we drop \\(x_1\\) together with the linear equality.  To ensure that solutions (\\(x_2\\), \\(x_3\\)) still satisfy the box bounds and constraints, we need to add the following two inequality constraints: $$ \\begin{align} x_1 \\geq 0.1 \\Longleftrightarrow x_2 + x_3 \\leq 0.9 \\newline x_1 \\leq 0.7 \\Longleftrightarrow x_2 + x_3 \\geq 0.3 \\end{align} $$</p> <p>The function <code>reduce_problem</code> automates this tedious task. Given a problem containing any number of linear inequalities and at least one equality constraint, it returns an equivalent problem where the linear equalities are removed by eliminating a corresponding number of inputs.</p> <pre><code>reduced_problem, transform = opti.tools.reduce_problem(problem)\nprint(reduced_problem)\n&gt;&gt;&gt; \nProblem(\n    inputs=Parameters([\n        Continuous('x2', domain=[0.0, 0.8]), \n        Continuous('x3', domain=[0.3, 0.9])\n    ]),\n    outputs=Parameters([Continuous('y')]),\n    objectives=Objectives([Minimize('y')]),\n    constraints=Constraints([\n        LinearInequality(names=['x2', 'x3'], lhs=[-1.0, -1.0], rhs=-0.3),\n        LinearInequality(names=['x2', 'x3'], lhs=[1.0, 1.0], rhs=0.9)\n    ])\n)\n</code></pre> <p>The transformer object allows to transfrom data to and from the reduced space.</p> <pre><code>X1 = problem.sample_inputs(10)\nXr = transform.drop_data(X1)\nX2 = transform.augment_data(Xr)\nassert np.allclose(X1, X2[X1.columns])\n</code></pre> <p>Equality constraints are not well supported in sampling (any form of acceptance-rejection sampling will not work) and optimization methods.  For example population-based optimization approaches such as evolutionary algorithms only approximately support linear equalities via penalties or a conversion to two-sided inequalites. By reducing the problem, such optimization tasks become significantly easier to solve.</p> <p>Finally, let's consider a more involved example involving two mixtures, A and B, as well as an additional discrete and categorical variable, and an extra inequality constraint for some of the components of mixture A. We also set up a function <code>y = f(X)</code> to evaluate the system.</p> <pre><code>def f(X):\n    y = X[[\"A1\", \"A2\", \"A3\", \"A4\"]] @ [1, -2, 3, 2]\n    y += X[[\"B1\", \"B2\", \"B3\"]] @ [0.1, 0.4, 0.3]\n    y += X[\"Temperature\"] / 30\n    y += X[\"Process\"] == \"process 2\"\n    return pd.DataFrame({\"y\": y})\n\nproblem = opti.Problem(\n    inputs=[\n        opti.Continuous(\"A1\", [0, 0.9]),\n        opti.Continuous(\"A2\", [0, 0.8]),\n        opti.Continuous(\"A3\", [0, 0.9]),\n        opti.Continuous(\"A4\", [0, 0.9]),\n        opti.Continuous(\"B1\", [0.3, 0.9]),\n        opti.Continuous(\"B2\", [0, 0.8]),\n        opti.Continuous(\"B3\", [0.1, 1]),\n        opti.Discrete(\"Temperature\", [20, 25, 30]),\n        opti.Categorical(\"Process\", [\"process 1\", \"process 2\", \"process 3\"])\n    ],\n    outputs=[opti.Continuous(\"y\")],\n    constraints=[\n        opti.LinearEquality([\"A1\", \"A2\", \"A3\", \"A4\"], rhs=1),\n        opti.LinearEquality([\"B1\", \"B2\", \"B3\"], rhs=1),\n        opti.LinearInequality([\"A1\", \"A2\"], lhs=[1, 2], rhs=0.8),\n    ],\n    f=f\n)\n</code></pre> <p>Reducing the problem works despite the discrete and categorical inputs as these don't appear in the linear equalities. We end up 7 out of 9 initial inputs and 5 inequality constraints, which are only referring to the remaining inputs. <pre><code>reduced_problem, transform = opti.tools.reduce_problem(problem)\nprint(reduced_problem)\n&gt;&gt;&gt; \nProblem(\n    inputs=Parameters([\n        Discrete('Temperature', domain=[20.0, 25.0, 30.0]),\n        Categorical('Process', domain=['process 1', 'process 2', 'process 3']),\n        Continuous('A2', domain=[0.0, 0.8]),\n        Continuous('A3', domain=[0.0, 0.9]),\n        Continuous('A4', domain=[0.0, 0.9]),\n        Continuous('B2', domain=[0.0, 0.8]),\n        Continuous('B3', domain=[0.1, 1.0])\n    ]),\n    outputs=Parameters([Continuous('y')]),\n    objectives=Objectives([Minimize('y')]),\n    constraints=Constraints([\n        LinearInequality(names=['A2' 'A3' 'A4'], lhs=[1.0, -1.0, -1.0], rhs=-0.2),\n        LinearInequality(names=['A2', 'A3', 'A4'], lhs=[-1.0, -1.0, -1.0], rhs=-0.1),\n        LinearInequality(names=['A2', 'A3', 'A4'], lhs=[1.0, 1.0, 1.0], rhs=1.0),\n        LinearInequality(names=['B2', 'B3'], lhs=[-1.0, -1.0], rhs=-0.1),\n        LinearInequality(names=['B2', 'B3'], lhs=[1.0, 1.0], rhs=0.7)\n    ])\n)\n</code></pre></p> <p>The function <code>f(X)</code> was automaticaly wrapped so in the reduced problem it can be evaluated for points in the reduced space, with the same result.</p> <pre><code>Xr = reduced_problem.sample_inputs(10)\nX = transform.augment_data(Xr)\ny1 = problem.f(X)\ny2 = reduced_problem.f(Xr)\nassert np.allclose(y1, y2)\n</code></pre>"}]}